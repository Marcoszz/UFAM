{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aula_Prática_SVM_02_07_20.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5R8YxNLOwgPAb1oarQ+j0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5Q936NDXKXYD","colab_type":"text"},"source":["# <h3 align=\"center\"> **Atividade Prática**</h3> \n","\n","\n","\n","\n","<h3 align=\"center\"> Data: 02/07/2010</h3> "]},{"cell_type":"markdown","metadata":{"id":"5XBdCvyUTLmg","colab_type":"text"},"source":["Vamos iniciar estudando o comportamento de SVM Linear\n","- No SktLearn, trata-se do classificador **LinearSVC** ou **svm.SVC(kernel='linear')**, embora essas duas versões possam gerar soluções ligeiramente differentes.\n"]},{"cell_type":"code","metadata":{"id":"B_pypY8nZ2v3","colab_type":"code","colab":{}},"source":["# Importando bibliotecas necessárias\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import sklearn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aiVGWEYQA_QZ","colab_type":"text"},"source":["Abaixo há uma função criada para plotar o hiperplano de separação ótima, os vetores de suporte e a margem. O código está oculto para não poluir o notebook. Não esqueça de executar essa função."]},{"cell_type":"code","metadata":{"id":"aP496m15937q","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Função para plotar hiperplano de separação ótima, vetores de suporte e margem (hiperplano_otimo_plot)\n","# Função criada para plotar hiperplano de separação ótima, vetores de suporte e margem\n","def hiperplano_otimo_plot(X, y, modelo, ax=None):\n","    if ax is None:\n","        ax = plt.gca()\n","    #ax.plot(x, y) ## example plot here\n","    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Classe 0\")\n","    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Classe 1\")\n","    plt.legend(loc=\"upper center\", fontsize=14) \n","    # plotando a função de decisão\n","    ax = plt.gca()\n","    xlim = ax.get_xlim()\n","    ylim = ax.get_ylim()\n","\n","    # criando um grid para avaliar o modelo\n","    xx = np.linspace(xlim[0], xlim[1], 30)\n","    yy = np.linspace(ylim[0], ylim[1], 30)\n","    YY, XX = np.meshgrid(yy, xx)\n","    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n","    Z = modelo.decision_function(xy).reshape(XX.shape)\n","\n","    # plotando a superfície de decisão e as margens\n","    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n","            linestyles=['--', '-', '--'])\n","\n","    # plotando vetores de suporte\n","    ax.scatter(modelo.support_vectors_[:, 0], modelo.support_vectors_[:, 1], s=100,\n","            linewidth=1, facecolors='none', edgecolors='k')\n","    return(ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1HJsXasHI0ip","colab_type":"text"},"source":["Nós vamos criar uma base artificial (base sintética) para conseguirmos visualizar melhor o resultado das modificações nos parâmetros de SVM. É uma base composta por dois atributos e 11 instâncias."]},{"cell_type":"code","metadata":{"id":"iA7gsTEbJNkZ","colab_type":"code","colab":{}},"source":["#Criando a base de dados X2D e rótulos correspondentes y.\n","X1D = np.linspace(-12, 12, 11).reshape(-1, 1)\n","X = np.c_[X1D, X1D**2]\n","y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n","\n","#Obtendo número de instâncias e de atributos da base\n","X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XnY62xFiPeDL","colab_type":"code","colab":{}},"source":["#A base representada no espaço de atributos mostra que esse é um problema linearmente separável\n","plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Classe 0\")\n","plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Classe 1\")\n","plt.legend(loc=\"upper center\", fontsize=14)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uIibvRKvQCPB","colab_type":"text"},"source":["Agora nós vamos treinar SVM linear para resolver esse problema de classificação"]},{"cell_type":"code","metadata":{"id":"aHMlFVbalg2s","colab_type":"code","colab":{}},"source":["from sklearn import svm\n","\n","#Vamos criar um classificador SVM linear com parâmetro de regularização C = 10\n","modelo = svm.SVC(kernel='linear', C=10) # Kernel Linear com parâmetro de regularização C = 10\n","\n","#Vamos treinar um modelo usando a base X\n","modelo.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_nfrsld7z3Ig","colab_type":"text"},"source":["Aqui nós veremos o hiperplano de separação ótima, os vetores de suporte e a margem"]},{"cell_type":"code","metadata":{"id":"P5VbiX3jcJys","colab_type":"code","colab":{}},"source":["hiperplano_otimo_plot(X, y, modelo)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOtwoLjyS0ZB","colab_type":"code","colab":{}},"source":["#Uma vez treinado o modelo, nós vamos criar uma instância separada para ser testada pelo modelo treinado\n","novoX =  np.array([-3.5, 85.25]).reshape(1, -1)\n","\n","#Vamos verificar para qual das duas classes a nova instância será atribuída\n","print(\"Classe predita\", modelo.predict(novoX))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2lbmzGkdSwY","colab_type":"code","colab":{}},"source":["#Vamos verificar onde a nova instância está localizada no espaço de atributos. A nova instância está marcada como um * vermelho\n","\n","hiperplano_otimo_plot(X, y, modelo)\n","\n","plt.plot(novoX[:, 0], novoX[:, 1], \"r*\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W85PSsFNCc-a","colab_type":"text"},"source":["É possível verificar que, visualmente, a classificação está correta."]},{"cell_type":"markdown","metadata":{"id":"q0cbS7ep2IAw","colab_type":"text"},"source":["##**Entendendo o efeito de *outliers* e da regularização:** nós vamos adicionar essa nova instância à base de treinamento e verificar o efeito no hiperplano de separação ótima"]},{"cell_type":"code","metadata":{"id":"1Tins1MdEQxB","colab_type":"code","colab":{}},"source":["#nX será a nova base de treinamento e ny será a nova lista de rótulos\n","\n","nX = np.append(X,  novoX, axis = 0)\n","ny = np.append(y, np.array([1]), axis =0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UEjTU2KJEWg1","colab_type":"text"},"source":["Nas células seguintes, **você deve fazer o seguinte**:\n","\n","\n","1.   Treinar SVM com a nova base nX e os mesmos parâmetros utilizados para treinar X, isto é, SVM com kernel linear e com C = 10\n","2.   Plotar o hiperplano resultante, junto com vetores de suporte e a margem\n","\n"]},{"cell_type":"code","metadata":{"id":"N7MjA_yoE711","colab_type":"code","colab":{}},"source":["# Treinando SVM com a nova base"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSgggth2FDNl","colab_type":"code","colab":{}},"source":["#Plotando o hiperplano de separação, os vetores de suporte e a margem"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VscsYwOJFoky","colab_type":"text"},"source":["**Responda**: \n","\n","\n","**1.**  Qual a diferença do hiperplano e da margem obtidos utilizando a nova base em comparação com esses itens obtidos ao treinar SVM com a base anterior?\n","\n","**2.**   O que ocasionou essas mudanças?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CW8caxHyGOS6","colab_type":"text"},"source":["Nós vimos que o parâmetro de regularização C ajuda SVM a estabelecer um melhor compromisso entre viés e variância e reduzir o impacto do *overfitting*, pois passa a permitir que SVM aceite erros de treinamento.\n","\n","\n","**3.**   Nesse caso, o valor de C deve aumentar ou diminuir?\n","\n","**4.**   Tente modificar o valor de C para gerar um hiperplano com mais capacidade de generalização. Mostre a figura do resultado obtido.\n","* Você pode modificar o valor de C usando intervalos relativamente longos, por exemplo: C = 0.1; C = 1; C = 10 e C = 100. Se for necessário, teste intervalos menores.\n","\n"]},{"cell_type":"code","metadata":{"id":"4tK8Qu8jHUuI","colab_type":"code","colab":{}},"source":["# Variando o valor de C"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VJ2pxTIYHXzi","colab_type":"code","colab":{}},"source":["# Melhor hiperplano de separaçao obtido"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yzqhg7sCHj4c","colab_type":"text"},"source":["# Trabalhando com problemas mais reias e com diferentes tipos de kernel"]},{"cell_type":"markdown","metadata":{"id":"neX2378I_3H5","colab_type":"text"},"source":["Nesse exemplo, nós utilizaremos a base **Breast Cancer** disponível no sklearn, mas originário do repositório da UCI (https://archive.ics.uci.edu/ml/datasets/breast+cancer). Trata-se de uma base de dados que foi obtida a partir de imagens digitalizadas de massas mamárias e que descrevem **dois** tipos de classe: maligno (harmful) e benigno (not harmful). Os atributos descrevem características dos núcleos celulares presentes nas imagens.\n","\n","No total, são **30 atributos** ordenados: raio médio, textura média, perímetro médio, área média, suavidade média, compacidade média, concavidade média, pontos côncavos médios, simetria média, dimensão fractal média, erro de raio, erro de textura, erro de perímetro, erro de área, erro de suavidade, erro de compactação, erro de concavidade, erro de pontos côncavos, erro de simetria, erro de dimensão fractal, pior raio, pior textura, pior perímetro, pior área, pior suavidade, pior compacidade, pior concavidade, piores pontos côncavos, pior simetria e pior dimensão fractal) e um alvo (tipo de câncer).\n","\n","Nós vamos utilizar o Pandas para facilitar a visualização dos dados estruturados"]},{"cell_type":"code","metadata":{"id":"5q4zYHzOTxwW","colab_type":"code","colab":{}},"source":["#Importando as bibliotecas\n","import pandas as pd\n","from sklearn import datasets\n","\n","#Carregando a base de dados\n","cancer = datasets.load_breast_cancer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJoo4WWMNXIm","colab_type":"code","colab":{}},"source":["#Vamos visualizar os dados\n","\n","cancer.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60aEZfrZNfNW","colab_type":"code","colab":{}},"source":["cancer.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QIrr3EbkYu0X","colab_type":"code","colab":{}},"source":["#Convertendo o dataset do Sklearn para o Dataframe do Pandas. Para isso,  nós vamos concatenar target e data usang o numpy \n","\n","df = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns = np.append(cancer['feature_names'], ['target']))\n","\n","#Para visualizar os dados na forma de tabela\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqak_diEVUqK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593908727459,"user_tz":240,"elapsed":1375,"user":{"displayName":"Eulanda Santos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giq8flmRbVBw82ZCrLQFM4poCw4q_eXybHktK2nPMbDh1V-oC3u23BOyoqbX9bukv4LtIDjG8ghZkxufHaWJjgcQZ5H7987nrntVeFQ7nG60-oMn27jueHWfbOzuqLGIRb7_309AAoEBnGIIm-QuYFL1uKIo5rIsmgBIiPWXYwMwkLDEMT5iCZRpRQFj8lT-svruVtpGaU2tUN1j9CZbzm1AA9zxifc4ACPpymbQw4V1qXC0QnEnswe7OefTQMVro7MG4CAaRSyjgcqaZfoA4tnOLf7keMCKwr69nZ61s5hmoRhkCQUkwXKLzj8Nl0iE31Z55pCjjmhWY7z3HQcxWPsErmsQ_4zL-bajO4vNu82AZdSPMI9ySq93cKEIQuK9MdBmuBd769eu6uoRHlbGa_4JlrOnsIzUml7niUeXOFlKbyWV7fQ_aLwK8Emew00hVKUWGaIRiZsDzktA6UTFb3cB5wNu0YpUhx-lC7mA5fiCWMkKOR1wBzHcSilKbLK0ej3u9xhfPx4BTc_KCAPcKPMOM_r9j9lXPDsIL00XLfvMUHGhZlygLGyhcblGqoGNKBM5Ph5RmTxLOFAvZkHHYRKBMRl1E5-sXa-4XJw3FTZjASfYu_VAFFegf58DUid6g8Sl9XLaqxCPvWjXbChkKVBEQ8s15MQaKoUOX7lcHuKHu7a6NEr4Qt4-MM4Y29LEyMMtqQBR8ooaOERme0E3EunzCUo8sn2XE4FChLPrflLXKlOPk2hTocPhqnJtqEm8hqdaw=s64","userId":"04288734832196105327"}},"outputId":"8109fd3e-1ce0-4843-9712-55070368d942"},"source":["#Vamos dividir a base em duas partições: treino e teste. A base de treino terá 80% das instâncias, enquanto a de teste terá 20%\n","\n","from sklearn.model_selection import train_test_split\n","\n","#Vamos definir X para receber as instâncias, enquanto y terá os rótulos\n","\n","X = df.iloc[:, 0:-1]\n","y = df.iloc[:, -1]\n","\n","#Dividindo a base (com seleção aleatória)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n","print(X_train.shape, y_train.shape)\n","print(X_test.shape, y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(455, 30) (455,)\n","(114, 30) (114,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aLueJ8W9XSUd","colab_type":"code","colab":{}},"source":["#Como os atributos originais estão em diferentes escalas, nós vamos normalizar os dados aqui\n","\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()  \n","X_train = sc.fit_transform(X_train)  \n","X_test = sc.transform(X_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OA_gIpHJIpYw","colab_type":"text"},"source":["# Trabalhando com problemas reais e com diferentes tipos de kernel#"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9Otvn1WMvO6Q"},"source":["Nesse exemplo, nós utilizaremos a base **Breast Cancer** disponível no sklearn, mas originária do repositório da UCI (https://archive.ics.uci.edu/ml/datasets/breast+cancer). Trata-se de uma base de dados que foi obtida a partir de imagens digitalizadas de massas mamárias e que descreve **dois** tipos de classe: maligno (harmful) e benigno (not harmful). Os atributos descrevem características dos núcleos celulares presentes nas imagens.\n","\n","No total, são **30 atributos** ordenados: raio médio, textura média, perímetro médio, área média, suavidade média, compacidade média, concavidade média, pontos côncavos médios, simetria média, dimensão fractal média, erro de raio, erro de textura, erro de perímetro, erro de área, erro de suavidade, erro de compactação, erro de concavidade, erro de pontos côncavos, erro de simetria, erro de dimensão fractal, pior raio, pior textura, pior perímetro, pior área, pior suavidade, pior compacidade, pior concavidade, piores pontos côncavos, pior simetria e pior dimensão fractal.\n","\n","Nós vamos utilizar o Pandas para facilitar a visualização dos dados estruturados"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SuFa2BZOvO6R","colab":{}},"source":["#Importando as bibliotecas\n","import pandas as pd\n","from sklearn import datasets\n","\n","#Carregando a base de dados\n","cancer = datasets.load_breast_cancer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wWbOLR4kvO6U","colab":{}},"source":["#Verificando quantidade de instâncias e de atributos\n","# Vamos também acessar os dados\n","\n","print(cancer.data.shape)\n","cancer.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JzA-pRwkvO6W","colab":{}},"source":["# E  as classes\n","cancer.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iwUZIi83vO6Y","colab":{}},"source":["#Convertendo o dataset do Sklearn para o Dataframe do Pandas. Para isso,  nós vamos concatenar target e data usando o numpy \n","\n","df = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns = np.append(cancer['feature_names'], ['target']))\n","\n","#Para visualizar os dados na forma de tabela\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NBeQv4W2vO6a","colab":{}},"source":["#Vamos dividir a base em duas partições: treino e teste. A base de treino terá 80% das instâncias, enquanto a de teste terá 20%\n","\n","from sklearn.model_selection import train_test_split\n","\n","#Vamos definir X para receber as instâncias, enquanto y terá os rótulos\n","X = df.iloc[:, 0:-1]\n","y = df.iloc[:, -1]\n","\n","#Dividindo a base (com seleção de instâncias aleatória)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n","\n","#Checando a divisão das instâncias entre as duas bases\n","print(X_train.shape, y_train.shape)\n","print(X_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GbEtPmC7vO6c","colab":{}},"source":["#Como os atributos originais estão em diferentes escalas, nós vamos normalizar os dados aqui, pois é importante deixar os atributos todos nos mesmos intervalos de dados\n","\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()  \n","X_train = sc.fit_transform(X_train)  \n","X_test = sc.transform(X_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jrHJ2HXnEO6","colab_type":"text"},"source":["Agora nós vamos criar dois modelos de SVM. Será um modelo não linear com kernel polinomial e um modelo não linear com kernel RBF. Vamos iniciar com o **kernel polinomial**.\n"]},{"cell_type":"code","metadata":{"id":"3hInUoC5YE8p","colab_type":"code","colab":{}},"source":["# Definição do algoritmo e passagem de parâmetros. O kernel polinomial, além do grau, tem como parâmetro gamma. Neste caso, nós deixaremos gamma como o valor default.\n","\n","modelo_polinomial = svm.SVC(kernel='poly', degree=2\n","                            , gamma='auto', C=10) # Kernel Polinomial com parâmetro de regularização C = 10\n","\n","#Vamos treinar o modelo usando a base X_Train\n","\n","modelo_polinomial.fit(X_train, y_train)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cG4bjkGZ7Rr","colab_type":"code","colab":{}},"source":["#Vamos fazer as predições para a base e teste\n","\n","y_pred_poli = modelo_polinomial.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zdKOPw-csd5k","colab_type":"text"},"source":["Avaliação de modelos"]},{"cell_type":"code","metadata":{"id":"wa01dtoraXon","colab_type":"code","colab":{}},"source":["#Importando o módulo de cálculo de métricas de avaliação do scikit-learn \n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","\n","# Avaliação do modelo\n","\n","print(confusion_matrix(y_test,y_pred_poli))\n","print(classification_report(y_test,y_pred_poli))  \n","print(\"Acurácia Polinomial:\", accuracy_score(y_test, y_pred_poli))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4sSUBR9iWp7U","colab_type":"text"},"source":["**Atividade**\n","\n","1. Tente ajustar o parâmetro relacionado ao grau do polinômio do kernel polinomial e responda: É possível melhorar o resultado? Obs: O grau do polinômio é um valor discreto, portanto, você pode utilizar valores como 2, 3, 4, etc. O valor de C já está superficialmente otimizado. Portanto, você não precisa modificar o valor de C.\n","2. Utilize o kernel rfb e faça ajustes no parâmetro gamma. Qual tipo de kernel apresentou melhor resultado? Obs: o valor de gamma é contínuo. Novamente, você não precisa modificar o valor de C.\n","3. Compare os resultados obtidos por SVM aos resultados obtidos por kNN, Naive Bayes e Árvores de Decisão. Qual dos algoritmos de Aprendizagem de Máquina testados apresentou melhor resultado?\n"]},{"cell_type":"code","metadata":{"id":"zojRGrmKsNvl","colab_type":"code","colab":{}},"source":["#Gerando o modelo com kernel RBF\n","\n","modelo_rbf = svm.SVC(kernel='rbf', degree=0.7, C=10) # Kernel RBF com parâmetro de regularização C = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qugBxW0jZRT9","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}