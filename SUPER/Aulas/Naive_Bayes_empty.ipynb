{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rbJczq-zZ_e"
   },
   "source": [
    "Os métodos **Naive** (ingênuos) **Bayes** são um conjunto de algoritmos de aprendizado supervisionado com base na aplicação do teorema de Bayes com a suposição \"ingênua\" de independência condicional entre cada par de features. \n",
    "\n",
    "Dada a variável $y$ (classes) e o vetor de características $[x_1,x_2,...x_n]$, o teorema de Bayes afirma que:\n",
    "\n",
    "$P(y|x_1,x_2,...x_n) = \\frac{P(y)P(x_1,x_2,...x_n|y)}{P(x_1,x_2,...x_n)}$\n",
    "\n",
    "Supondo de indepencia condicional temos:\n",
    "\n",
    "$P(y|x_1,x_2,...x_n) = \\frac{P(y) \\prod_i^n  P(x_i|y)}{P(x_1,x_2,...x_n)}$\n",
    "\n",
    "Finalmente, temos que a regra de classificação para várias classes resulta:\n",
    "\n",
    "$\\hat{y} = \\text{argmax}_y \\ P(y) \\prod_i^n  P(x_i|y)$\n",
    "\n",
    "**Métodos existentes:**\n",
    "\n",
    "1. Gaussian Naive Bayes (dados reais)\n",
    "2. Multinomial Naive Bayes (texto, $\\theta_y = (\\theta_{y1},\\theta_{y2},\\theta_{y3},...\\theta_{yn})$)\n",
    "3. Bernoulli Naive Bayes (colunas de features binárias)\n",
    "4. Categorical Naive Bayes (asume que cada feature segue uma distribuição em categorias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtXX_JSA5car"
   },
   "source": [
    "**Gaussian NB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Xbsg9Hb7uEZ"
   },
   "outputs": [],
   "source": [
    "# imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o11XYFYMqO9x"
   },
   "source": [
    "Talvez o classificador Gaussian NB seja o mais fácil de entender. Nesse classificador, supõe-se que os dados de cada rótulo sejam extraídos de uma distribuição gaussiana simples. Imagine que você tem os seguintes dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNwhOjAUqGLH"
   },
   "outputs": [],
   "source": [
    "# gerando um dataset toy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYZQZlBxLogw"
   },
   "source": [
    "Distribuiçãão Gaussiana:\n",
    "\n",
    "$f(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\mathrm{e}^{- \\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$\n",
    "\n",
    "Uma para cada classe, lembrando que desejamos calcular $P(x_i|y_c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtwpJqBWqUZ5"
   },
   "outputs": [],
   "source": [
    "# Explorando as superficies gausianas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmAwN59XmpC_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPX4mGP0HuTe"
   },
   "source": [
    "Com isto poderiamos criar a seguinte regra de decisão:\n",
    "\n",
    "$\\frac{P(l_1|x_1,x_2)}{P(l_2|x_1,x_2)} = \\frac{P(l_1)P(x_1,x_2|l_1)}{P(l_2)P(x_1,x_2|l_2)}$\n",
    "\n",
    "Mas, no lugar de fazer manualmente vamos usar o método do Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BboYiKotQww"
   },
   "outputs": [],
   "source": [
    "# teinando um NB Gaussiano\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQhCFt7D7mxH"
   },
   "source": [
    "**Discretzação de dados contínuos em intervalos (bins + one-hot)**\n",
    "\n",
    "A discretização (também conhecida como quantização ou **binning**) é uma maneira de particionar features contínuas em valores discretos. \n",
    "\n",
    "Certos conjuntos de dados com features contínuas podem se beneficiar da discretização porque a discretização pode transformar em atributos nominais, e com isso introduzir não-linearidade nos modelos lineares.\n",
    "\n",
    "As features discretizadas são codificados em um vetor binário chamado **one-hot**.\n",
    "\n",
    "Não confundir com Feature binarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BG-VT5DY9weT"
   },
   "outputs": [],
   "source": [
    "# discretizador \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANRGRGOt91Rk"
   },
   "outputs": [],
   "source": [
    "# podemos ver os intervalos dos bins e inferir que o métdo teve que escalar os \n",
    "# dados antes da transormação, senão iam cair todos no mesmo bin.\n",
    "\n",
    "# est.bin_edges_\n",
    "# est.inverse_transform(Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-PdfIxSJnxu"
   },
   "source": [
    "A regra de decisão para o Bernoulli NB é baseada em:\n",
    "\n",
    "$P(x_i|y) = P(i|y)x_i + (1-P(i|y)(1-x_i))$,\n",
    "\n",
    "onde $i$ é a feature.\n",
    "\n",
    "O que difere da regra do NB multinomial, pois penaliza explicitamente a não ocorrência de uma feature para a classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irr8lxDyAX6D"
   },
   "outputs": [],
   "source": [
    "# treinando um NB Bernoulli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHsN2tvwBBIN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcPO9mqdShYq"
   },
   "source": [
    "**Quando utilizar NB**\n",
    "\n",
    "Como os classificadores bayesianos fazem suposições rigorosas sobre os dados, eles geralmente não apresentam um desempenho tão bom quanto um modelo mais complexo. \n",
    "\n",
    "**Vantagens:**\n",
    "\n",
    "  1. são extremamente rápidos para treinamento e previsão\n",
    "  2. fornecem previsão probabilística direta\n",
    "  3. são frequentemente muito facilmente interpretáveis\n",
    "  4. têm muito poucos (se houver) parâmetros ajustáveis\n",
    "\n",
    "Essas vantagens significam que um classificador bayesiano costuma ser uma boa escolha como classificação inicial baseline. Se funcionar adequadamente, parabéns: você tem um classificador muito rápido e muito interpretável para o seu problema. Se não funcionar bem, você poderá começar a explorar modelos mais sofisticados.\n",
    "\n",
    "**Desempenho especialmente bom nas seguintes situações:**\n",
    "\n",
    "1. Quando as suposições realmente coincidem com os dados (muito raro na prática)\n",
    "2. Para classes linearmente separáveis (quando a complexidade do modelo é menos importante)\n",
    "3. Para dados de alta dimensão (quando a complexidade do modelo é menos importante)\n",
    "\n",
    "Os dois últimos pontos parecem distintos, mas na verdade estão relacionados: à medida que a dimensão de um conjunto de dados aumenta, é muito menos provável que dois pontos sejam encontrados próximos (afinal, eles devem estar próximos em todas as dimensões para estarem próximos no geral). \n",
    "\n",
    "Isso significa que, no geral, grupos de dadas em dimensões elevadas tendem a ser mais separváveis do que os grupos em dimensões baixas (sempre que cada feature contribua com informações novas). Por esse motivo, classificadores simples como Bayes tendem a funcionar tão bem quanto classificadores complexos à medida que a dimensionalidade cresce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AS1ms_HXT_VV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive Bayes_empty",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
