{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision tree_empty.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ctz4uhA0Phj",
        "colab_type": "text"
      },
      "source": [
        "As árvores de decisão são um tipo importante de algoritmo para o aprendizado de \n",
        "máquina de modelagem preditiva^1.\n",
        "\n",
        "\n",
        "Os algoritmos clássicos de árvore de decisão existem há décadas (ex. ID3, C4.5, CART, etc). Podem ser utilizados para \n",
        "\n",
        "1.   Classificação\n",
        "2.   Regressão\n",
        "\n",
        "Algumas variantes mais modernas, tais como Random Forest, extendem o poder deste tipo de preditores.\n",
        "\n",
        "\n",
        "A representação para o modelo **CART é uma árvore binária**.\n",
        "\n",
        "1. Cada nó raiz representa uma única variável de entrada (x) e um ponto de divisão nessa variável (assumindo que a variável seja numérica).\n",
        "2. Os nós das folhas da árvore contêm uma variável de saída (y) que é usada para fazer uma previsão.\n",
        "\n",
        "\n",
        "1. A modelagem preditiva é um processo que usa dados e estatísticas para prever resultados com modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44i7hVSkH6YD",
        "colab_type": "text"
      },
      "source": [
        "**Exemplo:**\n",
        "\n",
        "Sobrevivência dos passageiros no Titanic. Os valores nas folhas mostram a probabilidade de sobrevivência e a porcentagem de instâncias cobertas pela regra.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg)\n",
        "\n",
        "Resumindo: Suas chances de sobrevivência eram boas se você fosse (i) uma mulher ou (ii) um homem com menos de 9,5 anos e estritamente menos de três irmãos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqWtU86YD3_z",
        "colab_type": "text"
      },
      "source": [
        "**Vantagens:**\n",
        "\n",
        "1. Rápido e eficiente (baixo custo computacional, log_2(features))\n",
        "2. Simples de entender e interpretar. Árvores podem ser visualizadas.\n",
        "3.   São modelos autoexplicativos (ranking de regras e features, \"caixa branca\")\n",
        "4. Capaz de lidar com dados numéricos e categóricos (outras técnicas conseguem lidar com apenas um tipo de variável).\n",
        "5.   O algoritmo CART fornece uma base para algoritmos importantes, como bagging.\n",
        "6. Extensão multioutput trivial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S56vD4uJEax3",
        "colab_type": "text"
      },
      "source": [
        "**Desvantagens:**\n",
        "\n",
        "1. Criação de árvores super complexas que não generalizam bem os dados (overfiting). Solução: podar a árvore, limitar o crescimento, etc.\n",
        "2. Tendencia a representar melhor as classes dominantes.\n",
        "3. As árvores de decisão podem ser instáveis ​​porque pequenas variações nos dados podem resultar na geração de uma árvore completamente diferente.\n",
        "4. Árvore de decisão são baseados em algoritmos heurísticos (greedy, decisões localmente são tomadas em cada nó).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3kJKeUIxmwv",
        "colab_type": "text"
      },
      "source": [
        "**Teoria - Greedy Splitting**\n",
        "\n",
        "Criar uma árvore de decisão **binária** é um processo de dividir o espaço de entrada em splits binários de forma recursova. Uma abordagem para escolher a feature que será utilizada no split é adotado um método guloso.\n",
        "\n",
        "A avaliação gulosa requer que exista uma métrica chamada custo. O custo de cada feature é avaliado e aquela com menor custo é escolhida para realizar o split.\n",
        "\n",
        "---\n",
        "\n",
        "Para problemas de modelagem preditiva de regressão, a função de custo minimizada para escolher pontos de divisão é a soma dos erros ao quadrado:\n",
        "\n",
        "$ e = \\sum(y - prediction)^2$\n",
        "\n",
        "onde y é o valor da variable de saída na regressão.\n",
        "\n",
        "---\n",
        "\n",
        "Para problemas de **classificação** é utilizado o **coeficiente de Gini**, que fornece uma indicativo de quão **\"puros\"** são os nós das folhas (quão misturados são os dados de treinamento atribuídos a cada nó).\n",
        "\n",
        "$G = \\sum(p_k * (1 – p_k))$\n",
        "\n",
        "onde $G$ é o Gini para todas as classes, $p_k$ é a proporção de instancias em cada classe $k$ cobertas pela regra de split. \n",
        "\n",
        "Um nodo no qual todas as amostras são da mesma classe (máxima \"purity\") terá $G=0$, entretanto um nó com 50-50 split em duas classes terá $G=0.5$.\n",
        "\n",
        "Se o problema for classificação binária, então podemos reescrever Gini da seguinte forma:\n",
        "\n",
        "$ G = 2 * p_1 * p_2 = 1 - (p_1^2 + p_2^2)$\n",
        "\n",
        "O cálculo do índice Gini para cada nó é ponderado pelo número total de instâncias no nó pai. O valor final de Gini para um ponto de divisão escolhido em um problema de classificação binária é, portanto, calculada da seguinte maneira:\n",
        "\n",
        "$G = ((1 - (g1_1^2 + g1_2^2)) * (ng1/n)) + ((1 - (g2_1^2 + g2_2^2)) * (ng2/n))$\n",
        "\n",
        "onde $g1_1$ é a proporção de instâncias no grupo 1 para a classe 1, $g1_2$ para a classe 2, $g2_1$ para o grupo 2 e classe 1, $g2_2$, grupo 2, classe 2, $ng1$ e $ng2$ são o número total das instâncias nos grupos 1 e 2 e $n$ é o número total de instâncias que estamos tentando agrupar no nó pai."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36OpmSCwz_wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDSVFYqqIuSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tree"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzrsnTeeLwXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualização\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjSKYk5rMJQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature mais importantes\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwBg1F60YrRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mean accuracy\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6SmIhYTTIx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# função de decisão"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVAnOG1HNyI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "4c64c41d-b8a9-442b-b309-ca3db1f16769"
      },
      "source": [
        "# Examinando a melhor regra \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "n_classes = 3\n",
        "plot_colors = \"ryb\"\n",
        "plot_step = 0.02\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "\n",
        "plt.figure(figsize=(15,12))\n",
        "\n",
        "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
        "                                [1, 2], [1, 3], [2, 3]]):\n",
        "    # We only take the two corresponding features\n",
        "    X = iris.data[:, pair]\n",
        "    y = iris.target\n",
        "\n",
        "    # Train\n",
        "    DT = DecisionTreeClassifier().fit(X, y)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.subplot(2, 3, pairidx + 1)\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                         np.arange(y_min, y_max, plot_step))\n",
        "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
        "\n",
        "    Z = DT.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "    plt.xlabel(iris.feature_names[pair[0]])\n",
        "    plt.ylabel(iris.feature_names[pair[1]])\n",
        "\n",
        "    # Plot the training points\n",
        "    for i, color in zip(range(n_classes), plot_colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
        "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
        "\n",
        "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
        "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
        "plt.axis(\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c0dec66ce27f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0miris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_iris' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMu7KRxLai7K",
        "colab_type": "text"
      },
      "source": [
        "**Avaliando a regularização do pruning (poda)**\n",
        "\n",
        "O DecisionTreeClassifier fornece parâmetros como min_samples_leaf e max_depth para impedir overfiting. O Pruning fornece outra opção para controlar o tamanho de uma árvore. \n",
        "\n",
        "* No DecisionTreeClassifier, essa técnica de remoção é parametrizada pelo parâmetro **ccp_alpha**. \n",
        "* Valores elevados de ccp_alpha aumentam o número de nós removidos. \n",
        "\n",
        "\n",
        "**Minimal cost-complexity pruning** é um algoritmo utilizado para podar uma árvore e evitar overfitting\n",
        "\n",
        "Esse algoritmo é parametrizado pelo parâmetro de complexidade $\\alpha >= 0$. O objetivo da poda é encontrar uma árvore $T$ que minimiza:\n",
        "\n",
        "$R_{\\alpha}(T) = R(T) + \\alpha |T|$,\n",
        "\n",
        "onde $|T|$ é o número de nós folhas e $R(T)$ é a taxa total de classificação incorreta dos nós terminais. \n",
        "\n",
        "\n",
        "Vamos entender o efeito deste parâmetro na regularização das árvores e como escolher um ccp_alpha com base nos scores de validação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNyDIasQSaVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evitando overfitting"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmrmxE8CcRNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nodes vs alpha, Depth vs alpha\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WrX6-PKdAq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# acuracy vs alpha\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIHNoqoEiKmn",
        "colab_type": "text"
      },
      "source": [
        "Resultado esperado em outro dataset\n",
        "\n",
        "![exemplo em outro dataset](https://scikit-learn.org/stable/_images/sphx_glr_plot_cost_complexity_pruning_003.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj7b-Ci2WAHS",
        "colab_type": "text"
      },
      "source": [
        "**Grid search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f4ik0iMxmBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ajuste de parâmetros\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oO3HQBMZSEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# examinando resultados\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tembxOETXLxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# o score é a acuracia"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGj4V-RYZbf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ranking das combinações de parâmetros\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLJCI86HaTgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# verificar a melhor combinação\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3qI8qUoesE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}