{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A maioria dos conceitos utilizados neste notebook foi extraído de <a href= \"https://github.com/WJMatthew/WESAD/blob/master/m15_model_cv-binary.ipynb\"> WJMatthew </a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as scisig\n",
    "import scipy.stats\n",
    "import cvxEDA\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4 (wrist) Sampling Frequencies\n",
    "fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700}\n",
    "WINDOW_IN_SECONDS = 30\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "feat_names = None\n",
    "savePath = r'../../Datasets/WESAD/'\n",
    "subject_feature_path = '../../Datasets/WESAD/subject_feats'\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "if not os.path.exists(savePath + subject_feature_path):\n",
    "    os.makedirs(savePath + subject_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvxEDA\n",
    "def eda_stats(y):\n",
    "    Fs = fs_dict['EDA']\n",
    "    yn = (y - y.mean()) / y.std()\n",
    "    [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "    return [r, p, t, l, d, e, obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atributos escolhidos para classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectData:\n",
    "\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        data = self.data['signal']['wrist']\n",
    "        data.update({'Resp': self.data['signal']['chest']['Resp']})\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        return self.data['signal']['chest']\n",
    "\n",
    "    def extract_features(self):  # only wrist\n",
    "        results = \\\n",
    "            {\n",
    "                key: get_statistics(self.get_wrist_data()[key].flatten(), self.labels, key) for key in self.wrist_keys\n",
    "            }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipo de filtragem utilizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/load_files.py\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = scisig.butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    # Filtering Helper functions\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = scisig.lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def get_slope(series):\n",
    "    linreg = scipy.stats.linregress(np.arange(len(series)), series )\n",
    "    slope = linreg[0]\n",
    "    return slope\n",
    "\n",
    "def get_window_stats(data, label=-1):\n",
    "    mean_features = np.mean(data)\n",
    "    std_features = np.std(data)\n",
    "    min_features = np.amin(data)\n",
    "    max_features = np.amax(data)\n",
    "\n",
    "    features = {'mean': mean_features, 'std': std_features, 'min': min_features, 'max': max_features,\n",
    "                'label': label}\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_net_accel(data):\n",
    "    \"\"\"\n",
    "    Aplica o RMS - Root Mean Square \n",
    "    \"\"\"\n",
    "    return (data['ACC_x'] ** 2 + data['ACC_y'] ** 2 + data['ACC_z'] ** 2).apply(lambda x: np.sqrt(x))\n",
    "\n",
    "\n",
    "def get_peak_freq(x):\n",
    "    f, Pxx = scisig.periodogram(x, fs=8)\n",
    "    psd_dict = {amp: freq for amp, freq in zip(Pxx, f)}\n",
    "    peak_freq = psd_dict[max(psd_dict.keys())]\n",
    "    return peak_freq\n",
    "\n",
    "\n",
    "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/AccelerometerFeatureExtractionScript.py\n",
    "## estudar este filtro: Finite Impulse Response\n",
    "def filterSignalFIR(eda, cutoff=0.4, numtaps=64):\n",
    "    f = cutoff / (fs_dict['ACC'] / 2.0)\n",
    "    FIR_coeff = scisig.firwin(numtaps, f)\n",
    "\n",
    "    return scisig.lfilter(FIR_coeff, 1, eda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes unitários das funções - INICIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make subject data object for Sx\n",
    "subject = SubjectData(main_path=savePath, subject_number=2)\n",
    "\n",
    "# Empatica E4 data - now with resp\n",
    "e4_data_dict = subject.get_wrist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = pd.DataFrame(e4_data_dict['EDA'], columns=['EDA'])\n",
    "bvp_df = pd.DataFrame(e4_data_dict['BVP'], columns=['BVP'])\n",
    "acc_df = pd.DataFrame(e4_data_dict['ACC'], columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "temp_df = pd.DataFrame(e4_data_dict['TEMP'], columns=['TEMP'])\n",
    "label_df = pd.DataFrame(subject.labels, columns=['label'])\n",
    "resp_df = pd.DataFrame(e4_data_dict['Resp'], columns=['Resp'])\n",
    "\n",
    "## Aplica filtragem apenas no EDA e ACC\n",
    "# Filter EDA: Pq ele usa esses valores de corte e ordem?\n",
    "eda_df['EDA'] = butter_lowpass_filter(eda_df['EDA'], 1.0, fs_dict['EDA'], 6)\n",
    "\n",
    "# Filter ACM: Acelerometer\n",
    "for coluna in acc_df.columns:\n",
    "    acc_df[coluna] = filterSignalFIR(acc_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24316, 1), (389056, 1), (194528, 3), (24316, 1), (4255300, 1), (4255300, 1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_df.shape, bvp_df.shape, acc_df.shape, temp_df.shape, label_df.shape, resp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64Index([    0.0,    0.25,     0.5,    0.75,     1.0,    1.25,     1.5,\n",
       "                 1.75,     2.0,    2.25,\n",
       "              ...\n",
       "               6076.5, 6076.75,  6077.0, 6077.25,  6077.5, 6077.75,  6078.0,\n",
       "              6078.25,  6078.5, 6078.75],\n",
       "             dtype='float64', length=24316)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_df.index = [(1 / fs_dict['EDA']) * i for i in range(len(eda_df))]\n",
    "eda_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([     0,      1,      2,      3,      4,      5,      6,      7,\n",
       "                 8,      9,\n",
       "            ...\n",
       "            389046, 389047, 389048, 389049, 389050, 389051, 389052, 389053,\n",
       "            389054, 389055],\n",
       "           dtype='int64', length=389056)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bvp_df.index = [1 * i for i in range(len(bvp_df))]\n",
    "bvp_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1970-01-01 00:00:00.000</td>\n",
       "      <td>0.033679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970-01-01 00:00:00.250</td>\n",
       "      <td>0.235374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970-01-01 00:00:00.500</td>\n",
       "      <td>0.708718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970-01-01 00:00:00.750</td>\n",
       "      <td>1.200153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970-01-01 00:00:01.000</td>\n",
       "      <td>1.276165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970-01-01 01:41:17.750</td>\n",
       "      <td>0.062091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970-01-01 01:41:18.000</td>\n",
       "      <td>0.063236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970-01-01 01:41:18.250</td>\n",
       "      <td>0.063271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970-01-01 01:41:18.500</td>\n",
       "      <td>0.061811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970-01-01 01:41:18.750</td>\n",
       "      <td>0.061245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24316 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              EDA\n",
       "1970-01-01 00:00:00.000  0.033679\n",
       "1970-01-01 00:00:00.250  0.235374\n",
       "1970-01-01 00:00:00.500  0.708718\n",
       "1970-01-01 00:00:00.750  1.200153\n",
       "1970-01-01 00:00:01.000  1.276165\n",
       "...                           ...\n",
       "1970-01-01 01:41:17.750  0.062091\n",
       "1970-01-01 01:41:18.000  0.063236\n",
       "1970-01-01 01:41:18.250  0.063271\n",
       "1970-01-01 01:41:18.500  0.061811\n",
       "1970-01-01 01:41:18.750  0.061245\n",
       "\n",
       "[24316 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_df.index = pd.to_datetime(eda_df.index, unit='s')\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.033679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.235374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.708718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.200153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.276165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24311</td>\n",
       "      <td>0.062091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24312</td>\n",
       "      <td>0.063236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24313</td>\n",
       "      <td>0.063271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24314</td>\n",
       "      <td>0.061811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24315</td>\n",
       "      <td>0.061245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24316 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EDA\n",
       "0      0.033679\n",
       "1      0.235374\n",
       "2      0.708718\n",
       "3      1.200153\n",
       "4      1.276165\n",
       "...         ...\n",
       "24311  0.062091\n",
       "24312  0.063236\n",
       "24313  0.063271\n",
       "24314  0.061811\n",
       "24315  0.061245\n",
       "\n",
       "[24316 rows x 1 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_df.reset_index(drop=True, inplace=True)\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding indices for combination due to differing sampling frequencies\n",
    "# eda_df.index = [(1 / fs_dict['EDA']) * i for i in range(len(eda_df))]\n",
    "# bvp_df.index = [(1 / fs_dict['BVP']) * i for i in range(len(bvp_df))]\n",
    "# acc_df.index = [(1 / fs_dict['ACC']) * i for i in range(len(acc_df))]\n",
    "# temp_df.index = [(1 / fs_dict['TEMP']) * i for i in range(len(temp_df))]\n",
    "# label_df.index = [(1 / fs_dict['label']) * i for i in range(len(label_df))]\n",
    "# resp_df.index = [(1 / fs_dict['Resp']) * i for i in range(len(resp_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eda_df.join(bvp_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.033679</td>\n",
       "      <td>-59.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.235374</td>\n",
       "      <td>-53.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.708718</td>\n",
       "      <td>-44.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.200153</td>\n",
       "      <td>-33.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.276165</td>\n",
       "      <td>-20.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>389056 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             EDA    BVP\n",
       "0       0.033679 -59.37\n",
       "1       0.235374 -53.42\n",
       "2       0.708718 -44.40\n",
       "3       1.200153 -33.17\n",
       "4       1.276165 -20.79\n",
       "...          ...    ...\n",
       "389051       NaN  18.33\n",
       "389052       NaN  18.26\n",
       "389053       NaN  18.26\n",
       "389054       NaN  18.68\n",
       "389055       NaN  19.71\n",
       "\n",
       "[389056 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes unitários das funções - FIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computa as características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(e4_data_dict, labels, norm_type=None):\n",
    "    # Dataframes for each sensor type\n",
    "    eda_df = pd.DataFrame(e4_data_dict['EDA'], columns=['EDA'])\n",
    "    bvp_df = pd.DataFrame(e4_data_dict['BVP'], columns=['BVP'])\n",
    "    acc_df = pd.DataFrame(e4_data_dict['ACC'], columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
    "    temp_df = pd.DataFrame(e4_data_dict['TEMP'], columns=['TEMP'])\n",
    "    label_df = pd.DataFrame(labels, columns=['label'])\n",
    "    resp_df = pd.DataFrame(e4_data_dict['Resp'], columns=['Resp'])\n",
    "\n",
    "    ## Aplica filtragem apenas no EDA e ACC\n",
    "    # Filter EDA: Pq ele usa esses valores de corte e ordem?\n",
    "    eda_df['EDA'] = butter_lowpass_filter(eda_df['EDA'], 1.0, fs_dict['EDA'], 6)\n",
    "\n",
    "    # Filter ACM: Acelerometer\n",
    "    for coluna in acc_df.columns:\n",
    "        acc_df[coluna] = filterSignalFIR(acc_df.values)\n",
    "\n",
    "    # Adding indices for combination due to differing sampling frequencies\n",
    "    ## prepara o index com base na fz da frequencia de cada sinal\n",
    "    eda_df.index = [(1 / fs_dict['EDA']) * i for i in range(len(eda_df))]\n",
    "    bvp_df.index = [(1 / fs_dict['BVP']) * i for i in range(len(bvp_df))]\n",
    "    acc_df.index = [(1 / fs_dict['ACC']) * i for i in range(len(acc_df))]\n",
    "    temp_df.index = [(1 / fs_dict['TEMP']) * i for i in range(len(temp_df))]\n",
    "    label_df.index = [(1 / fs_dict['label']) * i for i in range(len(label_df))]\n",
    "    resp_df.index = [(1 / fs_dict['Resp']) * i for i in range(len(resp_df))]\n",
    "\n",
    "    # Change indices to datetime\n",
    "    ## Converte o intex das amostras em timestamp \n",
    "    eda_df.index = pd.to_datetime(eda_df.index, unit='s')\n",
    "    bvp_df.index = pd.to_datetime(bvp_df.index, unit='s')\n",
    "    temp_df.index = pd.to_datetime(temp_df.index, unit='s')\n",
    "    acc_df.index = pd.to_datetime(acc_df.index, unit='s')\n",
    "    label_df.index = pd.to_datetime(label_df.index, unit='s')\n",
    "    resp_df.index = pd.to_datetime(resp_df.index, unit='s')\n",
    "\n",
    "    # New EDA features\n",
    "    r, p, t, l, d, e, obj = eda_stats(eda_df['EDA'])\n",
    "    eda_df['EDA_phasic'] = r\n",
    "    eda_df['EDA_smna'] = p\n",
    "    eda_df['EDA_tonic'] = t\n",
    "        \n",
    "    # Combined dataframe - not used yet\n",
    "    df = eda_df.join(bvp_df, how='outer')\n",
    "    df = df.join(temp_df, how='outer')\n",
    "    df = df.join(acc_df, how='outer')\n",
    "    df = df.join(resp_df, how='outer')\n",
    "    df = df.join(label_df, how='outer')\n",
    "    df['label'] = df['label'].fillna(method='bfill')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if norm_type is 'std':\n",
    "        # std norm\n",
    "        df = (df - df.mean()) / df.std()\n",
    "    elif norm_type is 'minmax':\n",
    "        # minmax norm\n",
    "        df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    # Groupby\n",
    "    grouped = df.groupby('label')\n",
    "    baseline = grouped.get_group(1)\n",
    "    stress = grouped.get_group(2)\n",
    "    amusement = grouped.get_group(3)\n",
    "    return grouped, baseline, stress, amusement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amostras para uma janela - entender melhor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(data, n_windows, label):\n",
    "    global feat_names\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    samples = []\n",
    "    # Using label freq (700 Hz) as our reference frequency due to it being the largest\n",
    "    # and thus encompassing the lesser ones in its resolution.\n",
    "    # define o tamanho do segmento.\n",
    "    window_len = fs_dict['label'] * WINDOW_IN_SECONDS \n",
    "\n",
    "    for i in range(n_windows):\n",
    "        # Get window of data\n",
    "        w = data[window_len * i: window_len * (i + 1)]\n",
    "\n",
    "        # Add/Calc rms acc\n",
    "        # w['net_acc'] = get_net_accel(w)\n",
    "        # anexa mais uma coluna, com os rms do acelerômetro\n",
    "        w = pd.concat([w, get_net_accel(w)])\n",
    "        #w.columns = ['net_acc', 'ACC_x', 'ACC_y', 'ACC_z', 'BVP',\n",
    "          #           'EDA', 'EDA_phasic', 'EDA_smna', 'EDA_tonic', 'TEMP',\n",
    "            #         'label']\n",
    "        cols = list(w.columns)\n",
    "        cols[0] = 'net_acc'\n",
    "        w.columns = cols\n",
    "        \n",
    "        # Calculate stats for window: media, std, min, max - features sobre cada janela\n",
    "        wstats = get_window_stats(data=w, label=label)\n",
    "\n",
    "        # Seperating sample and label\n",
    "        x = pd.DataFrame(wstats).drop('label', axis=0)\n",
    "        y = x['label'][0]\n",
    "        x.drop('label', axis=1, inplace=True)\n",
    "\n",
    "        if feat_names is None:\n",
    "            feat_names = []\n",
    "            for row in x.index:\n",
    "                for col in x.columns:\n",
    "                    feat_names.append('_'.join([row, col]))\n",
    "\n",
    "        # sample df\n",
    "        wdf = pd.DataFrame(x.values.flatten()).T\n",
    "        wdf.columns = feat_names\n",
    "        wdf = pd.concat([wdf, pd.DataFrame({'label': y}, index=[0])], axis=1)\n",
    "        \n",
    "        # More feats\n",
    "        wdf['BVP_peak_freq'] = get_peak_freq(w['BVP'].dropna())\n",
    "        wdf['TEMP_slope'] = get_slope(w['TEMP'].dropna())\n",
    "        samples.append(wdf)\n",
    "\n",
    "    return pd.concat(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construção dos dados dos pacientes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patient_data(subject_id):\n",
    "    global savePath\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    # Make subject data object for Sx\n",
    "    subject = SubjectData(main_path=savePath, subject_number=subject_id)\n",
    "\n",
    "    # Empatica E4 data - now with resp\n",
    "    e4_data_dict = subject.get_wrist_data()\n",
    "\n",
    "    # norm type\n",
    "    norm_type = None\n",
    "\n",
    "    # The 3 classes we are classifying\n",
    "    grouped, baseline, stress, amusement = compute_features(e4_data_dict, subject.labels, norm_type)\n",
    "\n",
    "    # print(f'Available windows for {subject.name}:')\n",
    "    n_baseline_wdws = int(len(baseline) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    n_stress_wdws = int(len(stress) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    n_amusement_wdws = int(len(amusement) / (fs_dict['label'] * WINDOW_IN_SECONDS))\n",
    "    # print(f'Baseline: {n_baseline_wdws}\\nStress: {n_stress_wdws}\\nAmusement: {n_amusement_wdws}\\n')\n",
    "\n",
    "    #\n",
    "    baseline_samples = get_samples(baseline, n_baseline_wdws, 1)\n",
    "    # Downsampling\n",
    "    # baseline_samples = baseline_samples[::2]\n",
    "    stress_samples = get_samples(stress, n_stress_wdws, 2)\n",
    "    amusement_samples = get_samples(amusement, n_amusement_wdws, 0)\n",
    "\n",
    "    all_samples = pd.concat([baseline_samples, stress_samples, amusement_samples])\n",
    "    all_samples = pd.concat([all_samples.drop('label', axis=1), pd.get_dummies(all_samples['label'])], axis=1)\n",
    "    # Selected Features\n",
    "    # all_samples = all_samples[['EDA_mean', 'EDA_std', 'EDA_min', 'EDA_max',\n",
    "    #                          'BVP_mean', 'BVP_std', 'BVP_min', 'BVP_max',\n",
    "    #                        'TEMP_mean', 'TEMP_std', 'TEMP_min', 'TEMP_max',\n",
    "    #                        'net_acc_mean', 'net_acc_std', 'net_acc_min', 'net_acc_max',\n",
    "    #                        0, 1, 2]]\n",
    "    # Save file as csv (for now)\n",
    "    all_samples.to_csv(f'{savePath}{subject_feature_path}/S{subject_id}_feats_4.csv')\n",
    "\n",
    "    # Does this save any space?\n",
    "    subject = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cria apenas um csv com os dados anexados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(subjects):\n",
    "    df_list = []\n",
    "    for s in subjects:\n",
    "        df = pd.read_csv(f'{savePath}{subject_feature_path}/S{s}_feats_4.csv', index_col=0)\n",
    "        df['subject'] = s\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(lambda x: x.index('1'))\n",
    "    df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.to_csv(f'{savePath}/may14_feats4.csv')\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "    print('Number of samples per class:')\n",
    "    for label, number in zip(counts.index, counts.values):\n",
    "        print(f'{int_to_label[label]}: {number}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução de todo o código acima "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "\n",
    "for patient in subject_ids:\n",
    "    print(f'Processing data for S{patient}...')\n",
    "    make_patient_data(patient)\n",
    "\n",
    "combine_files(subject_ids)\n",
    "print('Processing complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start \n",
    "## Serão realizadas as segunites operações:\n",
    "* Carregamento dos dados das operações acima\n",
    "* Implementação da divisão da base em LOSO - leave one out subject one\n",
    "* Criação de uma modelo inicial para a rede\n",
    "\n",
    "#### * Os dados não foram tratados como série temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento dos dados gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Datasets/WESAD/\n"
     ]
    }
   ],
   "source": [
    "print(savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "feats =   ['BVP_mean', 'BVP_std', 'BVP_min', 'BVP_max',\n",
    "           'EDA_phasic_mean', 'EDA_phasic_std', 'EDA_phasic_min', 'EDA_phasic_max', 'EDA_smna_mean',\n",
    "           'EDA_smna_std', 'EDA_smna_min', 'EDA_smna_max', 'EDA_tonic_mean',\n",
    "           'EDA_tonic_std', 'EDA_tonic_min', 'EDA_tonic_max', 'Resp_mean',\n",
    "           'Resp_std', 'Resp_min', 'Resp_max', 'TEMP_mean', 'TEMP_std', 'TEMP_min',\n",
    "           'TEMP_max', 'BVP_peak_freq', 'TEMP_slope', 'subject', 'label']\n",
    "layer_1_dim = len(feats) -2\n",
    "print(layer_1_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(savePath+'may14_feats4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remoção de attrs que não aparecem em feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'net_acc_mean',\n",
       " 'net_acc_std',\n",
       " 'net_acc_min',\n",
       " 'net_acc_max',\n",
       " 'ACC_x_mean',\n",
       " 'ACC_x_std',\n",
       " 'ACC_x_min',\n",
       " 'ACC_x_max',\n",
       " 'ACC_y_mean',\n",
       " 'ACC_y_std',\n",
       " 'ACC_y_min',\n",
       " 'ACC_y_max',\n",
       " 'ACC_z_mean',\n",
       " 'ACC_z_std',\n",
       " 'ACC_z_min',\n",
       " 'ACC_z_max',\n",
       " 'EDA_mean',\n",
       " 'EDA_std',\n",
       " 'EDA_min',\n",
       " 'EDA_max']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_2_remove = []\n",
    "for i in df.columns:\n",
    "    if i not in feats:\n",
    "        cols_2_remove.append(i)\n",
    "cols_2_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(cols_2_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['BVP_mean', 'BVP_std', 'BVP_min', 'BVP_max', 'EDA_phasic_mean',\n",
       "        'EDA_phasic_std', 'EDA_phasic_min', 'EDA_phasic_max', 'EDA_smna_mean',\n",
       "        'EDA_smna_std', 'EDA_smna_min', 'EDA_smna_max', 'EDA_tonic_mean',\n",
       "        'EDA_tonic_std', 'EDA_tonic_min', 'EDA_tonic_max', 'Resp_mean',\n",
       "        'Resp_std', 'Resp_min', 'Resp_max', 'TEMP_mean', 'TEMP_std', 'TEMP_min',\n",
       "        'TEMP_max', 'BVP_peak_freq', 'TEMP_slope', 'subject', 'label'],\n",
       "       dtype='object'), (1178, 28))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns, df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binariza o label - > relembrar o que significa cada categoria\n",
    "#### * Apenas segui a mesma coisa feita no trabalho base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label(label):\n",
    "    return 0 if (label == 0 or label == 1) else 1\n",
    "    \n",
    "df['label'] = df['label'].apply(change_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separa a base por paciente, onde 'pacientes_teste' serão utilizadas apenas para avaliar o comportamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id dos pacientes contidos na base\n",
    "pacientes = df['subject'].unique().tolist()\n",
    "# escolhe os pacientes que ficarão de fora do treinamento\n",
    "pacientes_teste = np.random.choice(pacientes, 2).tolist()\n",
    "# seleciona apenas os pacientes que estarão no treinamento\n",
    "pacientes = [x for x in pacientes if x not in pacientes_teste]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pacientes para treino < | > pacientes para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16, 17], [15, 7])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pacientes, pacientes_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataframe de rótulos. Facilitou um pouco minha vida. :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['subject', 'label']\n",
    "df_paciente_rotulo = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### criação dos dummies. Neste caso, ficaram apenas duas classes. Talvez nem precisasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>BVP_max</th>\n",
       "      <th>EDA_phasic_mean</th>\n",
       "      <th>EDA_phasic_std</th>\n",
       "      <th>EDA_phasic_min</th>\n",
       "      <th>EDA_phasic_max</th>\n",
       "      <th>EDA_smna_mean</th>\n",
       "      <th>EDA_smna_std</th>\n",
       "      <th>...</th>\n",
       "      <th>Resp_max</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "      <th>subject</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.181673</td>\n",
       "      <td>107.648359</td>\n",
       "      <td>-358.13</td>\n",
       "      <td>554.77</td>\n",
       "      <td>1.824289</td>\n",
       "      <td>1.088328</td>\n",
       "      <td>0.367977</td>\n",
       "      <td>4.319987</td>\n",
       "      <td>1.284376</td>\n",
       "      <td>1.952823</td>\n",
       "      <td>...</td>\n",
       "      <td>6.504822</td>\n",
       "      <td>35.817091</td>\n",
       "      <td>0.012674</td>\n",
       "      <td>35.79</td>\n",
       "      <td>35.84</td>\n",
       "      <td>0.135670</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.830147</td>\n",
       "      <td>118.742089</td>\n",
       "      <td>-392.28</td>\n",
       "      <td>438.16</td>\n",
       "      <td>2.109146</td>\n",
       "      <td>1.223528</td>\n",
       "      <td>0.539150</td>\n",
       "      <td>4.459367</td>\n",
       "      <td>1.467865</td>\n",
       "      <td>2.852510</td>\n",
       "      <td>...</td>\n",
       "      <td>6.742859</td>\n",
       "      <td>35.797568</td>\n",
       "      <td>0.029901</td>\n",
       "      <td>35.75</td>\n",
       "      <td>35.87</td>\n",
       "      <td>0.095023</td>\n",
       "      <td>-0.000789</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>42.190039</td>\n",
       "      <td>-240.61</td>\n",
       "      <td>209.89</td>\n",
       "      <td>0.152828</td>\n",
       "      <td>0.128896</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.544346</td>\n",
       "      <td>0.105091</td>\n",
       "      <td>0.244891</td>\n",
       "      <td>...</td>\n",
       "      <td>3.260803</td>\n",
       "      <td>35.712909</td>\n",
       "      <td>0.027612</td>\n",
       "      <td>35.66</td>\n",
       "      <td>35.75</td>\n",
       "      <td>0.076880</td>\n",
       "      <td>-0.000717</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.107404</td>\n",
       "      <td>41.606872</td>\n",
       "      <td>-289.26</td>\n",
       "      <td>145.36</td>\n",
       "      <td>0.177595</td>\n",
       "      <td>0.126167</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.361388</td>\n",
       "      <td>0.110786</td>\n",
       "      <td>0.199704</td>\n",
       "      <td>...</td>\n",
       "      <td>3.730774</td>\n",
       "      <td>35.700811</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>35.66</td>\n",
       "      <td>35.73</td>\n",
       "      <td>0.140271</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.073620</td>\n",
       "      <td>43.121633</td>\n",
       "      <td>-197.37</td>\n",
       "      <td>194.12</td>\n",
       "      <td>0.035014</td>\n",
       "      <td>0.039616</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.132781</td>\n",
       "      <td>0.026716</td>\n",
       "      <td>0.114738</td>\n",
       "      <td>...</td>\n",
       "      <td>2.912903</td>\n",
       "      <td>35.744727</td>\n",
       "      <td>0.019386</td>\n",
       "      <td>35.71</td>\n",
       "      <td>35.79</td>\n",
       "      <td>0.149321</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>0.111318</td>\n",
       "      <td>9.667118</td>\n",
       "      <td>-83.56</td>\n",
       "      <td>30.05</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>...</td>\n",
       "      <td>6.335449</td>\n",
       "      <td>32.887818</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>32.87</td>\n",
       "      <td>32.91</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1174</td>\n",
       "      <td>0.038265</td>\n",
       "      <td>9.805945</td>\n",
       "      <td>-29.08</td>\n",
       "      <td>51.20</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.006730</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.028034</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.027377</td>\n",
       "      <td>...</td>\n",
       "      <td>6.094360</td>\n",
       "      <td>32.895225</td>\n",
       "      <td>0.012794</td>\n",
       "      <td>32.87</td>\n",
       "      <td>32.91</td>\n",
       "      <td>0.144715</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>-0.096324</td>\n",
       "      <td>9.154524</td>\n",
       "      <td>-63.68</td>\n",
       "      <td>37.41</td>\n",
       "      <td>0.008592</td>\n",
       "      <td>0.006620</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.023813</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.022838</td>\n",
       "      <td>...</td>\n",
       "      <td>19.221497</td>\n",
       "      <td>32.854000</td>\n",
       "      <td>0.022450</td>\n",
       "      <td>32.81</td>\n",
       "      <td>32.89</td>\n",
       "      <td>0.131222</td>\n",
       "      <td>-0.000512</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176</td>\n",
       "      <td>-0.021164</td>\n",
       "      <td>5.935253</td>\n",
       "      <td>-19.01</td>\n",
       "      <td>12.90</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.013490</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.016423</td>\n",
       "      <td>...</td>\n",
       "      <td>6.309509</td>\n",
       "      <td>32.792342</td>\n",
       "      <td>0.020222</td>\n",
       "      <td>32.75</td>\n",
       "      <td>32.83</td>\n",
       "      <td>0.158282</td>\n",
       "      <td>-0.000532</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1177</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>5.770773</td>\n",
       "      <td>-16.02</td>\n",
       "      <td>23.52</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.022724</td>\n",
       "      <td>0.005456</td>\n",
       "      <td>0.021636</td>\n",
       "      <td>...</td>\n",
       "      <td>5.418396</td>\n",
       "      <td>32.731636</td>\n",
       "      <td>0.016653</td>\n",
       "      <td>32.71</td>\n",
       "      <td>32.77</td>\n",
       "      <td>0.149321</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1178 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BVP_mean     BVP_std  BVP_min  BVP_max  EDA_phasic_mean  EDA_phasic_std  \\\n",
       "0    -0.181673  107.648359  -358.13   554.77         1.824289        1.088328   \n",
       "1    -0.830147  118.742089  -392.28   438.16         2.109146        1.223528   \n",
       "2     0.939683   42.190039  -240.61   209.89         0.152828        0.128896   \n",
       "3     0.107404   41.606872  -289.26   145.36         0.177595        0.126167   \n",
       "4    -0.073620   43.121633  -197.37   194.12         0.035014        0.039616   \n",
       "...        ...         ...      ...      ...              ...             ...   \n",
       "1173  0.111318    9.667118   -83.56    30.05         0.005391        0.004034   \n",
       "1174  0.038265    9.805945   -29.08    51.20         0.010002        0.006730   \n",
       "1175 -0.096324    9.154524   -63.68    37.41         0.008592        0.006620   \n",
       "1176 -0.021164    5.935253   -19.01    12.90         0.004904        0.003361   \n",
       "1177  0.081216    5.770773   -16.02    23.52         0.006322        0.006260   \n",
       "\n",
       "      EDA_phasic_min  EDA_phasic_max  EDA_smna_mean  EDA_smna_std  ...  \\\n",
       "0           0.367977        4.319987       1.284376      1.952823  ...   \n",
       "1           0.539150        4.459367       1.467865      2.852510  ...   \n",
       "2           0.006950        0.544346       0.105091      0.244891  ...   \n",
       "3           0.002789        0.361388       0.110786      0.199704  ...   \n",
       "4           0.001144        0.132781       0.026716      0.114738  ...   \n",
       "...              ...             ...            ...           ...  ...   \n",
       "1173        0.000659        0.014062       0.004986      0.016789  ...   \n",
       "1174        0.000590        0.028034       0.006968      0.027377  ...   \n",
       "1175        0.000192        0.023813       0.006563      0.022838  ...   \n",
       "1176        0.000789        0.013490       0.004013      0.016423  ...   \n",
       "1177        0.000024        0.022724       0.005456      0.021636  ...   \n",
       "\n",
       "       Resp_max  TEMP_mean  TEMP_std  TEMP_min  TEMP_max  BVP_peak_freq  \\\n",
       "0      6.504822  35.817091  0.012674     35.79     35.84       0.135670   \n",
       "1      6.742859  35.797568  0.029901     35.75     35.87       0.095023   \n",
       "2      3.260803  35.712909  0.027612     35.66     35.75       0.076880   \n",
       "3      3.730774  35.700811  0.019504     35.66     35.73       0.140271   \n",
       "4      2.912903  35.744727  0.019386     35.71     35.79       0.149321   \n",
       "...         ...        ...       ...       ...       ...            ...   \n",
       "1173   6.335449  32.887818  0.014358     32.87     32.91       0.153846   \n",
       "1174   6.094360  32.895225  0.012794     32.87     32.91       0.144715   \n",
       "1175  19.221497  32.854000  0.022450     32.81     32.89       0.131222   \n",
       "1176   6.309509  32.792342  0.020222     32.75     32.83       0.158282   \n",
       "1177   5.418396  32.731636  0.016653     32.71     32.77       0.149321   \n",
       "\n",
       "      TEMP_slope  subject  label_0  label_1  \n",
       "0      -0.000169        2        1        0  \n",
       "1      -0.000789        2        1        0  \n",
       "2      -0.000717        2        1        0  \n",
       "3       0.000075        2        1        0  \n",
       "4       0.000442        2        1        0  \n",
       "...          ...      ...      ...      ...  \n",
       "1173    0.000181       17        1        0  \n",
       "1174   -0.000037       17        1        0  \n",
       "1175   -0.000512       17        1        0  \n",
       "1176   -0.000532       17        1        0  \n",
       "1177   -0.000327       17        1        0  \n",
       "\n",
       "[1178 rows x 29 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns=['label'], prefix='label')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separa o dicionário somente para teste final e o rótulo de leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df = []\n",
    "l_pac_rot = []\n",
    "for idx in pacientes_teste:\n",
    "    l_df.append(df.loc[df['subject'] == idx])\n",
    "    l_pac_rot.append(df_paciente_rotulo.loc[df_paciente_rotulo['subject'] == idx])\n",
    "# concatena os DFs dos pacientes separadas para teste\n",
    "df_teste = pd.concat(l_df, axis = 0)\n",
    "df_rotulos = pd.concat(l_pac_rot, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>BVP_max</th>\n",
       "      <th>EDA_phasic_mean</th>\n",
       "      <th>EDA_phasic_std</th>\n",
       "      <th>EDA_phasic_min</th>\n",
       "      <th>EDA_phasic_max</th>\n",
       "      <th>EDA_smna_mean</th>\n",
       "      <th>EDA_smna_std</th>\n",
       "      <th>...</th>\n",
       "      <th>Resp_max</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "      <th>subject</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>60.695993</td>\n",
       "      <td>-277.90</td>\n",
       "      <td>288.48</td>\n",
       "      <td>0.021046</td>\n",
       "      <td>0.030881</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.110353</td>\n",
       "      <td>0.008045</td>\n",
       "      <td>0.038205</td>\n",
       "      <td>...</td>\n",
       "      <td>8.006287</td>\n",
       "      <td>29.862252</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.89</td>\n",
       "      <td>0.094969</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>-0.971686</td>\n",
       "      <td>82.993211</td>\n",
       "      <td>-412.89</td>\n",
       "      <td>360.39</td>\n",
       "      <td>0.018719</td>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.072954</td>\n",
       "      <td>0.018824</td>\n",
       "      <td>0.085339</td>\n",
       "      <td>...</td>\n",
       "      <td>19.873047</td>\n",
       "      <td>29.848727</td>\n",
       "      <td>0.016901</td>\n",
       "      <td>29.83</td>\n",
       "      <td>29.89</td>\n",
       "      <td>0.067873</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>0.942630</td>\n",
       "      <td>54.254277</td>\n",
       "      <td>-410.39</td>\n",
       "      <td>202.78</td>\n",
       "      <td>0.036566</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.078219</td>\n",
       "      <td>0.023816</td>\n",
       "      <td>0.067389</td>\n",
       "      <td>...</td>\n",
       "      <td>7.398987</td>\n",
       "      <td>29.871081</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>29.83</td>\n",
       "      <td>29.91</td>\n",
       "      <td>0.054299</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>-2.954975</td>\n",
       "      <td>127.075119</td>\n",
       "      <td>-585.27</td>\n",
       "      <td>486.88</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.016542</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.016718</td>\n",
       "      <td>...</td>\n",
       "      <td>7.875061</td>\n",
       "      <td>29.899009</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>29.87</td>\n",
       "      <td>29.93</td>\n",
       "      <td>0.113058</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>3.300170</td>\n",
       "      <td>106.941650</td>\n",
       "      <td>-440.77</td>\n",
       "      <td>394.06</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.008245</td>\n",
       "      <td>...</td>\n",
       "      <td>15.803528</td>\n",
       "      <td>29.887091</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.91</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>-0.073968</td>\n",
       "      <td>6.867706</td>\n",
       "      <td>-17.72</td>\n",
       "      <td>14.45</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.027218</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.017644</td>\n",
       "      <td>...</td>\n",
       "      <td>4.936218</td>\n",
       "      <td>33.133063</td>\n",
       "      <td>0.011452</td>\n",
       "      <td>33.11</td>\n",
       "      <td>33.15</td>\n",
       "      <td>0.117581</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.011646</td>\n",
       "      <td>7.172588</td>\n",
       "      <td>-34.76</td>\n",
       "      <td>17.74</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.033803</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>0.019328</td>\n",
       "      <td>...</td>\n",
       "      <td>4.719543</td>\n",
       "      <td>33.104182</td>\n",
       "      <td>0.013030</td>\n",
       "      <td>33.09</td>\n",
       "      <td>33.13</td>\n",
       "      <td>0.126697</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>-0.028213</td>\n",
       "      <td>6.643487</td>\n",
       "      <td>-16.16</td>\n",
       "      <td>11.36</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.004593</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.015161</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>...</td>\n",
       "      <td>4.676819</td>\n",
       "      <td>33.100991</td>\n",
       "      <td>0.014392</td>\n",
       "      <td>33.07</td>\n",
       "      <td>33.13</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>0.065913</td>\n",
       "      <td>8.291118</td>\n",
       "      <td>-25.00</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.015541</td>\n",
       "      <td>0.011821</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.038344</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.035899</td>\n",
       "      <td>...</td>\n",
       "      <td>16.873169</td>\n",
       "      <td>33.102364</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>33.07</td>\n",
       "      <td>33.13</td>\n",
       "      <td>0.122103</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>0.059044</td>\n",
       "      <td>8.539519</td>\n",
       "      <td>-20.93</td>\n",
       "      <td>15.62</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.016053</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>...</td>\n",
       "      <td>4.182434</td>\n",
       "      <td>33.095586</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>33.05</td>\n",
       "      <td>33.13</td>\n",
       "      <td>0.113122</td>\n",
       "      <td>-0.000243</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BVP_mean     BVP_std  BVP_min  BVP_max  EDA_phasic_mean  EDA_phasic_std  \\\n",
       "939  0.108700   60.695993  -277.90   288.48         0.021046        0.030881   \n",
       "940 -0.971686   82.993211  -412.89   360.39         0.018719        0.021084   \n",
       "941  0.942630   54.254277  -410.39   202.78         0.036566        0.020929   \n",
       "942 -2.954975  127.075119  -585.27   486.88         0.006829        0.005181   \n",
       "943  3.300170  106.941650  -440.77   394.06         0.002161        0.002002   \n",
       "..        ...         ...      ...      ...              ...             ...   \n",
       "459 -0.073968    6.867706   -17.72    14.45         0.007650        0.007973   \n",
       "460  0.011646    7.172588   -34.76    17.74         0.006245        0.008343   \n",
       "461 -0.028213    6.643487   -16.16    11.36         0.005205        0.004593   \n",
       "462  0.065913    8.291118   -25.00    24.72         0.015541        0.011821   \n",
       "463  0.059044    8.539519   -20.93    15.62         0.002778        0.003695   \n",
       "\n",
       "     EDA_phasic_min  EDA_phasic_max  EDA_smna_mean  EDA_smna_std  ...  \\\n",
       "939        0.000089        0.110353       0.008045      0.038205  ...   \n",
       "940        0.000054        0.072954       0.018824      0.085339  ...   \n",
       "941        0.001480        0.078219       0.023816      0.067389  ...   \n",
       "942        0.000202        0.016542       0.005229      0.016718  ...   \n",
       "943        0.000240        0.008263       0.001626      0.008245  ...   \n",
       "..              ...             ...            ...           ...  ...   \n",
       "459        0.000053        0.027218       0.005059      0.017644  ...   \n",
       "460        0.000133        0.033803       0.004278      0.019328  ...   \n",
       "461        0.000061        0.015161       0.004031      0.014358  ...   \n",
       "462        0.000361        0.038344       0.012832      0.035899  ...   \n",
       "463        0.000069        0.016053       0.001559      0.007376  ...   \n",
       "\n",
       "      Resp_max  TEMP_mean  TEMP_std  TEMP_min  TEMP_max  BVP_peak_freq  \\\n",
       "939   8.006287  29.862252  0.012353     29.85     29.89       0.094969   \n",
       "940  19.873047  29.848727  0.016901     29.83     29.89       0.067873   \n",
       "941   7.398987  29.871081  0.020767     29.83     29.91       0.054299   \n",
       "942   7.875061  29.899009  0.015594     29.87     29.93       0.113058   \n",
       "943  15.803528  29.887091  0.012887     29.85     29.91       0.104072   \n",
       "..         ...        ...       ...       ...       ...            ...   \n",
       "459   4.936218  33.133063  0.011452     33.11     33.15       0.117581   \n",
       "460   4.719543  33.104182  0.013030     33.09     33.13       0.126697   \n",
       "461   4.676819  33.100991  0.014392     33.07     33.13       0.117647   \n",
       "462  16.873169  33.102364  0.012644     33.07     33.13       0.122103   \n",
       "463   4.182434  33.095586  0.014989     33.05     33.13       0.113122   \n",
       "\n",
       "     TEMP_slope  subject  label_0  label_1  \n",
       "939    0.000096       15        1        0  \n",
       "940   -0.000316       15        1        0  \n",
       "941    0.000549       15        1        0  \n",
       "942   -0.000043       15        1        0  \n",
       "943    0.000058       15        1        0  \n",
       "..          ...      ...      ...      ...  \n",
       "459   -0.000163        7        1        0  \n",
       "460   -0.000156        7        1        0  \n",
       "461   -0.000144        7        1        0  \n",
       "462    0.000227        7        1        0  \n",
       "463   -0.000243        7        1        0  \n",
       "\n",
       "[157 rows x 29 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe teste com dummy \n",
    "df_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remoção dos três últimos atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BVP_mean</th>\n",
       "      <th>BVP_std</th>\n",
       "      <th>BVP_min</th>\n",
       "      <th>BVP_max</th>\n",
       "      <th>EDA_phasic_mean</th>\n",
       "      <th>EDA_phasic_std</th>\n",
       "      <th>EDA_phasic_min</th>\n",
       "      <th>EDA_phasic_max</th>\n",
       "      <th>EDA_smna_mean</th>\n",
       "      <th>EDA_smna_std</th>\n",
       "      <th>...</th>\n",
       "      <th>Resp_mean</th>\n",
       "      <th>Resp_std</th>\n",
       "      <th>Resp_min</th>\n",
       "      <th>Resp_max</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>60.695993</td>\n",
       "      <td>-277.90</td>\n",
       "      <td>288.48</td>\n",
       "      <td>0.021046</td>\n",
       "      <td>0.030881</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.110353</td>\n",
       "      <td>0.008045</td>\n",
       "      <td>0.038205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130691</td>\n",
       "      <td>3.439650</td>\n",
       "      <td>-6.658936</td>\n",
       "      <td>8.006287</td>\n",
       "      <td>29.862252</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.89</td>\n",
       "      <td>0.094969</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>-0.971686</td>\n",
       "      <td>82.993211</td>\n",
       "      <td>-412.89</td>\n",
       "      <td>360.39</td>\n",
       "      <td>0.018719</td>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.072954</td>\n",
       "      <td>0.018824</td>\n",
       "      <td>0.085339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054086</td>\n",
       "      <td>6.621717</td>\n",
       "      <td>-17.582703</td>\n",
       "      <td>19.873047</td>\n",
       "      <td>29.848727</td>\n",
       "      <td>0.016901</td>\n",
       "      <td>29.83</td>\n",
       "      <td>29.89</td>\n",
       "      <td>0.067873</td>\n",
       "      <td>-0.000316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>0.942630</td>\n",
       "      <td>54.254277</td>\n",
       "      <td>-410.39</td>\n",
       "      <td>202.78</td>\n",
       "      <td>0.036566</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.078219</td>\n",
       "      <td>0.023816</td>\n",
       "      <td>0.067389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176058</td>\n",
       "      <td>2.780933</td>\n",
       "      <td>-5.476379</td>\n",
       "      <td>7.398987</td>\n",
       "      <td>29.871081</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>29.83</td>\n",
       "      <td>29.91</td>\n",
       "      <td>0.054299</td>\n",
       "      <td>0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>-2.954975</td>\n",
       "      <td>127.075119</td>\n",
       "      <td>-585.27</td>\n",
       "      <td>486.88</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.016542</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.016718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130277</td>\n",
       "      <td>3.703247</td>\n",
       "      <td>-6.877136</td>\n",
       "      <td>7.875061</td>\n",
       "      <td>29.899009</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>29.87</td>\n",
       "      <td>29.93</td>\n",
       "      <td>0.113058</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>3.300170</td>\n",
       "      <td>106.941650</td>\n",
       "      <td>-440.77</td>\n",
       "      <td>394.06</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.008245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072091</td>\n",
       "      <td>6.377189</td>\n",
       "      <td>-15.760803</td>\n",
       "      <td>15.803528</td>\n",
       "      <td>29.887091</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.91</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>-0.073968</td>\n",
       "      <td>6.867706</td>\n",
       "      <td>-17.72</td>\n",
       "      <td>14.45</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.027218</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.017644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>1.962939</td>\n",
       "      <td>-4.010010</td>\n",
       "      <td>4.936218</td>\n",
       "      <td>33.133063</td>\n",
       "      <td>0.011452</td>\n",
       "      <td>33.11</td>\n",
       "      <td>33.15</td>\n",
       "      <td>0.117581</td>\n",
       "      <td>-0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.011646</td>\n",
       "      <td>7.172588</td>\n",
       "      <td>-34.76</td>\n",
       "      <td>17.74</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.033803</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>0.019328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105936</td>\n",
       "      <td>2.312353</td>\n",
       "      <td>-3.471375</td>\n",
       "      <td>4.719543</td>\n",
       "      <td>33.104182</td>\n",
       "      <td>0.013030</td>\n",
       "      <td>33.09</td>\n",
       "      <td>33.13</td>\n",
       "      <td>0.126697</td>\n",
       "      <td>-0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>-0.028213</td>\n",
       "      <td>6.643487</td>\n",
       "      <td>-16.16</td>\n",
       "      <td>11.36</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.004593</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.015161</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109959</td>\n",
       "      <td>2.321951</td>\n",
       "      <td>-4.231262</td>\n",
       "      <td>4.676819</td>\n",
       "      <td>33.100991</td>\n",
       "      <td>0.014392</td>\n",
       "      <td>33.07</td>\n",
       "      <td>33.13</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>-0.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>0.065913</td>\n",
       "      <td>8.291118</td>\n",
       "      <td>-25.00</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.015541</td>\n",
       "      <td>0.011821</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.038344</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.035899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043015</td>\n",
       "      <td>3.454660</td>\n",
       "      <td>-8.525085</td>\n",
       "      <td>16.873169</td>\n",
       "      <td>33.102364</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>33.07</td>\n",
       "      <td>33.13</td>\n",
       "      <td>0.122103</td>\n",
       "      <td>0.000227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>0.059044</td>\n",
       "      <td>8.539519</td>\n",
       "      <td>-20.93</td>\n",
       "      <td>15.62</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.016053</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063661</td>\n",
       "      <td>1.842110</td>\n",
       "      <td>-3.662109</td>\n",
       "      <td>4.182434</td>\n",
       "      <td>33.095586</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>33.05</td>\n",
       "      <td>33.13</td>\n",
       "      <td>0.113122</td>\n",
       "      <td>-0.000243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BVP_mean     BVP_std  BVP_min  BVP_max  EDA_phasic_mean  EDA_phasic_std  \\\n",
       "939  0.108700   60.695993  -277.90   288.48         0.021046        0.030881   \n",
       "940 -0.971686   82.993211  -412.89   360.39         0.018719        0.021084   \n",
       "941  0.942630   54.254277  -410.39   202.78         0.036566        0.020929   \n",
       "942 -2.954975  127.075119  -585.27   486.88         0.006829        0.005181   \n",
       "943  3.300170  106.941650  -440.77   394.06         0.002161        0.002002   \n",
       "..        ...         ...      ...      ...              ...             ...   \n",
       "459 -0.073968    6.867706   -17.72    14.45         0.007650        0.007973   \n",
       "460  0.011646    7.172588   -34.76    17.74         0.006245        0.008343   \n",
       "461 -0.028213    6.643487   -16.16    11.36         0.005205        0.004593   \n",
       "462  0.065913    8.291118   -25.00    24.72         0.015541        0.011821   \n",
       "463  0.059044    8.539519   -20.93    15.62         0.002778        0.003695   \n",
       "\n",
       "     EDA_phasic_min  EDA_phasic_max  EDA_smna_mean  EDA_smna_std  ...  \\\n",
       "939        0.000089        0.110353       0.008045      0.038205  ...   \n",
       "940        0.000054        0.072954       0.018824      0.085339  ...   \n",
       "941        0.001480        0.078219       0.023816      0.067389  ...   \n",
       "942        0.000202        0.016542       0.005229      0.016718  ...   \n",
       "943        0.000240        0.008263       0.001626      0.008245  ...   \n",
       "..              ...             ...            ...           ...  ...   \n",
       "459        0.000053        0.027218       0.005059      0.017644  ...   \n",
       "460        0.000133        0.033803       0.004278      0.019328  ...   \n",
       "461        0.000061        0.015161       0.004031      0.014358  ...   \n",
       "462        0.000361        0.038344       0.012832      0.035899  ...   \n",
       "463        0.000069        0.016053       0.001559      0.007376  ...   \n",
       "\n",
       "     Resp_mean  Resp_std   Resp_min   Resp_max  TEMP_mean  TEMP_std  TEMP_min  \\\n",
       "939   0.130691  3.439650  -6.658936   8.006287  29.862252  0.012353     29.85   \n",
       "940  -0.054086  6.621717 -17.582703  19.873047  29.848727  0.016901     29.83   \n",
       "941   0.176058  2.780933  -5.476379   7.398987  29.871081  0.020767     29.83   \n",
       "942   0.130277  3.703247  -6.877136   7.875061  29.899009  0.015594     29.87   \n",
       "943   0.072091  6.377189 -15.760803  15.803528  29.887091  0.012887     29.85   \n",
       "..         ...       ...        ...        ...        ...       ...       ...   \n",
       "459   0.000537  1.962939  -4.010010   4.936218  33.133063  0.011452     33.11   \n",
       "460   0.105936  2.312353  -3.471375   4.719543  33.104182  0.013030     33.09   \n",
       "461   0.109959  2.321951  -4.231262   4.676819  33.100991  0.014392     33.07   \n",
       "462   0.043015  3.454660  -8.525085  16.873169  33.102364  0.012644     33.07   \n",
       "463   0.063661  1.842110  -3.662109   4.182434  33.095586  0.014989     33.05   \n",
       "\n",
       "     TEMP_max  BVP_peak_freq  TEMP_slope  \n",
       "939     29.89       0.094969    0.000096  \n",
       "940     29.89       0.067873   -0.000316  \n",
       "941     29.91       0.054299    0.000549  \n",
       "942     29.93       0.113058   -0.000043  \n",
       "943     29.91       0.104072    0.000058  \n",
       "..        ...            ...         ...  \n",
       "459     33.15       0.117581   -0.000163  \n",
       "460     33.13       0.126697   -0.000156  \n",
       "461     33.13       0.117647   -0.000144  \n",
       "462     33.13       0.122103    0.000227  \n",
       "463     33.13       0.113122   -0.000243  \n",
       "\n",
       "[157 rows x 26 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_teste = df_teste.iloc[:, :-3]\n",
    "df_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rótulos sem dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject  label\n",
       "939       15      0\n",
       "940       15      0\n",
       "941       15      0\n",
       "942       15      0\n",
       "943       15      0\n",
       "..       ...    ...\n",
       "459        7      0\n",
       "460        7      0\n",
       "461        7      0\n",
       "462        7      0\n",
       "463        7      0\n",
       "\n",
       "[157 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rotulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções que irão para o LOSO - Leave one Out Subject Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modelo neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_nn(input_shape):\n",
    "    x_inp = Input(input_shape)\n",
    "    x = Dense(units=256, activation='relu')(x_inp)\n",
    "    x = Dense(units=128, activation='relu')(x)\n",
    "    x = Dense(units=64, activation='relu')(x)\n",
    "    x_out = Dense(units=2, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=x_inp, outputs=x_out, name = \"Shallow_Model\")\n",
    "    model.compile(loss='binary_crossentropy', optimizer = 'rmsProp', metrics = ['acc'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out (df : pd.DataFrame, paciente : int) -> [pd.DataFrame, int]:\n",
    "    \"\"\" Retorna os dados de treino e teste, além de seus rótulos\n",
    "    \n",
    "    Keyword arguments:\n",
    "    df : dataframe contendo todos os dados\n",
    "    paciente : número correspondente ao pacientes\n",
    "    \"\"\"\n",
    "    df_treino = df[df['subject'] != paciente]\n",
    "    df_teste = df[df['subject'] == paciente]\n",
    "    \n",
    "    # capta os rótulos e os valores para atributo\n",
    "    y_train = df_treino.iloc[:, [-2,-1]].values\n",
    "    df_treino = df_treino.drop(['subject', 'label_0', 'label_1'], axis = 1)\n",
    "    X_train = df_treino.values\n",
    "    \n",
    "    y_test = df_teste.iloc[:, [-2,-1]].values\n",
    "    df_teste = df_teste.drop(['subject', 'label_0', 'label_1'], axis = 1)\n",
    "    X_test = df_teste.values\n",
    "        \n",
    "    # remove attr que não serão utilizados no treino\n",
    "    # df_treino.drop(['subject','label'], axis = 1, inplace=True)\n",
    "\n",
    "    return X_train, \\\n",
    "           y_train, \\\n",
    "           X_test, \\\n",
    "           y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### para avaliar a acurácia de cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medir_acuracia(y_true, y_pred):\n",
    "    comp = len(y_pred)\n",
    "    cont = 0\n",
    "    for x,y in zip (y_true, y_pred):\n",
    "        if x is y:\n",
    "            cont += 1\n",
    "    acc = cont / comp * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### faz o teste do modelo após seu treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testar_modelo(model : Model, df_teste = df_teste, df_rotulos = df_rotulos):\n",
    "    \"\"\" Retorna um dataframe com dados de pacientes para treino e teste\n",
    "    \n",
    "    Keyword arguments:\n",
    "    model: modelo neural\n",
    "    df : dataframe contendo todos os dados\n",
    "    pacientes : lista de pacientes reservados para teste\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true = df_rotulos['label'].values\n",
    "    X_test = df_teste.values\n",
    "    \n",
    "    # realiza a predição\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis = 1)\n",
    "    \n",
    "    matriz = confusion_matrix(y_true, y_pred)\n",
    "    acc = medir_acuracia(y_true.tolist(), y_pred.tolist())\n",
    "    \n",
    "    return matriz, acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes unitários: :')\n",
    "\n",
    "### ---------------------------- Pequeno TESTE de carrregamento dos dados e execução do modelo ---------------------\n",
    "#### Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrega uma amostra do paciente '2'\n",
    "X_train, y_train, X_val, y_val = leave_one_out(df, 2)\n",
    "# ... para pegarmos o shape que determinará o formato do dado de entrada da rede\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste de execução de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_modelo= modelo_nn((layer_1_dim,))\n",
    "teste_modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = teste_modelo.fit(x = X_train,\n",
    "                 y = y_train,\n",
    "                 epochs=100,\n",
    "                 batch_size = 25,\n",
    "                 validation_data = (X_val, y_val),\n",
    "                 verbose = 1\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teste do modelo\n",
    "x_teste = df_teste.values\n",
    "\n",
    "y_pred = teste_modelo.predict(x_teste)\n",
    "y_pred = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(df_rotulos['label'].values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medir_acuracia(df_rotulos['label'].values.tolist(), y_pred.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------- Final do TESTE --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_hist = []\n",
    "lista_acc = []\n",
    "lista_matriz = []\n",
    "# lista_modelo = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução do modelo com LOSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paciente:  2\n",
      "Train on 1102 samples, validate on 76 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /home/hygo/anaconda3/envs/ammd2/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "1102/1102 [==============================] - 3s 3ms/sample - loss: 3.8321 - acc: 0.6670 - val_loss: 4.4446 - val_acc: 0.7105\n",
      "Epoch 2/100\n",
      "1102/1102 [==============================] - 0s 156us/sample - loss: 3.4682 - acc: 0.6452 - val_loss: 4.3780 - val_acc: 0.7105\n",
      "Epoch 3/100\n",
      "1102/1102 [==============================] - 0s 147us/sample - loss: 2.1682 - acc: 0.6688 - val_loss: 2.2815 - val_acc: 0.7105\n",
      "Epoch 4/100\n",
      "1102/1102 [==============================] - 0s 142us/sample - loss: 1.1922 - acc: 0.7396 - val_loss: 0.4163 - val_acc: 0.8684\n",
      "Epoch 5/100\n",
      "1102/1102 [==============================] - 0s 136us/sample - loss: 0.7630 - acc: 0.7813 - val_loss: 3.7393 - val_acc: 0.5789\n",
      "Epoch 6/100\n",
      "1102/1102 [==============================] - 0s 148us/sample - loss: 0.6139 - acc: 0.8149 - val_loss: 1.3732 - val_acc: 0.6711\n",
      "Epoch 7/100\n",
      "1102/1102 [==============================] - 0s 145us/sample - loss: 0.4679 - acc: 0.8330 - val_loss: 1.3057 - val_acc: 0.7237\n",
      "Epoch 8/100\n",
      "1102/1102 [==============================] - 0s 142us/sample - loss: 0.4534 - acc: 0.8439 - val_loss: 0.8214 - val_acc: 0.7237\n",
      "Epoch 9/100\n",
      "1102/1102 [==============================] - 0s 145us/sample - loss: 0.3669 - acc: 0.8693 - val_loss: 1.1399 - val_acc: 0.6447\n",
      "Epoch 10/100\n",
      "1102/1102 [==============================] - 0s 139us/sample - loss: 0.4040 - acc: 0.8612 - val_loss: 0.9509 - val_acc: 0.6974\n",
      "Epoch 11/100\n",
      "1102/1102 [==============================] - 0s 147us/sample - loss: 0.3303 - acc: 0.8748 - val_loss: 0.6531 - val_acc: 0.7500\n",
      "Epoch 12/100\n",
      "1102/1102 [==============================] - 0s 147us/sample - loss: 0.3514 - acc: 0.8757 - val_loss: 0.7795 - val_acc: 0.7237\n",
      "Epoch 13/100\n",
      "1102/1102 [==============================] - 0s 142us/sample - loss: 0.3370 - acc: 0.8802 - val_loss: 0.3210 - val_acc: 0.9342\n",
      "Epoch 14/100\n",
      "1102/1102 [==============================] - 0s 139us/sample - loss: 0.3219 - acc: 0.8938 - val_loss: 0.7519 - val_acc: 0.7763\n",
      "Epoch 15/100\n",
      "1102/1102 [==============================] - 0s 145us/sample - loss: 0.2970 - acc: 0.8975 - val_loss: 0.9470 - val_acc: 0.7105\n",
      "Epoch 16/100\n",
      "1102/1102 [==============================] - 0s 138us/sample - loss: 0.2647 - acc: 0.9002 - val_loss: 0.3745 - val_acc: 0.8947\n",
      "Epoch 17/100\n",
      "1102/1102 [==============================] - 0s 140us/sample - loss: 0.2944 - acc: 0.8929 - val_loss: 0.5759 - val_acc: 0.7763\n",
      "Epoch 18/100\n",
      "1102/1102 [==============================] - 0s 140us/sample - loss: 0.2563 - acc: 0.9074 - val_loss: 0.2663 - val_acc: 0.9079\n",
      "Epoch 19/100\n",
      "1102/1102 [==============================] - 0s 144us/sample - loss: 0.2557 - acc: 0.8984 - val_loss: 1.1403 - val_acc: 0.7632\n",
      "Epoch 20/100\n",
      "1102/1102 [==============================] - 0s 142us/sample - loss: 0.2398 - acc: 0.9047 - val_loss: 0.5184 - val_acc: 0.8026\n",
      "Epoch 21/100\n",
      "1102/1102 [==============================] - 0s 139us/sample - loss: 0.2399 - acc: 0.9074 - val_loss: 0.2749 - val_acc: 0.9342\n",
      "Epoch 22/100\n",
      "1102/1102 [==============================] - 0s 142us/sample - loss: 0.2156 - acc: 0.9120 - val_loss: 0.2752 - val_acc: 0.8684\n",
      "Epoch 23/100\n",
      "1102/1102 [==============================] - 0s 142us/sample - loss: 0.2107 - acc: 0.9138 - val_loss: 0.4905 - val_acc: 0.7368\n",
      "Epoch 24/100\n",
      "1102/1102 [==============================] - 0s 137us/sample - loss: 0.2280 - acc: 0.9201 - val_loss: 0.6990 - val_acc: 0.6974\n",
      "Epoch 25/100\n",
      "1102/1102 [==============================] - 0s 141us/sample - loss: 0.2079 - acc: 0.9083 - val_loss: 0.4674 - val_acc: 0.8026\n",
      "Epoch 26/100\n",
      "1102/1102 [==============================] - 0s 139us/sample - loss: 0.2626 - acc: 0.9138 - val_loss: 0.4777 - val_acc: 0.9079\n",
      "Epoch 27/100\n",
      "1102/1102 [==============================] - 0s 141us/sample - loss: 0.1957 - acc: 0.9247 - val_loss: 0.7613 - val_acc: 0.8158\n",
      "Epoch 28/100\n",
      "1102/1102 [==============================] - 0s 140us/sample - loss: 0.1997 - acc: 0.9192 - val_loss: 0.7605 - val_acc: 0.8026\n",
      "Epoch 29/100\n",
      "1102/1102 [==============================] - 0s 139us/sample - loss: 0.2112 - acc: 0.9120 - val_loss: 0.2670 - val_acc: 0.9342\n",
      "Epoch 30/100\n",
      "1102/1102 [==============================] - 0s 134us/sample - loss: 0.1772 - acc: 0.9319 - val_loss: 2.3580 - val_acc: 0.7368\n",
      "Epoch 31/100\n",
      "1102/1102 [==============================] - 0s 136us/sample - loss: 0.2100 - acc: 0.9292 - val_loss: 0.2748 - val_acc: 0.8421\n",
      "Epoch 32/100\n",
      "1102/1102 [==============================] - 0s 144us/sample - loss: 0.1915 - acc: 0.9247 - val_loss: 1.4954 - val_acc: 0.7368\n",
      "Epoch 33/100\n",
      "1102/1102 [==============================] - 0s 137us/sample - loss: 0.2318 - acc: 0.9265 - val_loss: 0.2134 - val_acc: 0.9211\n",
      "Epoch 34/100\n",
      "1102/1102 [==============================] - 0s 140us/sample - loss: 0.1844 - acc: 0.9328 - val_loss: 0.4237 - val_acc: 0.8289\n",
      "Epoch 35/100\n",
      "1102/1102 [==============================] - 0s 145us/sample - loss: 0.2041 - acc: 0.9247 - val_loss: 0.3371 - val_acc: 0.8816\n",
      "Epoch 36/100\n",
      "1102/1102 [==============================] - 0s 132us/sample - loss: 0.2078 - acc: 0.9283 - val_loss: 0.2438 - val_acc: 0.9474\n",
      "Epoch 37/100\n",
      "1102/1102 [==============================] - 0s 143us/sample - loss: 0.1902 - acc: 0.9310 - val_loss: 0.2399 - val_acc: 0.9342\n",
      "Epoch 38/100\n",
      "1102/1102 [==============================] - 0s 141us/sample - loss: 0.1656 - acc: 0.9365 - val_loss: 1.4857 - val_acc: 0.7500\n",
      "Epoch 39/100\n",
      "1102/1102 [==============================] - 0s 137us/sample - loss: 0.1857 - acc: 0.9310 - val_loss: 0.7907 - val_acc: 0.7895\n",
      "Epoch 40/100\n",
      "1102/1102 [==============================] - 0s 135us/sample - loss: 0.1672 - acc: 0.9328 - val_loss: 0.4684 - val_acc: 0.7895\n",
      "Epoch 41/100\n",
      "1102/1102 [==============================] - 0s 140us/sample - loss: 0.1986 - acc: 0.9310 - val_loss: 0.2485 - val_acc: 0.9079\n",
      "Epoch 42/100\n",
      "1102/1102 [==============================] - 0s 137us/sample - loss: 0.1577 - acc: 0.9401 - val_loss: 0.1844 - val_acc: 0.9342\n",
      "Epoch 43/100\n",
      "1102/1102 [==============================] - 0s 140us/sample - loss: 0.1836 - acc: 0.9347 - val_loss: 0.3525 - val_acc: 0.8947\n",
      "Epoch 44/100\n",
      "1102/1102 [==============================] - 0s 135us/sample - loss: 0.1803 - acc: 0.9310 - val_loss: 0.9381 - val_acc: 0.7632\n",
      "Epoch 45/100\n",
      "1102/1102 [==============================] - 0s 156us/sample - loss: 0.1658 - acc: 0.9419 - val_loss: 0.3288 - val_acc: 0.8553\n",
      "Epoch 46/100\n",
      "1102/1102 [==============================] - 0s 156us/sample - loss: 0.1542 - acc: 0.9446 - val_loss: 0.3451 - val_acc: 0.8816\n",
      "Epoch 47/100\n",
      "1102/1102 [==============================] - 0s 168us/sample - loss: 0.1653 - acc: 0.9292 - val_loss: 0.1615 - val_acc: 0.9342\n",
      "Epoch 48/100\n",
      "1102/1102 [==============================] - 0s 149us/sample - loss: 0.1732 - acc: 0.9419 - val_loss: 0.1893 - val_acc: 0.9474\n",
      "Epoch 49/100\n",
      "1102/1102 [==============================] - 0s 137us/sample - loss: 0.1244 - acc: 0.9465 - val_loss: 0.4080 - val_acc: 0.8816\n",
      "Epoch 50/100\n",
      "1102/1102 [==============================] - 0s 156us/sample - loss: 0.1630 - acc: 0.9347 - val_loss: 0.3215 - val_acc: 0.8816\n",
      "Epoch 51/100\n",
      "1102/1102 [==============================] - 0s 154us/sample - loss: 0.1546 - acc: 0.9437 - val_loss: 0.2018 - val_acc: 0.9342\n",
      "Epoch 52/100\n",
      "1102/1102 [==============================] - 0s 151us/sample - loss: 0.1673 - acc: 0.9446 - val_loss: 0.5312 - val_acc: 0.8553\n",
      "Epoch 53/100\n",
      "1102/1102 [==============================] - 0s 166us/sample - loss: 0.1431 - acc: 0.9383 - val_loss: 0.1697 - val_acc: 0.9211\n",
      "Epoch 54/100\n",
      "1102/1102 [==============================] - 0s 149us/sample - loss: 0.1758 - acc: 0.9347 - val_loss: 0.5408 - val_acc: 0.8553\n",
      "Epoch 55/100\n",
      "1102/1102 [==============================] - 0s 150us/sample - loss: 0.1462 - acc: 0.9528 - val_loss: 0.2360 - val_acc: 0.9342\n",
      "Epoch 56/100\n",
      "1102/1102 [==============================] - 0s 144us/sample - loss: 0.1491 - acc: 0.9428 - val_loss: 0.5700 - val_acc: 0.7895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "1102/1102 [==============================] - 0s 147us/sample - loss: 0.1340 - acc: 0.9501 - val_loss: 0.4144 - val_acc: 0.8289\n",
      "Epoch 58/100\n",
      "1102/1102 [==============================] - 0s 141us/sample - loss: 0.1626 - acc: 0.9365 - val_loss: 0.1100 - val_acc: 0.9605\n",
      "Epoch 59/100\n",
      "1102/1102 [==============================] - 0s 135us/sample - loss: 0.1211 - acc: 0.9492 - val_loss: 0.4297 - val_acc: 0.9211\n",
      "Epoch 60/100\n",
      "1102/1102 [==============================] - 0s 126us/sample - loss: 0.1481 - acc: 0.9419 - val_loss: 0.3322 - val_acc: 0.8816\n",
      "Epoch 61/100\n",
      "1102/1102 [==============================] - 0s 124us/sample - loss: 0.1531 - acc: 0.9365 - val_loss: 0.1367 - val_acc: 0.9211\n",
      "Epoch 62/100\n",
      "1102/1102 [==============================] - 0s 129us/sample - loss: 0.1760 - acc: 0.9428 - val_loss: 0.1062 - val_acc: 0.9737\n",
      "Epoch 63/100\n",
      "1102/1102 [==============================] - 0s 134us/sample - loss: 0.1228 - acc: 0.9528 - val_loss: 0.9209 - val_acc: 0.8289\n",
      "Epoch 64/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.1411 - acc: 0.9510 - val_loss: 0.8348 - val_acc: 0.8421\n",
      "Epoch 65/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.1643 - acc: 0.9419 - val_loss: 0.5334 - val_acc: 0.7763\n",
      "Epoch 66/100\n",
      "1102/1102 [==============================] - 0s 131us/sample - loss: 0.1388 - acc: 0.9437 - val_loss: 0.4621 - val_acc: 0.8684\n",
      "Epoch 67/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.1379 - acc: 0.9456 - val_loss: 0.2498 - val_acc: 0.9211\n",
      "Epoch 68/100\n",
      "1102/1102 [==============================] - 0s 124us/sample - loss: 0.1811 - acc: 0.9456 - val_loss: 0.2752 - val_acc: 0.9211\n",
      "Epoch 69/100\n",
      "1102/1102 [==============================] - 0s 123us/sample - loss: 0.1322 - acc: 0.9474 - val_loss: 0.0853 - val_acc: 0.9737\n",
      "Epoch 70/100\n",
      "1102/1102 [==============================] - 0s 129us/sample - loss: 0.1835 - acc: 0.9410 - val_loss: 0.2127 - val_acc: 0.9474\n",
      "Epoch 71/100\n",
      "1102/1102 [==============================] - 0s 124us/sample - loss: 0.1263 - acc: 0.9519 - val_loss: 0.5130 - val_acc: 0.8158\n",
      "Epoch 72/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.1503 - acc: 0.9492 - val_loss: 0.2492 - val_acc: 0.8553\n",
      "Epoch 73/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.1968 - acc: 0.9437 - val_loss: 0.2755 - val_acc: 0.9079\n",
      "Epoch 74/100\n",
      "1102/1102 [==============================] - 0s 126us/sample - loss: 0.1345 - acc: 0.9510 - val_loss: 0.1814 - val_acc: 0.9211\n",
      "Epoch 75/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.1440 - acc: 0.9410 - val_loss: 0.1933 - val_acc: 0.9211\n",
      "Epoch 76/100\n",
      "1102/1102 [==============================] - 0s 126us/sample - loss: 0.1145 - acc: 0.9546 - val_loss: 0.5682 - val_acc: 0.9211\n",
      "Epoch 77/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.1256 - acc: 0.9465 - val_loss: 0.3037 - val_acc: 0.9211\n",
      "Epoch 78/100\n",
      "1102/1102 [==============================] - 0s 135us/sample - loss: 0.1466 - acc: 0.9492 - val_loss: 0.1878 - val_acc: 0.9211\n",
      "Epoch 79/100\n",
      "1102/1102 [==============================] - 0s 139us/sample - loss: 0.1626 - acc: 0.9519 - val_loss: 0.2949 - val_acc: 0.9474\n",
      "Epoch 80/100\n",
      "1102/1102 [==============================] - 0s 190us/sample - loss: 0.1359 - acc: 0.9519 - val_loss: 0.2999 - val_acc: 0.9605\n",
      "Epoch 81/100\n",
      "1102/1102 [==============================] - 0s 181us/sample - loss: 0.1353 - acc: 0.9528 - val_loss: 0.1712 - val_acc: 0.9211\n",
      "Epoch 82/100\n",
      "1102/1102 [==============================] - 0s 189us/sample - loss: 0.1243 - acc: 0.9537 - val_loss: 0.2173 - val_acc: 0.9342\n",
      "Epoch 83/100\n",
      "1102/1102 [==============================] - 0s 193us/sample - loss: 0.1268 - acc: 0.9555 - val_loss: 0.0837 - val_acc: 0.9474\n",
      "Epoch 84/100\n",
      "1102/1102 [==============================] - 0s 189us/sample - loss: 0.1611 - acc: 0.9519 - val_loss: 0.2918 - val_acc: 0.8947\n",
      "Epoch 85/100\n",
      "1102/1102 [==============================] - 0s 196us/sample - loss: 0.1182 - acc: 0.9546 - val_loss: 0.1228 - val_acc: 0.9474\n",
      "Epoch 86/100\n",
      "1102/1102 [==============================] - 0s 153us/sample - loss: 0.1434 - acc: 0.9501 - val_loss: 0.5214 - val_acc: 0.8421\n",
      "Epoch 87/100\n",
      "1102/1102 [==============================] - 0s 147us/sample - loss: 0.1383 - acc: 0.9465 - val_loss: 0.2343 - val_acc: 0.9342\n",
      "Epoch 88/100\n",
      "1102/1102 [==============================] - 0s 162us/sample - loss: 0.1014 - acc: 0.9564 - val_loss: 0.3533 - val_acc: 0.9079\n",
      "Epoch 89/100\n",
      "1102/1102 [==============================] - 0s 154us/sample - loss: 0.1237 - acc: 0.9510 - val_loss: 0.0802 - val_acc: 0.9474\n",
      "Epoch 90/100\n",
      "1102/1102 [==============================] - 0s 138us/sample - loss: 0.1035 - acc: 0.9528 - val_loss: 0.5640 - val_acc: 0.8421\n",
      "Epoch 91/100\n",
      "1102/1102 [==============================] - 0s 157us/sample - loss: 0.1222 - acc: 0.9519 - val_loss: 0.1382 - val_acc: 0.9342\n",
      "Epoch 92/100\n",
      "1102/1102 [==============================] - 0s 149us/sample - loss: 0.0993 - acc: 0.9592 - val_loss: 0.1516 - val_acc: 0.9342\n",
      "Epoch 93/100\n",
      "1102/1102 [==============================] - 0s 140us/sample - loss: 0.1314 - acc: 0.9474 - val_loss: 0.4723 - val_acc: 0.8684\n",
      "Epoch 94/100\n",
      "1102/1102 [==============================] - 0s 135us/sample - loss: 0.1364 - acc: 0.9519 - val_loss: 0.1942 - val_acc: 0.9079\n",
      "Epoch 95/100\n",
      "1102/1102 [==============================] - 0s 144us/sample - loss: 0.1424 - acc: 0.9528 - val_loss: 0.2474 - val_acc: 0.8816\n",
      "Epoch 96/100\n",
      "1102/1102 [==============================] - 0s 138us/sample - loss: 0.1028 - acc: 0.9583 - val_loss: 0.1473 - val_acc: 0.9474\n",
      "Epoch 97/100\n",
      "1102/1102 [==============================] - 0s 154us/sample - loss: 0.1104 - acc: 0.9646 - val_loss: 0.3021 - val_acc: 0.8684\n",
      "Epoch 98/100\n",
      "1102/1102 [==============================] - 0s 139us/sample - loss: 0.1207 - acc: 0.9474 - val_loss: 0.1396 - val_acc: 0.9342\n",
      "Epoch 99/100\n",
      "1102/1102 [==============================] - 0s 145us/sample - loss: 0.1466 - acc: 0.9537 - val_loss: 0.1193 - val_acc: 0.9474\n",
      "Epoch 100/100\n",
      "1102/1102 [==============================] - 0s 145us/sample - loss: 0.1094 - acc: 0.9574 - val_loss: 0.1714 - val_acc: 0.8947\n",
      "\n",
      "Paciente:  3\n",
      "Train on 1101 samples, validate on 77 samples\n",
      "Epoch 1/100\n",
      "1101/1101 [==============================] - 1s 710us/sample - loss: 4.7670 - acc: 0.6058 - val_loss: 1.9615 - val_acc: 0.7143\n",
      "Epoch 2/100\n",
      "1101/1101 [==============================] - 0s 135us/sample - loss: 2.4710 - acc: 0.6503 - val_loss: 2.3084 - val_acc: 0.4805\n",
      "Epoch 3/100\n",
      "1101/1101 [==============================] - 0s 127us/sample - loss: 1.2800 - acc: 0.7121 - val_loss: 0.6189 - val_acc: 0.7143\n",
      "Epoch 4/100\n",
      "1101/1101 [==============================] - 0s 138us/sample - loss: 0.9352 - acc: 0.7520 - val_loss: 0.6869 - val_acc: 0.7143\n",
      "Epoch 5/100\n",
      "1101/1101 [==============================] - 0s 130us/sample - loss: 0.6432 - acc: 0.7675 - val_loss: 0.6737 - val_acc: 0.7273\n",
      "Epoch 6/100\n",
      "1101/1101 [==============================] - 0s 129us/sample - loss: 0.4458 - acc: 0.8420 - val_loss: 1.0143 - val_acc: 0.7403\n",
      "Epoch 7/100\n",
      "1101/1101 [==============================] - 0s 134us/sample - loss: 0.5688 - acc: 0.8084 - val_loss: 0.5470 - val_acc: 0.7532\n",
      "Epoch 8/100\n",
      "1101/1101 [==============================] - 0s 143us/sample - loss: 0.4606 - acc: 0.8202 - val_loss: 0.8196 - val_acc: 0.7532\n",
      "Epoch 9/100\n",
      "1101/1101 [==============================] - 0s 161us/sample - loss: 0.4687 - acc: 0.8102 - val_loss: 0.7830 - val_acc: 0.6753\n",
      "Epoch 10/100\n",
      "1101/1101 [==============================] - 0s 172us/sample - loss: 0.3919 - acc: 0.8619 - val_loss: 0.5733 - val_acc: 0.8052\n",
      "Epoch 11/100\n",
      "1101/1101 [==============================] - 0s 161us/sample - loss: 0.3834 - acc: 0.8719 - val_loss: 1.1500 - val_acc: 0.6234\n",
      "Epoch 12/100\n",
      "1101/1101 [==============================] - 0s 163us/sample - loss: 0.3441 - acc: 0.8783 - val_loss: 0.9535 - val_acc: 0.5974\n",
      "Epoch 13/100\n",
      "1101/1101 [==============================] - 0s 155us/sample - loss: 0.3515 - acc: 0.8774 - val_loss: 0.4703 - val_acc: 0.7922\n",
      "Epoch 14/100\n",
      "1101/1101 [==============================] - 0s 143us/sample - loss: 0.2907 - acc: 0.8865 - val_loss: 0.6521 - val_acc: 0.7662\n",
      "Epoch 15/100\n",
      "1101/1101 [==============================] - 0s 151us/sample - loss: 0.2916 - acc: 0.8910 - val_loss: 0.4962 - val_acc: 0.7922\n",
      "Epoch 16/100\n",
      "1101/1101 [==============================] - 0s 222us/sample - loss: 0.3057 - acc: 0.8910 - val_loss: 0.4151 - val_acc: 0.7922\n",
      "Epoch 17/100\n",
      "1101/1101 [==============================] - 0s 172us/sample - loss: 0.2905 - acc: 0.8901 - val_loss: 0.9769 - val_acc: 0.7662\n",
      "Epoch 18/100\n",
      "1101/1101 [==============================] - 0s 188us/sample - loss: 0.2904 - acc: 0.8919 - val_loss: 0.9608 - val_acc: 0.7143\n",
      "Epoch 19/100\n",
      "1101/1101 [==============================] - 0s 140us/sample - loss: 0.2662 - acc: 0.9037 - val_loss: 0.9448 - val_acc: 0.7273\n",
      "Epoch 20/100\n",
      "1101/1101 [==============================] - 0s 137us/sample - loss: 0.3053 - acc: 0.9028 - val_loss: 0.4358 - val_acc: 0.8312\n",
      "Epoch 21/100\n",
      "1101/1101 [==============================] - 0s 140us/sample - loss: 0.2640 - acc: 0.9010 - val_loss: 0.5766 - val_acc: 0.7662\n",
      "Epoch 22/100\n",
      "1101/1101 [==============================] - 0s 135us/sample - loss: 0.2289 - acc: 0.9083 - val_loss: 0.5864 - val_acc: 0.8312\n",
      "Epoch 23/100\n",
      "1101/1101 [==============================] - 0s 122us/sample - loss: 0.2422 - acc: 0.9092 - val_loss: 0.8522 - val_acc: 0.6494\n",
      "Epoch 24/100\n",
      "1101/1101 [==============================] - 0s 127us/sample - loss: 0.2169 - acc: 0.9119 - val_loss: 0.4893 - val_acc: 0.8182\n",
      "Epoch 25/100\n",
      "1101/1101 [==============================] - 0s 124us/sample - loss: 0.2169 - acc: 0.9192 - val_loss: 0.9264 - val_acc: 0.7403\n",
      "Epoch 26/100\n",
      "1101/1101 [==============================] - 0s 129us/sample - loss: 0.2848 - acc: 0.9037 - val_loss: 1.0547 - val_acc: 0.6234\n",
      "Epoch 27/100\n",
      "1101/1101 [==============================] - 0s 132us/sample - loss: 0.2388 - acc: 0.9164 - val_loss: 1.1343 - val_acc: 0.7792\n",
      "Epoch 28/100\n",
      "1101/1101 [==============================] - 0s 125us/sample - loss: 0.2346 - acc: 0.9101 - val_loss: 0.6878 - val_acc: 0.7922\n",
      "Epoch 29/100\n",
      "1101/1101 [==============================] - 0s 120us/sample - loss: 0.2167 - acc: 0.9083 - val_loss: 0.5261 - val_acc: 0.8182\n",
      "Epoch 30/100\n",
      "1101/1101 [==============================] - 0s 119us/sample - loss: 0.1927 - acc: 0.9228 - val_loss: 0.4536 - val_acc: 0.8052\n",
      "Epoch 31/100\n",
      "1101/1101 [==============================] - 0s 122us/sample - loss: 0.1977 - acc: 0.9292 - val_loss: 0.6523 - val_acc: 0.7532\n",
      "Epoch 32/100\n",
      "1101/1101 [==============================] - 0s 122us/sample - loss: 0.1994 - acc: 0.9292 - val_loss: 0.9222 - val_acc: 0.7532\n",
      "Epoch 33/100\n",
      "1101/1101 [==============================] - 0s 121us/sample - loss: 0.2099 - acc: 0.9219 - val_loss: 0.9852 - val_acc: 0.7922\n",
      "Epoch 34/100\n",
      "1101/1101 [==============================] - 0s 128us/sample - loss: 0.1951 - acc: 0.9264 - val_loss: 1.3209 - val_acc: 0.6883\n",
      "Epoch 35/100\n",
      "1101/1101 [==============================] - 0s 129us/sample - loss: 0.1882 - acc: 0.9273 - val_loss: 0.8796 - val_acc: 0.7662\n",
      "Epoch 36/100\n",
      "1101/1101 [==============================] - 0s 127us/sample - loss: 0.1847 - acc: 0.9255 - val_loss: 0.9077 - val_acc: 0.6883\n",
      "Epoch 37/100\n",
      "1101/1101 [==============================] - 0s 134us/sample - loss: 0.1714 - acc: 0.9301 - val_loss: 1.0025 - val_acc: 0.7792\n",
      "Epoch 38/100\n",
      "1101/1101 [==============================] - 0s 130us/sample - loss: 0.1863 - acc: 0.9337 - val_loss: 1.2772 - val_acc: 0.8052\n",
      "Epoch 39/100\n",
      "1101/1101 [==============================] - 0s 172us/sample - loss: 0.1982 - acc: 0.9282 - val_loss: 1.0580 - val_acc: 0.8052\n",
      "Epoch 40/100\n",
      "1101/1101 [==============================] - 0s 154us/sample - loss: 0.1789 - acc: 0.9337 - val_loss: 1.0564 - val_acc: 0.7662\n",
      "Epoch 41/100\n",
      "1101/1101 [==============================] - 0s 152us/sample - loss: 0.1758 - acc: 0.9355 - val_loss: 1.3903 - val_acc: 0.7792\n",
      "Epoch 42/100\n",
      "1101/1101 [==============================] - 0s 149us/sample - loss: 0.1686 - acc: 0.9328 - val_loss: 1.3793 - val_acc: 0.7922\n",
      "Epoch 43/100\n",
      "1101/1101 [==============================] - 0s 139us/sample - loss: 0.1867 - acc: 0.9273 - val_loss: 0.7465 - val_acc: 0.8182\n",
      "Epoch 44/100\n",
      "1101/1101 [==============================] - 0s 130us/sample - loss: 0.1647 - acc: 0.9328 - val_loss: 1.1856 - val_acc: 0.7922\n",
      "Epoch 45/100\n",
      "1101/1101 [==============================] - 0s 129us/sample - loss: 0.2064 - acc: 0.9355 - val_loss: 1.8113 - val_acc: 0.7143\n",
      "Epoch 46/100\n",
      "1101/1101 [==============================] - 0s 203us/sample - loss: 0.1920 - acc: 0.9273 - val_loss: 1.2738 - val_acc: 0.7792\n",
      "Epoch 47/100\n",
      "1101/1101 [==============================] - 0s 171us/sample - loss: 0.1730 - acc: 0.9437 - val_loss: 1.2698 - val_acc: 0.7792\n",
      "Epoch 48/100\n",
      "1101/1101 [==============================] - 0s 145us/sample - loss: 0.1507 - acc: 0.9410 - val_loss: 1.1544 - val_acc: 0.7143\n",
      "Epoch 49/100\n",
      "1101/1101 [==============================] - 0s 143us/sample - loss: 0.1678 - acc: 0.9428 - val_loss: 0.8061 - val_acc: 0.7792\n",
      "Epoch 50/100\n",
      "1101/1101 [==============================] - 0s 139us/sample - loss: 0.1581 - acc: 0.9373 - val_loss: 1.0559 - val_acc: 0.8052\n",
      "Epoch 51/100\n",
      "1101/1101 [==============================] - 0s 136us/sample - loss: 0.1801 - acc: 0.9310 - val_loss: 1.1347 - val_acc: 0.7922\n",
      "Epoch 52/100\n",
      "1101/1101 [==============================] - 0s 152us/sample - loss: 0.1323 - acc: 0.9455 - val_loss: 1.1854 - val_acc: 0.7922\n",
      "Epoch 53/100\n",
      "1101/1101 [==============================] - 0s 140us/sample - loss: 0.1733 - acc: 0.9355 - val_loss: 1.2552 - val_acc: 0.5974\n",
      "Epoch 54/100\n",
      "1101/1101 [==============================] - 0s 190us/sample - loss: 0.1666 - acc: 0.9319 - val_loss: 1.3650 - val_acc: 0.8052\n",
      "Epoch 55/100\n",
      "1101/1101 [==============================] - 0s 168us/sample - loss: 0.1330 - acc: 0.9500 - val_loss: 1.3328 - val_acc: 0.7922\n",
      "Epoch 56/100\n",
      "1101/1101 [==============================] - 0s 166us/sample - loss: 0.1422 - acc: 0.9455 - val_loss: 1.0965 - val_acc: 0.8052\n",
      "Epoch 57/100\n",
      "1101/1101 [==============================] - 0s 153us/sample - loss: 0.1589 - acc: 0.9437 - val_loss: 1.1767 - val_acc: 0.7792\n",
      "Epoch 58/100\n",
      "1101/1101 [==============================] - 0s 162us/sample - loss: 0.1799 - acc: 0.9391 - val_loss: 1.1584 - val_acc: 0.8052\n",
      "Epoch 59/100\n",
      "1101/1101 [==============================] - 0s 160us/sample - loss: 0.1227 - acc: 0.9510 - val_loss: 1.2553 - val_acc: 0.8052\n",
      "Epoch 60/100\n",
      "1101/1101 [==============================] - 0s 221us/sample - loss: 0.1527 - acc: 0.9437 - val_loss: 1.8894 - val_acc: 0.7662\n",
      "Epoch 61/100\n",
      "1101/1101 [==============================] - 0s 168us/sample - loss: 0.1860 - acc: 0.9382 - val_loss: 1.3205 - val_acc: 0.7792\n",
      "Epoch 62/100\n",
      "1101/1101 [==============================] - 0s 140us/sample - loss: 0.1287 - acc: 0.9491 - val_loss: 1.4401 - val_acc: 0.6883\n",
      "Epoch 63/100\n",
      "1101/1101 [==============================] - 0s 144us/sample - loss: 0.1397 - acc: 0.9510 - val_loss: 1.1073 - val_acc: 0.8052\n",
      "Epoch 64/100\n",
      "1101/1101 [==============================] - 0s 127us/sample - loss: 0.1341 - acc: 0.9482 - val_loss: 1.3137 - val_acc: 0.7792\n",
      "Epoch 65/100\n",
      "1101/1101 [==============================] - 0s 179us/sample - loss: 0.1535 - acc: 0.9410 - val_loss: 1.5952 - val_acc: 0.6623\n",
      "Epoch 66/100\n",
      "1101/1101 [==============================] - 0s 178us/sample - loss: 0.1470 - acc: 0.9428 - val_loss: 1.6903 - val_acc: 0.7792\n",
      "Epoch 67/100\n",
      "1101/1101 [==============================] - 0s 144us/sample - loss: 0.1716 - acc: 0.9391 - val_loss: 1.5916 - val_acc: 0.7143\n",
      "Epoch 68/100\n",
      "1101/1101 [==============================] - 0s 140us/sample - loss: 0.1313 - acc: 0.9528 - val_loss: 1.5275 - val_acc: 0.7792\n",
      "Epoch 69/100\n",
      "1101/1101 [==============================] - 0s 125us/sample - loss: 0.1621 - acc: 0.9482 - val_loss: 1.4154 - val_acc: 0.7922\n",
      "Epoch 70/100\n",
      "1101/1101 [==============================] - 0s 114us/sample - loss: 0.1249 - acc: 0.9455 - val_loss: 1.4655 - val_acc: 0.7403\n",
      "Epoch 71/100\n",
      "1101/1101 [==============================] - 0s 118us/sample - loss: 0.1228 - acc: 0.9446 - val_loss: 1.3375 - val_acc: 0.7662\n",
      "Epoch 72/100\n",
      "1101/1101 [==============================] - 0s 128us/sample - loss: 0.1253 - acc: 0.9555 - val_loss: 1.1861 - val_acc: 0.7662\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101/1101 [==============================] - 0s 133us/sample - loss: 0.1236 - acc: 0.9500 - val_loss: 0.8070 - val_acc: 0.7922\n",
      "Epoch 74/100\n",
      "1101/1101 [==============================] - 0s 122us/sample - loss: 0.1377 - acc: 0.9500 - val_loss: 1.4389 - val_acc: 0.7403\n",
      "Epoch 75/100\n",
      "1101/1101 [==============================] - 0s 119us/sample - loss: 0.1498 - acc: 0.9455 - val_loss: 1.7071 - val_acc: 0.7662\n",
      "Epoch 76/100\n",
      "1101/1101 [==============================] - 0s 137us/sample - loss: 0.1197 - acc: 0.9510 - val_loss: 1.4403 - val_acc: 0.7532\n",
      "Epoch 77/100\n",
      "1101/1101 [==============================] - 0s 201us/sample - loss: 0.1246 - acc: 0.9473 - val_loss: 1.4243 - val_acc: 0.7532\n",
      "Epoch 78/100\n",
      "1101/1101 [==============================] - 0s 148us/sample - loss: 0.1257 - acc: 0.9537 - val_loss: 2.0187 - val_acc: 0.7792\n",
      "Epoch 79/100\n",
      "1101/1101 [==============================] - 0s 141us/sample - loss: 0.1205 - acc: 0.9546 - val_loss: 1.7880 - val_acc: 0.7403\n",
      "Epoch 80/100\n",
      "1101/1101 [==============================] - 0s 122us/sample - loss: 0.1379 - acc: 0.9519 - val_loss: 1.1297 - val_acc: 0.7792\n",
      "Epoch 81/100\n",
      "1101/1101 [==============================] - 0s 122us/sample - loss: 0.1143 - acc: 0.9455 - val_loss: 2.1097 - val_acc: 0.7662\n",
      "Epoch 82/100\n",
      "1101/1101 [==============================] - 0s 118us/sample - loss: 0.1193 - acc: 0.9528 - val_loss: 1.5189 - val_acc: 0.7403\n",
      "Epoch 83/100\n",
      "1101/1101 [==============================] - 0s 126us/sample - loss: 0.1083 - acc: 0.9546 - val_loss: 1.4893 - val_acc: 0.7792\n",
      "Epoch 84/100\n",
      "1101/1101 [==============================] - 0s 132us/sample - loss: 0.1234 - acc: 0.9482 - val_loss: 1.3775 - val_acc: 0.7532\n",
      "Epoch 85/100\n",
      "1101/1101 [==============================] - 0s 158us/sample - loss: 0.1056 - acc: 0.9546 - val_loss: 1.9048 - val_acc: 0.7662\n",
      "Epoch 86/100\n",
      "1101/1101 [==============================] - 0s 159us/sample - loss: 0.1260 - acc: 0.9528 - val_loss: 2.1682 - val_acc: 0.7273\n",
      "Epoch 87/100\n",
      "1101/1101 [==============================] - 0s 130us/sample - loss: 0.1042 - acc: 0.9500 - val_loss: 2.0157 - val_acc: 0.7792\n",
      "Epoch 88/100\n",
      "1101/1101 [==============================] - 0s 157us/sample - loss: 0.1162 - acc: 0.9555 - val_loss: 1.9338 - val_acc: 0.7922\n",
      "Epoch 89/100\n",
      "1101/1101 [==============================] - 0s 158us/sample - loss: 0.1362 - acc: 0.9482 - val_loss: 1.4550 - val_acc: 0.7922\n",
      "Epoch 90/100\n",
      "1101/1101 [==============================] - 0s 145us/sample - loss: 0.1092 - acc: 0.9582 - val_loss: 2.1274 - val_acc: 0.7792\n",
      "Epoch 91/100\n",
      "1101/1101 [==============================] - 0s 143us/sample - loss: 0.1034 - acc: 0.9491 - val_loss: 2.2896 - val_acc: 0.7532\n",
      "Epoch 92/100\n",
      "1101/1101 [==============================] - 0s 137us/sample - loss: 0.1190 - acc: 0.9537 - val_loss: 1.3329 - val_acc: 0.7792\n",
      "Epoch 93/100\n",
      "1101/1101 [==============================] - 0s 141us/sample - loss: 0.1512 - acc: 0.9482 - val_loss: 1.2164 - val_acc: 0.7792\n",
      "Epoch 94/100\n",
      "1101/1101 [==============================] - 0s 140us/sample - loss: 0.1132 - acc: 0.9546 - val_loss: 1.2131 - val_acc: 0.8052\n",
      "Epoch 95/100\n",
      "1101/1101 [==============================] - 0s 141us/sample - loss: 0.0947 - acc: 0.9600 - val_loss: 1.6412 - val_acc: 0.7662\n",
      "Epoch 96/100\n",
      "1101/1101 [==============================] - 0s 137us/sample - loss: 0.1015 - acc: 0.9573 - val_loss: 2.1542 - val_acc: 0.7662\n",
      "Epoch 97/100\n",
      "1101/1101 [==============================] - 0s 134us/sample - loss: 0.0944 - acc: 0.9555 - val_loss: 2.0043 - val_acc: 0.7792\n",
      "Epoch 98/100\n",
      "1101/1101 [==============================] - 0s 123us/sample - loss: 0.0926 - acc: 0.9582 - val_loss: 2.1087 - val_acc: 0.7273\n",
      "Epoch 99/100\n",
      "1101/1101 [==============================] - 0s 135us/sample - loss: 0.1034 - acc: 0.9555 - val_loss: 1.7927 - val_acc: 0.7792\n",
      "Epoch 100/100\n",
      "1101/1101 [==============================] - 0s 145us/sample - loss: 0.1111 - acc: 0.9573 - val_loss: 2.0063 - val_acc: 0.7922\n",
      "\n",
      "Paciente:  4\n",
      "Train on 1102 samples, validate on 76 samples\n",
      "Epoch 1/100\n",
      "1102/1102 [==============================] - 1s 608us/sample - loss: 4.2196 - acc: 0.6534 - val_loss: 4.4521 - val_acc: 0.7105\n",
      "Epoch 2/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 3.7896 - acc: 0.6570 - val_loss: 4.4518 - val_acc: 0.7105\n",
      "Epoch 3/100\n",
      "1102/1102 [==============================] - 0s 119us/sample - loss: 2.4266 - acc: 0.6851 - val_loss: 0.7510 - val_acc: 0.8947\n",
      "Epoch 4/100\n",
      "1102/1102 [==============================] - 0s 129us/sample - loss: 1.4069 - acc: 0.7033 - val_loss: 0.4669 - val_acc: 0.8026\n",
      "Epoch 5/100\n",
      "1102/1102 [==============================] - 0s 127us/sample - loss: 0.8197 - acc: 0.7632 - val_loss: 0.8910 - val_acc: 0.7105\n",
      "Epoch 6/100\n",
      "1102/1102 [==============================] - 0s 150us/sample - loss: 0.6033 - acc: 0.7967 - val_loss: 1.1211 - val_acc: 0.7105\n",
      "Epoch 7/100\n",
      "1102/1102 [==============================] - 0s 118us/sample - loss: 0.5475 - acc: 0.7985 - val_loss: 1.0858 - val_acc: 0.7105\n",
      "Epoch 8/100\n",
      "1102/1102 [==============================] - 0s 139us/sample - loss: 0.4923 - acc: 0.8230 - val_loss: 0.4387 - val_acc: 0.8553\n",
      "Epoch 9/100\n",
      "1102/1102 [==============================] - 0s 133us/sample - loss: 0.5538 - acc: 0.7868 - val_loss: 1.1352 - val_acc: 0.7105\n",
      "Epoch 10/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.4064 - acc: 0.8485 - val_loss: 0.0962 - val_acc: 0.9737\n",
      "Epoch 11/100\n",
      "1102/1102 [==============================] - 0s 140us/sample - loss: 0.3817 - acc: 0.8648 - val_loss: 0.9128 - val_acc: 0.7368\n",
      "Epoch 12/100\n",
      "1102/1102 [==============================] - 0s 162us/sample - loss: 0.4084 - acc: 0.8294 - val_loss: 0.3971 - val_acc: 0.7500\n",
      "Epoch 13/100\n",
      "1102/1102 [==============================] - 0s 169us/sample - loss: 0.3392 - acc: 0.8639 - val_loss: 0.4997 - val_acc: 0.8684\n",
      "Epoch 14/100\n",
      "1102/1102 [==============================] - 0s 202us/sample - loss: 0.3498 - acc: 0.8593 - val_loss: 0.1235 - val_acc: 0.9474\n",
      "Epoch 15/100\n",
      "1102/1102 [==============================] - 0s 183us/sample - loss: 0.3139 - acc: 0.8766 - val_loss: 1.6126 - val_acc: 0.7105\n",
      "Epoch 16/100\n",
      "1102/1102 [==============================] - 0s 131us/sample - loss: 0.3186 - acc: 0.8938 - val_loss: 0.0175 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "1102/1102 [==============================] - 0s 128us/sample - loss: 0.3313 - acc: 0.8775 - val_loss: 0.0247 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "1102/1102 [==============================] - 0s 115us/sample - loss: 0.2835 - acc: 0.8947 - val_loss: 0.1870 - val_acc: 0.9211\n",
      "Epoch 19/100\n",
      "1102/1102 [==============================] - 0s 119us/sample - loss: 0.2749 - acc: 0.8911 - val_loss: 0.1367 - val_acc: 0.9474\n",
      "Epoch 20/100\n",
      "1102/1102 [==============================] - 0s 108us/sample - loss: 0.2702 - acc: 0.8884 - val_loss: 0.0262 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "1102/1102 [==============================] - 0s 127us/sample - loss: 0.2522 - acc: 0.9111 - val_loss: 0.0449 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "1102/1102 [==============================] - 0s 123us/sample - loss: 0.2482 - acc: 0.9011 - val_loss: 0.0236 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "1102/1102 [==============================] - 0s 116us/sample - loss: 0.2602 - acc: 0.8984 - val_loss: 0.0388 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "1102/1102 [==============================] - 0s 119us/sample - loss: 0.2401 - acc: 0.9074 - val_loss: 0.0445 - val_acc: 0.9868\n",
      "Epoch 25/100\n",
      "1102/1102 [==============================] - 0s 125us/sample - loss: 0.2246 - acc: 0.9111 - val_loss: 0.0198 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "1102/1102 [==============================] - 0s 135us/sample - loss: 0.2243 - acc: 0.9093 - val_loss: 0.0959 - val_acc: 0.9868\n",
      "Epoch 27/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.2366 - acc: 0.9138 - val_loss: 0.0562 - val_acc: 0.9868\n",
      "Epoch 28/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.2127 - acc: 0.9074 - val_loss: 0.0502 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "1102/1102 [==============================] - 0s 118us/sample - loss: 0.2315 - acc: 0.9129 - val_loss: 0.0774 - val_acc: 0.9605\n",
      "Epoch 30/100\n",
      "1102/1102 [==============================] - 0s 126us/sample - loss: 0.2623 - acc: 0.9111 - val_loss: 0.0371 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "1102/1102 [==============================] - 0s 117us/sample - loss: 0.2038 - acc: 0.9229 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.2208 - acc: 0.9265 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.2368 - acc: 0.9056 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "1102/1102 [==============================] - 0s 118us/sample - loss: 0.2197 - acc: 0.9174 - val_loss: 0.1419 - val_acc: 0.9079\n",
      "Epoch 35/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.2107 - acc: 0.9165 - val_loss: 0.0370 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "1102/1102 [==============================] - 0s 118us/sample - loss: 0.1912 - acc: 0.9211 - val_loss: 0.1769 - val_acc: 0.9079\n",
      "Epoch 37/100\n",
      "1102/1102 [==============================] - 0s 118us/sample - loss: 0.1701 - acc: 0.9328 - val_loss: 0.8495 - val_acc: 0.7368\n",
      "Epoch 38/100\n",
      "1102/1102 [==============================] - 0s 115us/sample - loss: 0.2006 - acc: 0.9283 - val_loss: 0.0845 - val_acc: 0.9868\n",
      "Epoch 39/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1898 - acc: 0.9301 - val_loss: 0.0272 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "1102/1102 [==============================] - 0s 129us/sample - loss: 0.1810 - acc: 0.9265 - val_loss: 0.1106 - val_acc: 0.9474\n",
      "Epoch 41/100\n",
      "1102/1102 [==============================] - 0s 125us/sample - loss: 0.1773 - acc: 0.9347 - val_loss: 0.0184 - val_acc: 0.9868\n",
      "Epoch 42/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.1752 - acc: 0.9292 - val_loss: 0.0137 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.2017 - acc: 0.9283 - val_loss: 0.0314 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1614 - acc: 0.9338 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1705 - acc: 0.9301 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "1102/1102 [==============================] - 0s 118us/sample - loss: 0.1910 - acc: 0.9319 - val_loss: 0.9875 - val_acc: 0.7105\n",
      "Epoch 47/100\n",
      "1102/1102 [==============================] - 0s 125us/sample - loss: 0.2155 - acc: 0.9301 - val_loss: 0.0421 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1498 - acc: 0.9310 - val_loss: 0.0214 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.1841 - acc: 0.9338 - val_loss: 0.0644 - val_acc: 0.9868\n",
      "Epoch 50/100\n",
      "1102/1102 [==============================] - 0s 119us/sample - loss: 0.1631 - acc: 0.9274 - val_loss: 0.0215 - val_acc: 0.9868\n",
      "Epoch 51/100\n",
      "1102/1102 [==============================] - 0s 119us/sample - loss: 0.1621 - acc: 0.9338 - val_loss: 0.0849 - val_acc: 0.9605\n",
      "Epoch 52/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1597 - acc: 0.9310 - val_loss: 0.0455 - val_acc: 0.9737\n",
      "Epoch 53/100\n",
      "1102/1102 [==============================] - 0s 114us/sample - loss: 0.1821 - acc: 0.9328 - val_loss: 0.0247 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "1102/1102 [==============================] - 0s 123us/sample - loss: 0.1439 - acc: 0.9374 - val_loss: 0.0398 - val_acc: 0.9868\n",
      "Epoch 55/100\n",
      "1102/1102 [==============================] - 0s 112us/sample - loss: 0.1413 - acc: 0.9428 - val_loss: 0.1767 - val_acc: 0.9079\n",
      "Epoch 56/100\n",
      "1102/1102 [==============================] - 0s 119us/sample - loss: 0.1612 - acc: 0.9347 - val_loss: 0.1140 - val_acc: 0.9474\n",
      "Epoch 57/100\n",
      "1102/1102 [==============================] - 0s 123us/sample - loss: 0.1479 - acc: 0.9365 - val_loss: 0.0651 - val_acc: 0.9868\n",
      "Epoch 58/100\n",
      "1102/1102 [==============================] - 0s 119us/sample - loss: 0.1687 - acc: 0.9338 - val_loss: 0.0602 - val_acc: 0.9737\n",
      "Epoch 59/100\n",
      "1102/1102 [==============================] - 0s 112us/sample - loss: 0.1746 - acc: 0.9383 - val_loss: 0.0550 - val_acc: 0.9868\n",
      "Epoch 60/100\n",
      "1102/1102 [==============================] - 0s 133us/sample - loss: 0.1291 - acc: 0.9410 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "1102/1102 [==============================] - 0s 131us/sample - loss: 0.1669 - acc: 0.9374 - val_loss: 0.0175 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "1102/1102 [==============================] - 0s 125us/sample - loss: 0.1498 - acc: 0.9428 - val_loss: 0.0509 - val_acc: 0.9737\n",
      "Epoch 63/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.1375 - acc: 0.9437 - val_loss: 0.0210 - val_acc: 0.9868\n",
      "Epoch 64/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.1754 - acc: 0.9365 - val_loss: 0.1100 - val_acc: 0.9474\n",
      "Epoch 65/100\n",
      "1102/1102 [==============================] - 0s 119us/sample - loss: 0.1281 - acc: 0.9446 - val_loss: 0.0826 - val_acc: 0.9605\n",
      "Epoch 66/100\n",
      "1102/1102 [==============================] - 0s 117us/sample - loss: 0.1357 - acc: 0.9456 - val_loss: 0.2718 - val_acc: 0.8684\n",
      "Epoch 67/100\n",
      "1102/1102 [==============================] - 0s 115us/sample - loss: 0.1545 - acc: 0.9383 - val_loss: 0.0572 - val_acc: 0.9737\n",
      "Epoch 68/100\n",
      "1102/1102 [==============================] - 0s 114us/sample - loss: 0.1289 - acc: 0.9437 - val_loss: 0.0470 - val_acc: 0.9868\n",
      "Epoch 69/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.1387 - acc: 0.9383 - val_loss: 0.0192 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "1102/1102 [==============================] - 0s 118us/sample - loss: 0.1270 - acc: 0.9428 - val_loss: 0.0156 - val_acc: 0.9868\n",
      "Epoch 71/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1485 - acc: 0.9356 - val_loss: 0.1016 - val_acc: 0.9605\n",
      "Epoch 72/100\n",
      "1102/1102 [==============================] - 0s 113us/sample - loss: 0.1418 - acc: 0.9501 - val_loss: 1.4542 - val_acc: 0.7368\n",
      "Epoch 73/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1315 - acc: 0.9537 - val_loss: 0.4389 - val_acc: 0.8026\n",
      "Epoch 74/100\n",
      "1102/1102 [==============================] - 0s 123us/sample - loss: 0.1384 - acc: 0.9483 - val_loss: 0.1066 - val_acc: 0.9737\n",
      "Epoch 75/100\n",
      "1102/1102 [==============================] - 0s 131us/sample - loss: 0.1437 - acc: 0.9492 - val_loss: 0.0748 - val_acc: 0.9605\n",
      "Epoch 76/100\n",
      "1102/1102 [==============================] - 0s 118us/sample - loss: 0.1026 - acc: 0.9501 - val_loss: 0.0290 - val_acc: 0.9868\n",
      "Epoch 77/100\n",
      "1102/1102 [==============================] - 0s 117us/sample - loss: 0.1565 - acc: 0.9483 - val_loss: 0.0956 - val_acc: 0.9605\n",
      "Epoch 78/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.1096 - acc: 0.9564 - val_loss: 0.2975 - val_acc: 0.8553\n",
      "Epoch 79/100\n",
      "1102/1102 [==============================] - 0s 106us/sample - loss: 0.1818 - acc: 0.9456 - val_loss: 0.1982 - val_acc: 0.9474\n",
      "Epoch 80/100\n",
      "1102/1102 [==============================] - 0s 116us/sample - loss: 0.1295 - acc: 0.9492 - val_loss: 0.3382 - val_acc: 0.8553\n",
      "Epoch 81/100\n",
      "1102/1102 [==============================] - 0s 114us/sample - loss: 0.1071 - acc: 0.9555 - val_loss: 0.0930 - val_acc: 0.9605\n",
      "Epoch 82/100\n",
      "1102/1102 [==============================] - 0s 122us/sample - loss: 0.1292 - acc: 0.9501 - val_loss: 0.3018 - val_acc: 0.8289\n",
      "Epoch 83/100\n",
      "1102/1102 [==============================] - 0s 112us/sample - loss: 0.1305 - acc: 0.9510 - val_loss: 0.6036 - val_acc: 0.8421\n",
      "Epoch 84/100\n",
      "1102/1102 [==============================] - 0s 117us/sample - loss: 0.1269 - acc: 0.9492 - val_loss: 0.1046 - val_acc: 0.9737\n",
      "Epoch 85/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.1413 - acc: 0.9501 - val_loss: 0.7497 - val_acc: 0.7632\n",
      "Epoch 86/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1096 - acc: 0.9546 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "1102/1102 [==============================] - 0s 116us/sample - loss: 0.1265 - acc: 0.9446 - val_loss: 0.0418 - val_acc: 0.9737\n",
      "Epoch 88/100\n",
      "1102/1102 [==============================] - 0s 117us/sample - loss: 0.1022 - acc: 0.9601 - val_loss: 0.0452 - val_acc: 0.9737\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.1188 - acc: 0.9501 - val_loss: 0.1053 - val_acc: 0.9474\n",
      "Epoch 90/100\n",
      "1102/1102 [==============================] - 0s 123us/sample - loss: 0.0992 - acc: 0.9592 - val_loss: 0.2457 - val_acc: 0.9079\n",
      "Epoch 91/100\n",
      "1102/1102 [==============================] - 0s 119us/sample - loss: 0.1342 - acc: 0.9592 - val_loss: 0.1229 - val_acc: 0.9605\n",
      "Epoch 92/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.0944 - acc: 0.9628 - val_loss: 0.0474 - val_acc: 0.9737\n",
      "Epoch 93/100\n",
      "1102/1102 [==============================] - 0s 126us/sample - loss: 0.0967 - acc: 0.9592 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "1102/1102 [==============================] - 0s 123us/sample - loss: 0.1492 - acc: 0.9537 - val_loss: 0.3380 - val_acc: 0.9211\n",
      "Epoch 95/100\n",
      "1102/1102 [==============================] - 0s 124us/sample - loss: 0.1112 - acc: 0.9601 - val_loss: 0.1561 - val_acc: 0.9605\n",
      "Epoch 96/100\n",
      "1102/1102 [==============================] - 0s 121us/sample - loss: 0.1570 - acc: 0.9537 - val_loss: 0.0782 - val_acc: 0.9737\n",
      "Epoch 97/100\n",
      "1102/1102 [==============================] - 0s 108us/sample - loss: 0.1156 - acc: 0.9528 - val_loss: 0.3819 - val_acc: 0.8816\n",
      "Epoch 98/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1097 - acc: 0.9601 - val_loss: 0.9092 - val_acc: 0.7500\n",
      "Epoch 99/100\n",
      "1102/1102 [==============================] - 0s 120us/sample - loss: 0.1165 - acc: 0.9483 - val_loss: 0.0360 - val_acc: 0.9868\n",
      "Epoch 100/100\n",
      "1102/1102 [==============================] - 0s 124us/sample - loss: 0.0988 - acc: 0.9610 - val_loss: 0.0699 - val_acc: 0.9737\n",
      "\n",
      "Paciente:  5\n",
      "Train on 1099 samples, validate on 79 samples\n",
      "Epoch 1/100\n",
      "1099/1099 [==============================] - 1s 587us/sample - loss: 4.3272 - acc: 0.6024 - val_loss: 4.4870 - val_acc: 0.7089\n",
      "Epoch 2/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 2.2480 - acc: 0.6451 - val_loss: 4.0774 - val_acc: 0.7089\n",
      "Epoch 3/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 1.1792 - acc: 0.6897 - val_loss: 0.6145 - val_acc: 0.7722\n",
      "Epoch 4/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.7127 - acc: 0.7498 - val_loss: 0.3428 - val_acc: 0.8734\n",
      "Epoch 5/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.5895 - acc: 0.7843 - val_loss: 2.7353 - val_acc: 0.7089\n",
      "Epoch 6/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.4862 - acc: 0.8080 - val_loss: 0.5339 - val_acc: 0.6835\n",
      "Epoch 7/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.4989 - acc: 0.7953 - val_loss: 0.7226 - val_acc: 0.7342\n",
      "Epoch 8/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.3717 - acc: 0.8544 - val_loss: 0.4706 - val_acc: 0.8481\n",
      "Epoch 9/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.3733 - acc: 0.8535 - val_loss: 0.4085 - val_acc: 0.8354\n",
      "Epoch 10/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.3832 - acc: 0.8571 - val_loss: 0.6725 - val_acc: 0.7848\n",
      "Epoch 11/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.3352 - acc: 0.8553 - val_loss: 1.0747 - val_acc: 0.7342\n",
      "Epoch 12/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.3059 - acc: 0.8662 - val_loss: 0.2330 - val_acc: 0.8987\n",
      "Epoch 13/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.3211 - acc: 0.8790 - val_loss: 0.0831 - val_acc: 0.9873\n",
      "Epoch 14/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.3135 - acc: 0.8708 - val_loss: 0.1866 - val_acc: 0.9494\n",
      "Epoch 15/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.2482 - acc: 0.8981 - val_loss: 0.3527 - val_acc: 0.8481\n",
      "Epoch 16/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.2706 - acc: 0.8935 - val_loss: 1.7295 - val_acc: 0.7089\n",
      "Epoch 17/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2808 - acc: 0.8954 - val_loss: 2.2601 - val_acc: 0.5443\n",
      "Epoch 18/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2562 - acc: 0.8935 - val_loss: 0.2034 - val_acc: 0.9494\n",
      "Epoch 19/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.2737 - acc: 0.8972 - val_loss: 0.1377 - val_acc: 0.9494\n",
      "Epoch 20/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.2651 - acc: 0.9045 - val_loss: 0.1829 - val_acc: 0.9367\n",
      "Epoch 21/100\n",
      "1099/1099 [==============================] - 0s 110us/sample - loss: 0.2425 - acc: 0.9081 - val_loss: 0.0321 - val_acc: 0.9873\n",
      "Epoch 22/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.2710 - acc: 0.9017 - val_loss: 0.0500 - val_acc: 0.9873\n",
      "Epoch 23/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.2390 - acc: 0.9063 - val_loss: 0.0789 - val_acc: 0.9873\n",
      "Epoch 24/100\n",
      "1099/1099 [==============================] - 0s 140us/sample - loss: 0.2668 - acc: 0.9045 - val_loss: 0.8958 - val_acc: 0.8101\n",
      "Epoch 25/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.2262 - acc: 0.9126 - val_loss: 0.9291 - val_acc: 0.7215\n",
      "Epoch 26/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.2455 - acc: 0.9054 - val_loss: 0.3005 - val_acc: 0.9241\n",
      "Epoch 27/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1916 - acc: 0.9227 - val_loss: 0.1159 - val_acc: 0.9620\n",
      "Epoch 28/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.2233 - acc: 0.9236 - val_loss: 0.0700 - val_acc: 0.9620\n",
      "Epoch 29/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.2234 - acc: 0.9126 - val_loss: 0.1337 - val_acc: 0.9620\n",
      "Epoch 30/100\n",
      "1099/1099 [==============================] - 0s 111us/sample - loss: 0.2098 - acc: 0.9136 - val_loss: 0.2913 - val_acc: 0.8228\n",
      "Epoch 31/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1931 - acc: 0.9154 - val_loss: 0.1083 - val_acc: 0.9494\n",
      "Epoch 32/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.2239 - acc: 0.9126 - val_loss: 1.1942 - val_acc: 0.7975\n",
      "Epoch 33/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.2077 - acc: 0.9154 - val_loss: 0.6597 - val_acc: 0.8861\n",
      "Epoch 34/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.2193 - acc: 0.9172 - val_loss: 0.0755 - val_acc: 0.9620\n",
      "Epoch 35/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.1915 - acc: 0.9245 - val_loss: 0.0324 - val_acc: 0.9873\n",
      "Epoch 36/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2209 - acc: 0.9136 - val_loss: 0.2652 - val_acc: 0.9367\n",
      "Epoch 37/100\n",
      "1099/1099 [==============================] - 0s 106us/sample - loss: 0.1910 - acc: 0.9290 - val_loss: 0.4760 - val_acc: 0.9241\n",
      "Epoch 38/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1933 - acc: 0.9308 - val_loss: 0.1115 - val_acc: 0.9620\n",
      "Epoch 39/100\n",
      "1099/1099 [==============================] - 0s 107us/sample - loss: 0.1769 - acc: 0.9327 - val_loss: 0.7535 - val_acc: 0.8101\n",
      "Epoch 40/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1781 - acc: 0.9281 - val_loss: 0.0532 - val_acc: 0.9747\n",
      "Epoch 41/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1883 - acc: 0.9290 - val_loss: 0.0617 - val_acc: 0.9747\n",
      "Epoch 42/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1495 - acc: 0.9436 - val_loss: 0.7394 - val_acc: 0.8734\n",
      "Epoch 43/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1896 - acc: 0.9308 - val_loss: 0.1938 - val_acc: 0.9367\n",
      "Epoch 44/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1759 - acc: 0.9299 - val_loss: 0.2934 - val_acc: 0.9114\n",
      "Epoch 45/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1845 - acc: 0.9245 - val_loss: 0.1593 - val_acc: 0.9367\n",
      "Epoch 46/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1559 - acc: 0.9372 - val_loss: 2.1286 - val_acc: 0.7215\n",
      "Epoch 47/100\n",
      "1099/1099 [==============================] - 0s 126us/sample - loss: 0.1687 - acc: 0.9290 - val_loss: 0.2815 - val_acc: 0.9367\n",
      "Epoch 48/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.1928 - acc: 0.9308 - val_loss: 0.0328 - val_acc: 0.9873\n",
      "Epoch 49/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1470 - acc: 0.9381 - val_loss: 0.1193 - val_acc: 0.9494\n",
      "Epoch 50/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1619 - acc: 0.9318 - val_loss: 0.0406 - val_acc: 0.9747\n",
      "Epoch 51/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1582 - acc: 0.9363 - val_loss: 0.0539 - val_acc: 0.9747\n",
      "Epoch 52/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.1508 - acc: 0.9399 - val_loss: 0.0517 - val_acc: 0.9620\n",
      "Epoch 53/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1421 - acc: 0.9409 - val_loss: 0.5573 - val_acc: 0.8861\n",
      "Epoch 54/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1576 - acc: 0.9418 - val_loss: 0.0114 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "1099/1099 [==============================] - 0s 128us/sample - loss: 0.1510 - acc: 0.9463 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1473 - acc: 0.9463 - val_loss: 0.2270 - val_acc: 0.9114\n",
      "Epoch 57/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1492 - acc: 0.9418 - val_loss: 0.0818 - val_acc: 0.9620\n",
      "Epoch 58/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1599 - acc: 0.9436 - val_loss: 0.2428 - val_acc: 0.9241\n",
      "Epoch 59/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1457 - acc: 0.9436 - val_loss: 0.2030 - val_acc: 0.9367\n",
      "Epoch 60/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1362 - acc: 0.9409 - val_loss: 0.0261 - val_acc: 0.9873\n",
      "Epoch 61/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1425 - acc: 0.9463 - val_loss: 0.6086 - val_acc: 0.8101\n",
      "Epoch 62/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1421 - acc: 0.9372 - val_loss: 0.0179 - val_acc: 0.9873\n",
      "Epoch 63/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1325 - acc: 0.9472 - val_loss: 0.0757 - val_acc: 0.9620\n",
      "Epoch 64/100\n",
      "1099/1099 [==============================] - 0s 110us/sample - loss: 0.1364 - acc: 0.9463 - val_loss: 0.4372 - val_acc: 0.8481\n",
      "Epoch 65/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1343 - acc: 0.9418 - val_loss: 0.0367 - val_acc: 0.9873\n",
      "Epoch 66/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1280 - acc: 0.9490 - val_loss: 0.1074 - val_acc: 0.9620\n",
      "Epoch 67/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1658 - acc: 0.9418 - val_loss: 0.4026 - val_acc: 0.9494\n",
      "Epoch 68/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1359 - acc: 0.9445 - val_loss: 0.2122 - val_acc: 0.9494\n",
      "Epoch 69/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.1537 - acc: 0.9381 - val_loss: 0.1904 - val_acc: 0.9494\n",
      "Epoch 70/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1313 - acc: 0.9490 - val_loss: 0.0409 - val_acc: 0.9747\n",
      "Epoch 71/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1342 - acc: 0.9472 - val_loss: 0.0746 - val_acc: 0.9620\n",
      "Epoch 72/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.1320 - acc: 0.9500 - val_loss: 0.2239 - val_acc: 0.9494\n",
      "Epoch 73/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1298 - acc: 0.9500 - val_loss: 0.0463 - val_acc: 0.9873\n",
      "Epoch 74/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1549 - acc: 0.9518 - val_loss: 0.0194 - val_acc: 0.9873\n",
      "Epoch 75/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1286 - acc: 0.9472 - val_loss: 0.0326 - val_acc: 0.9873\n",
      "Epoch 76/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1374 - acc: 0.9490 - val_loss: 0.0559 - val_acc: 0.9873\n",
      "Epoch 77/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.0988 - acc: 0.9609 - val_loss: 0.1899 - val_acc: 0.9620\n",
      "Epoch 78/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1352 - acc: 0.9500 - val_loss: 0.0281 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.1387 - acc: 0.9454 - val_loss: 0.1528 - val_acc: 0.9494\n",
      "Epoch 80/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1248 - acc: 0.9509 - val_loss: 0.0912 - val_acc: 0.9747\n",
      "Epoch 81/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1390 - acc: 0.9472 - val_loss: 0.0355 - val_acc: 0.9873\n",
      "Epoch 82/100\n",
      "1099/1099 [==============================] - 0s 111us/sample - loss: 0.1185 - acc: 0.9509 - val_loss: 0.0198 - val_acc: 0.9873\n",
      "Epoch 83/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1145 - acc: 0.9572 - val_loss: 0.1007 - val_acc: 0.9620\n",
      "Epoch 84/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1047 - acc: 0.9554 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1150 - acc: 0.9554 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1222 - acc: 0.9591 - val_loss: 0.0790 - val_acc: 0.9747\n",
      "Epoch 87/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1355 - acc: 0.9518 - val_loss: 0.0286 - val_acc: 0.9747\n",
      "Epoch 88/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1261 - acc: 0.9527 - val_loss: 2.4041 - val_acc: 0.6962\n",
      "Epoch 89/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1122 - acc: 0.9572 - val_loss: 0.0165 - val_acc: 0.9873\n",
      "Epoch 90/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1305 - acc: 0.9554 - val_loss: 0.0286 - val_acc: 0.9873\n",
      "Epoch 91/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.0846 - acc: 0.9663 - val_loss: 0.3206 - val_acc: 0.9367\n",
      "Epoch 92/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1066 - acc: 0.9618 - val_loss: 0.3148 - val_acc: 0.9494\n",
      "Epoch 93/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1312 - acc: 0.9527 - val_loss: 0.0345 - val_acc: 0.9873\n",
      "Epoch 94/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1024 - acc: 0.9591 - val_loss: 0.1379 - val_acc: 0.9494\n",
      "Epoch 95/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.0996 - acc: 0.9581 - val_loss: 0.0641 - val_acc: 0.9494\n",
      "Epoch 96/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1087 - acc: 0.9563 - val_loss: 0.0936 - val_acc: 0.9747\n",
      "Epoch 97/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1034 - acc: 0.9554 - val_loss: 0.0115 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1300 - acc: 0.9636 - val_loss: 0.0261 - val_acc: 0.9873\n",
      "Epoch 99/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1167 - acc: 0.9609 - val_loss: 0.0557 - val_acc: 0.9873\n",
      "Epoch 100/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1054 - acc: 0.9645 - val_loss: 0.0553 - val_acc: 0.9873\n",
      "\n",
      "Paciente:  6\n",
      "Train on 1100 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "1100/1100 [==============================] - 1s 813us/sample - loss: 3.7093 - acc: 0.5955 - val_loss: 2.2637 - val_acc: 0.7051\n",
      "Epoch 2/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 1.7690 - acc: 0.6464 - val_loss: 0.9406 - val_acc: 0.7051\n",
      "Epoch 3/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.9909 - acc: 0.6900 - val_loss: 0.6102 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.6224 - acc: 0.7236 - val_loss: 0.5668 - val_acc: 0.7821\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.5829 - acc: 0.7600 - val_loss: 0.2867 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.4442 - acc: 0.7982 - val_loss: 0.2015 - val_acc: 0.9615\n",
      "Epoch 7/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.4215 - acc: 0.8364 - val_loss: 0.1596 - val_acc: 0.9615\n",
      "Epoch 8/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.4144 - acc: 0.8191 - val_loss: 0.2092 - val_acc: 0.9103\n",
      "Epoch 9/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.3903 - acc: 0.8236 - val_loss: 0.2005 - val_acc: 0.8462\n",
      "Epoch 10/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.3397 - acc: 0.8591 - val_loss: 0.1523 - val_acc: 0.9487\n",
      "Epoch 11/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.2970 - acc: 0.8800 - val_loss: 0.2108 - val_acc: 0.8974\n",
      "Epoch 12/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.3232 - acc: 0.8636 - val_loss: 0.1464 - val_acc: 0.9231\n",
      "Epoch 13/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.2841 - acc: 0.8800 - val_loss: 0.1126 - val_acc: 0.9487\n",
      "Epoch 14/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.3259 - acc: 0.8673 - val_loss: 0.1275 - val_acc: 0.9615\n",
      "Epoch 15/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.2697 - acc: 0.8864 - val_loss: 0.0877 - val_acc: 0.9872\n",
      "Epoch 16/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.2775 - acc: 0.8782 - val_loss: 0.0691 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.2589 - acc: 0.9036 - val_loss: 0.1109 - val_acc: 0.9872\n",
      "Epoch 18/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.2775 - acc: 0.8909 - val_loss: 0.0978 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "1100/1100 [==============================] - 0s 111us/sample - loss: 0.2600 - acc: 0.8964 - val_loss: 0.0618 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.2291 - acc: 0.9027 - val_loss: 0.0376 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.2239 - acc: 0.9127 - val_loss: 0.1275 - val_acc: 0.9615\n",
      "Epoch 22/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.2385 - acc: 0.9082 - val_loss: 0.0853 - val_acc: 0.9872\n",
      "Epoch 23/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.2110 - acc: 0.9109 - val_loss: 0.0453 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.2179 - acc: 0.9127 - val_loss: 0.0855 - val_acc: 0.9744\n",
      "Epoch 25/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.2249 - acc: 0.9109 - val_loss: 0.0516 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.2268 - acc: 0.9136 - val_loss: 0.0647 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "1100/1100 [==============================] - 0s 118us/sample - loss: 0.2073 - acc: 0.9209 - val_loss: 0.0485 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.2035 - acc: 0.9164 - val_loss: 0.0873 - val_acc: 0.9872\n",
      "Epoch 29/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1888 - acc: 0.9227 - val_loss: 0.0542 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.2192 - acc: 0.9155 - val_loss: 0.0426 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1967 - acc: 0.9264 - val_loss: 0.0221 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "1100/1100 [==============================] - 0s 101us/sample - loss: 0.1829 - acc: 0.9245 - val_loss: 0.0591 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "1100/1100 [==============================] - 0s 112us/sample - loss: 0.2000 - acc: 0.9173 - val_loss: 0.0602 - val_acc: 0.9872\n",
      "Epoch 34/100\n",
      "1100/1100 [==============================] - 0s 99us/sample - loss: 0.1797 - acc: 0.9255 - val_loss: 0.0676 - val_acc: 0.9872\n",
      "Epoch 35/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1672 - acc: 0.9282 - val_loss: 0.0377 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1829 - acc: 0.9273 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1782 - acc: 0.9264 - val_loss: 0.0306 - val_acc: 0.9872\n",
      "Epoch 38/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1900 - acc: 0.9164 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1793 - acc: 0.9282 - val_loss: 0.0625 - val_acc: 0.9872\n",
      "Epoch 40/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1639 - acc: 0.9318 - val_loss: 0.0350 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "1100/1100 [==============================] - 0s 101us/sample - loss: 0.1670 - acc: 0.9291 - val_loss: 0.0437 - val_acc: 0.9872\n",
      "Epoch 42/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1517 - acc: 0.9382 - val_loss: 0.1470 - val_acc: 0.9231\n",
      "Epoch 43/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1733 - acc: 0.9282 - val_loss: 0.0527 - val_acc: 0.9872\n",
      "Epoch 44/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1783 - acc: 0.9264 - val_loss: 0.0518 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1468 - acc: 0.9382 - val_loss: 0.0281 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1930 - acc: 0.9364 - val_loss: 0.0296 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1666 - acc: 0.9291 - val_loss: 0.0490 - val_acc: 0.9872\n",
      "Epoch 48/100\n",
      "1100/1100 [==============================] - 0s 101us/sample - loss: 0.1488 - acc: 0.9364 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1657 - acc: 0.9300 - val_loss: 0.0536 - val_acc: 0.9872\n",
      "Epoch 50/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1576 - acc: 0.9355 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "1100/1100 [==============================] - 0s 102us/sample - loss: 0.1526 - acc: 0.9427 - val_loss: 0.0456 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "1100/1100 [==============================] - 0s 102us/sample - loss: 0.1784 - acc: 0.9391 - val_loss: 0.0373 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1557 - acc: 0.9382 - val_loss: 0.0830 - val_acc: 0.9615\n",
      "Epoch 54/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1325 - acc: 0.9482 - val_loss: 0.0565 - val_acc: 0.9872\n",
      "Epoch 55/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1768 - acc: 0.9373 - val_loss: 0.0305 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1419 - acc: 0.9418 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1639 - acc: 0.9400 - val_loss: 0.0954 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1615 - acc: 0.9345 - val_loss: 0.0502 - val_acc: 0.9872\n",
      "Epoch 59/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1722 - acc: 0.9327 - val_loss: 0.0415 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1705 - acc: 0.9345 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "1100/1100 [==============================] - 0s 102us/sample - loss: 0.1797 - acc: 0.9327 - val_loss: 0.0885 - val_acc: 0.9744\n",
      "Epoch 62/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1362 - acc: 0.9373 - val_loss: 0.0498 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1374 - acc: 0.9509 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "1100/1100 [==============================] - 0s 113us/sample - loss: 0.1555 - acc: 0.9445 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1408 - acc: 0.9400 - val_loss: 0.0293 - val_acc: 0.9872\n",
      "Epoch 66/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1331 - acc: 0.9482 - val_loss: 0.0171 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.1761 - acc: 0.9309 - val_loss: 0.0395 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1393 - acc: 0.9427 - val_loss: 0.0269 - val_acc: 0.9872\n",
      "Epoch 69/100\n",
      "1100/1100 [==============================] - 0s 115us/sample - loss: 0.1343 - acc: 0.9473 - val_loss: 0.0271 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1406 - acc: 0.9455 - val_loss: 0.1183 - val_acc: 0.9615\n",
      "Epoch 71/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1131 - acc: 0.9455 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "1100/1100 [==============================] - 0s 110us/sample - loss: 0.1658 - acc: 0.9436 - val_loss: 0.0244 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.1327 - acc: 0.9464 - val_loss: 0.0235 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1296 - acc: 0.9418 - val_loss: 0.0542 - val_acc: 0.9872\n",
      "Epoch 75/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1419 - acc: 0.9427 - val_loss: 0.0148 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1659 - acc: 0.9409 - val_loss: 0.0394 - val_acc: 0.9872\n",
      "Epoch 77/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1215 - acc: 0.9500 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1432 - acc: 0.9418 - val_loss: 0.0297 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "1100/1100 [==============================] - 0s 120us/sample - loss: 0.1154 - acc: 0.9536 - val_loss: 0.1667 - val_acc: 0.9103\n",
      "Epoch 80/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1323 - acc: 0.9473 - val_loss: 0.0783 - val_acc: 0.9615\n",
      "Epoch 81/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.1389 - acc: 0.9445 - val_loss: 0.0283 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1394 - acc: 0.9464 - val_loss: 0.0251 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1228 - acc: 0.9445 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1414 - acc: 0.9491 - val_loss: 0.0300 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1574 - acc: 0.9491 - val_loss: 0.1545 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1295 - acc: 0.9564 - val_loss: 0.0266 - val_acc: 0.9872\n",
      "Epoch 87/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1481 - acc: 0.9464 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "1100/1100 [==============================] - 0s 102us/sample - loss: 0.1485 - acc: 0.9555 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "1100/1100 [==============================] - 0s 110us/sample - loss: 0.1082 - acc: 0.9564 - val_loss: 0.1535 - val_acc: 0.9359\n",
      "Epoch 90/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.0987 - acc: 0.9564 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "1100/1100 [==============================] - 0s 100us/sample - loss: 0.1100 - acc: 0.9527 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1404 - acc: 0.9445 - val_loss: 0.0249 - val_acc: 0.9872\n",
      "Epoch 93/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1158 - acc: 0.9536 - val_loss: 0.1090 - val_acc: 0.9872\n",
      "Epoch 94/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1053 - acc: 0.9618 - val_loss: 0.0478 - val_acc: 0.9872\n",
      "Epoch 95/100\n",
      "1100/1100 [==============================] - 0s 100us/sample - loss: 0.0973 - acc: 0.9600 - val_loss: 0.0642 - val_acc: 0.9872\n",
      "Epoch 96/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1321 - acc: 0.9555 - val_loss: 0.0211 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "1100/1100 [==============================] - 0s 101us/sample - loss: 0.1192 - acc: 0.9518 - val_loss: 0.0639 - val_acc: 0.9872\n",
      "Epoch 98/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1095 - acc: 0.9582 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "1100/1100 [==============================] - 0s 101us/sample - loss: 0.1375 - acc: 0.9618 - val_loss: 0.0292 - val_acc: 0.9872\n",
      "Epoch 100/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.0926 - acc: 0.9627 - val_loss: 0.0288 - val_acc: 0.9872\n",
      "\n",
      "Paciente:  8\n",
      "Train on 1099 samples, validate on 79 samples\n",
      "Epoch 1/100\n",
      "1099/1099 [==============================] - 1s 691us/sample - loss: 4.0335 - acc: 0.6561 - val_loss: 2.4729 - val_acc: 0.3544\n",
      "Epoch 2/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 2.9712 - acc: 0.6242 - val_loss: 5.2755 - val_acc: 0.3924\n",
      "Epoch 3/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 1.8415 - acc: 0.6588 - val_loss: 1.2644 - val_acc: 0.6962\n",
      "Epoch 4/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.9313 - acc: 0.7334 - val_loss: 0.9989 - val_acc: 0.7089\n",
      "Epoch 5/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.7867 - acc: 0.7571 - val_loss: 0.4437 - val_acc: 0.9241\n",
      "Epoch 6/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.6015 - acc: 0.7843 - val_loss: 0.4668 - val_acc: 0.8228\n",
      "Epoch 7/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.5369 - acc: 0.7898 - val_loss: 0.1974 - val_acc: 0.9367\n",
      "Epoch 8/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.4034 - acc: 0.8371 - val_loss: 0.5775 - val_acc: 0.7595\n",
      "Epoch 9/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.4302 - acc: 0.8326 - val_loss: 0.5700 - val_acc: 0.7595\n",
      "Epoch 10/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.3877 - acc: 0.8399 - val_loss: 0.2222 - val_acc: 0.9367\n",
      "Epoch 11/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.3359 - acc: 0.8626 - val_loss: 0.1902 - val_acc: 0.9114\n",
      "Epoch 12/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.3570 - acc: 0.8699 - val_loss: 0.3612 - val_acc: 0.8228\n",
      "Epoch 13/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.3045 - acc: 0.8890 - val_loss: 1.5454 - val_acc: 0.7342\n",
      "Epoch 14/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.3167 - acc: 0.8817 - val_loss: 0.0951 - val_acc: 0.9873\n",
      "Epoch 15/100\n",
      "1099/1099 [==============================] - 0s 112us/sample - loss: 0.2861 - acc: 0.8981 - val_loss: 0.7031 - val_acc: 0.7722\n",
      "Epoch 16/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.2907 - acc: 0.8890 - val_loss: 1.0907 - val_acc: 0.7468\n",
      "Epoch 17/100\n",
      "1099/1099 [==============================] - 0s 105us/sample - loss: 0.3027 - acc: 0.8881 - val_loss: 0.1052 - val_acc: 0.9747\n",
      "Epoch 18/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.2484 - acc: 0.8972 - val_loss: 0.4430 - val_acc: 0.7848\n",
      "Epoch 19/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.2626 - acc: 0.8963 - val_loss: 0.0638 - val_acc: 0.9873\n",
      "Epoch 20/100\n",
      "1099/1099 [==============================] - 0s 126us/sample - loss: 0.2466 - acc: 0.9063 - val_loss: 0.2852 - val_acc: 0.8861\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.2487 - acc: 0.9026 - val_loss: 0.0715 - val_acc: 0.9873\n",
      "Epoch 22/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.2125 - acc: 0.9099 - val_loss: 0.0950 - val_acc: 0.9620\n",
      "Epoch 23/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.2218 - acc: 0.9035 - val_loss: 0.0449 - val_acc: 0.9873\n",
      "Epoch 24/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.2598 - acc: 0.9072 - val_loss: 0.0648 - val_acc: 0.9873\n",
      "Epoch 25/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.2365 - acc: 0.9108 - val_loss: 0.0736 - val_acc: 0.9873\n",
      "Epoch 26/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2078 - acc: 0.9136 - val_loss: 0.4600 - val_acc: 0.8861\n",
      "Epoch 27/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.2342 - acc: 0.9090 - val_loss: 0.3572 - val_acc: 0.8861\n",
      "Epoch 28/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1953 - acc: 0.9254 - val_loss: 0.1930 - val_acc: 0.8987\n",
      "Epoch 29/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1849 - acc: 0.9272 - val_loss: 0.1292 - val_acc: 0.9620\n",
      "Epoch 30/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.2001 - acc: 0.9272 - val_loss: 0.1505 - val_acc: 0.9494\n",
      "Epoch 31/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.2107 - acc: 0.9199 - val_loss: 0.0911 - val_acc: 0.9494\n",
      "Epoch 32/100\n",
      "1099/1099 [==============================] - 0s 107us/sample - loss: 0.1895 - acc: 0.9181 - val_loss: 0.0739 - val_acc: 0.9620\n",
      "Epoch 33/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2001 - acc: 0.9126 - val_loss: 0.0639 - val_acc: 0.9747\n",
      "Epoch 34/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1791 - acc: 0.9263 - val_loss: 0.0763 - val_acc: 0.9747\n",
      "Epoch 35/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1760 - acc: 0.9327 - val_loss: 0.1736 - val_acc: 0.9241\n",
      "Epoch 36/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1760 - acc: 0.9299 - val_loss: 0.0965 - val_acc: 0.9747\n",
      "Epoch 37/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.2098 - acc: 0.9254 - val_loss: 0.0795 - val_acc: 0.9747\n",
      "Epoch 38/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1684 - acc: 0.9336 - val_loss: 0.1030 - val_acc: 0.9620\n",
      "Epoch 39/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1631 - acc: 0.9427 - val_loss: 0.0854 - val_acc: 0.9620\n",
      "Epoch 40/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1938 - acc: 0.9236 - val_loss: 0.1238 - val_acc: 0.9367\n",
      "Epoch 41/100\n",
      "1099/1099 [==============================] - 0s 106us/sample - loss: 0.1789 - acc: 0.9299 - val_loss: 0.1941 - val_acc: 0.9367\n",
      "Epoch 42/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1694 - acc: 0.9272 - val_loss: 0.0749 - val_acc: 0.9620\n",
      "Epoch 43/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1479 - acc: 0.9409 - val_loss: 0.1802 - val_acc: 0.9494\n",
      "Epoch 44/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.2204 - acc: 0.9254 - val_loss: 0.0650 - val_acc: 0.9747\n",
      "Epoch 45/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1487 - acc: 0.9445 - val_loss: 0.1639 - val_acc: 0.9241\n",
      "Epoch 46/100\n",
      "1099/1099 [==============================] - 0s 107us/sample - loss: 0.1610 - acc: 0.9327 - val_loss: 0.1563 - val_acc: 0.9620\n",
      "Epoch 47/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1628 - acc: 0.9390 - val_loss: 0.1637 - val_acc: 0.9114\n",
      "Epoch 48/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1917 - acc: 0.9345 - val_loss: 0.1616 - val_acc: 0.9494\n",
      "Epoch 49/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1809 - acc: 0.9372 - val_loss: 0.0716 - val_acc: 0.9620\n",
      "Epoch 50/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1401 - acc: 0.9390 - val_loss: 0.2044 - val_acc: 0.9114\n",
      "Epoch 51/100\n",
      "1099/1099 [==============================] - 0s 127us/sample - loss: 0.1458 - acc: 0.9399 - val_loss: 0.1006 - val_acc: 0.9620\n",
      "Epoch 52/100\n",
      "1099/1099 [==============================] - 0s 129us/sample - loss: 0.1461 - acc: 0.9427 - val_loss: 1.6315 - val_acc: 0.8101\n",
      "Epoch 53/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1589 - acc: 0.9381 - val_loss: 0.1040 - val_acc: 0.9620\n",
      "Epoch 54/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1501 - acc: 0.9290 - val_loss: 0.4655 - val_acc: 0.9367\n",
      "Epoch 55/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1391 - acc: 0.9390 - val_loss: 0.2056 - val_acc: 0.9620\n",
      "Epoch 56/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1497 - acc: 0.9427 - val_loss: 0.0548 - val_acc: 0.9747\n",
      "Epoch 57/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1425 - acc: 0.9436 - val_loss: 0.1248 - val_acc: 0.9620\n",
      "Epoch 58/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1515 - acc: 0.9381 - val_loss: 0.6707 - val_acc: 0.9241\n",
      "Epoch 59/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1440 - acc: 0.9445 - val_loss: 0.4372 - val_acc: 0.8861\n",
      "Epoch 60/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1354 - acc: 0.9445 - val_loss: 0.1448 - val_acc: 0.9494\n",
      "Epoch 61/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1528 - acc: 0.9445 - val_loss: 0.3327 - val_acc: 0.9241\n",
      "Epoch 62/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1515 - acc: 0.9372 - val_loss: 0.1884 - val_acc: 0.9494\n",
      "Epoch 63/100\n",
      "1099/1099 [==============================] - 0s 107us/sample - loss: 0.1589 - acc: 0.9409 - val_loss: 0.0837 - val_acc: 0.9620\n",
      "Epoch 64/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1445 - acc: 0.9390 - val_loss: 0.2659 - val_acc: 0.9367\n",
      "Epoch 65/100\n",
      "1099/1099 [==============================] - 0s 111us/sample - loss: 0.1249 - acc: 0.9500 - val_loss: 0.6152 - val_acc: 0.8987\n",
      "Epoch 66/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1608 - acc: 0.9399 - val_loss: 0.1996 - val_acc: 0.9620\n",
      "Epoch 67/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1316 - acc: 0.9436 - val_loss: 0.1858 - val_acc: 0.9620\n",
      "Epoch 68/100\n",
      "1099/1099 [==============================] - 0s 107us/sample - loss: 0.1494 - acc: 0.9336 - val_loss: 0.4326 - val_acc: 0.9241\n",
      "Epoch 69/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1262 - acc: 0.9509 - val_loss: 0.4872 - val_acc: 0.9367\n",
      "Epoch 70/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1389 - acc: 0.9454 - val_loss: 0.1503 - val_acc: 0.9620\n",
      "Epoch 71/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.1084 - acc: 0.9545 - val_loss: 0.1946 - val_acc: 0.9367\n",
      "Epoch 72/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1198 - acc: 0.9481 - val_loss: 0.2136 - val_acc: 0.9241\n",
      "Epoch 73/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1181 - acc: 0.9490 - val_loss: 0.2201 - val_acc: 0.9494\n",
      "Epoch 74/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1642 - acc: 0.9481 - val_loss: 0.4677 - val_acc: 0.9241\n",
      "Epoch 75/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.1350 - acc: 0.9390 - val_loss: 0.2439 - val_acc: 0.9620\n",
      "Epoch 76/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1096 - acc: 0.9509 - val_loss: 0.6865 - val_acc: 0.9114\n",
      "Epoch 77/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.1337 - acc: 0.9490 - val_loss: 0.2557 - val_acc: 0.9620\n",
      "Epoch 78/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1240 - acc: 0.9500 - val_loss: 0.2487 - val_acc: 0.9620\n",
      "Epoch 79/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1184 - acc: 0.9481 - val_loss: 0.5950 - val_acc: 0.9241\n",
      "Epoch 80/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1477 - acc: 0.9445 - val_loss: 0.0953 - val_acc: 0.9620\n",
      "Epoch 81/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1121 - acc: 0.9481 - val_loss: 0.1416 - val_acc: 0.9620\n",
      "Epoch 82/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1282 - acc: 0.9536 - val_loss: 0.2301 - val_acc: 0.9620\n",
      "Epoch 83/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1245 - acc: 0.9536 - val_loss: 0.1549 - val_acc: 0.9241\n",
      "Epoch 84/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1430 - acc: 0.9509 - val_loss: 0.1561 - val_acc: 0.9620\n",
      "Epoch 85/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.1460 - acc: 0.9445 - val_loss: 0.1900 - val_acc: 0.9367\n",
      "Epoch 86/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1045 - acc: 0.9581 - val_loss: 0.1752 - val_acc: 0.9620\n",
      "Epoch 87/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1166 - acc: 0.9527 - val_loss: 0.3016 - val_acc: 0.9620\n",
      "Epoch 88/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.1435 - acc: 0.9554 - val_loss: 0.2323 - val_acc: 0.9620\n",
      "Epoch 89/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1042 - acc: 0.9554 - val_loss: 0.1184 - val_acc: 0.9620\n",
      "Epoch 90/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1152 - acc: 0.9536 - val_loss: 0.1457 - val_acc: 0.9367\n",
      "Epoch 91/100\n",
      "1099/1099 [==============================] - 0s 126us/sample - loss: 0.1086 - acc: 0.9554 - val_loss: 0.3182 - val_acc: 0.9241\n",
      "Epoch 92/100\n",
      "1099/1099 [==============================] - 0s 126us/sample - loss: 0.1406 - acc: 0.9563 - val_loss: 0.2715 - val_acc: 0.9747\n",
      "Epoch 93/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1489 - acc: 0.9500 - val_loss: 0.2013 - val_acc: 0.9620\n",
      "Epoch 94/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1038 - acc: 0.9572 - val_loss: 0.2845 - val_acc: 0.9620\n",
      "Epoch 95/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1185 - acc: 0.9572 - val_loss: 0.2495 - val_acc: 0.9620\n",
      "Epoch 96/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1190 - acc: 0.9490 - val_loss: 0.2410 - val_acc: 0.9620\n",
      "Epoch 97/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1151 - acc: 0.9627 - val_loss: 0.2508 - val_acc: 0.9620\n",
      "Epoch 98/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1116 - acc: 0.9554 - val_loss: 0.2553 - val_acc: 0.9620\n",
      "Epoch 99/100\n",
      "1099/1099 [==============================] - 0s 107us/sample - loss: 0.1158 - acc: 0.9563 - val_loss: 0.3796 - val_acc: 0.9367\n",
      "Epoch 100/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.0999 - acc: 0.9581 - val_loss: 0.2405 - val_acc: 0.9494\n",
      "\n",
      "Paciente:  9\n",
      "Train on 1100 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "1100/1100 [==============================] - 1s 633us/sample - loss: 3.7710 - acc: 0.6327 - val_loss: 2.4762 - val_acc: 0.7051\n",
      "Epoch 2/100\n",
      "1100/1100 [==============================] - 0s 101us/sample - loss: 1.3170 - acc: 0.6564 - val_loss: 0.9567 - val_acc: 0.5385\n",
      "Epoch 3/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.8751 - acc: 0.7000 - val_loss: 0.5217 - val_acc: 0.7051\n",
      "Epoch 4/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.7152 - acc: 0.7364 - val_loss: 0.5806 - val_acc: 0.7179\n",
      "Epoch 5/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.6574 - acc: 0.7445 - val_loss: 0.3132 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.5761 - acc: 0.7709 - val_loss: 0.2611 - val_acc: 0.8974\n",
      "Epoch 7/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.5022 - acc: 0.8200 - val_loss: 0.3741 - val_acc: 0.7564\n",
      "Epoch 8/100\n",
      "1100/1100 [==============================] - 0s 110us/sample - loss: 0.4210 - acc: 0.8182 - val_loss: 0.1815 - val_acc: 0.9615\n",
      "Epoch 9/100\n",
      "1100/1100 [==============================] - 0s 115us/sample - loss: 0.4194 - acc: 0.8209 - val_loss: 0.3093 - val_acc: 0.8205\n",
      "Epoch 10/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.3383 - acc: 0.8455 - val_loss: 0.1609 - val_acc: 0.9744\n",
      "Epoch 11/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.3420 - acc: 0.8618 - val_loss: 0.1492 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.3323 - acc: 0.8591 - val_loss: 0.1229 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.3201 - acc: 0.8791 - val_loss: 0.0980 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.2953 - acc: 0.8900 - val_loss: 0.1289 - val_acc: 0.9487\n",
      "Epoch 15/100\n",
      "1100/1100 [==============================] - 0s 102us/sample - loss: 0.3079 - acc: 0.8818 - val_loss: 0.1035 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "1100/1100 [==============================] - 0s 100us/sample - loss: 0.2601 - acc: 0.8882 - val_loss: 0.0928 - val_acc: 0.9872\n",
      "Epoch 17/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.2938 - acc: 0.8900 - val_loss: 0.1263 - val_acc: 0.9487\n",
      "Epoch 18/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.2409 - acc: 0.9018 - val_loss: 0.1378 - val_acc: 0.9231\n",
      "Epoch 19/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.2577 - acc: 0.8909 - val_loss: 0.1152 - val_acc: 0.9615\n",
      "Epoch 20/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.2722 - acc: 0.8964 - val_loss: 0.1181 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.2304 - acc: 0.9064 - val_loss: 0.2165 - val_acc: 0.9103\n",
      "Epoch 22/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.2494 - acc: 0.8964 - val_loss: 0.1423 - val_acc: 0.9359\n",
      "Epoch 23/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.2339 - acc: 0.9018 - val_loss: 0.0745 - val_acc: 0.9744\n",
      "Epoch 24/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.2434 - acc: 0.8955 - val_loss: 0.0660 - val_acc: 0.9872\n",
      "Epoch 25/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.2386 - acc: 0.9118 - val_loss: 0.0714 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.2028 - acc: 0.9173 - val_loss: 0.0785 - val_acc: 0.9744\n",
      "Epoch 27/100\n",
      "1100/1100 [==============================] - 0s 101us/sample - loss: 0.2231 - acc: 0.9136 - val_loss: 0.0751 - val_acc: 0.9744\n",
      "Epoch 28/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.2190 - acc: 0.9091 - val_loss: 0.0843 - val_acc: 0.9744\n",
      "Epoch 29/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.2255 - acc: 0.9082 - val_loss: 0.1609 - val_acc: 0.9359\n",
      "Epoch 30/100\n",
      "1100/1100 [==============================] - 0s 100us/sample - loss: 0.1812 - acc: 0.9318 - val_loss: 0.1971 - val_acc: 0.9615\n",
      "Epoch 31/100\n",
      "1100/1100 [==============================] - 0s 98us/sample - loss: 0.1981 - acc: 0.9145 - val_loss: 0.2752 - val_acc: 0.9359\n",
      "Epoch 32/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1716 - acc: 0.9282 - val_loss: 0.0233 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.2071 - acc: 0.9182 - val_loss: 0.4765 - val_acc: 0.8974\n",
      "Epoch 34/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1857 - acc: 0.9245 - val_loss: 0.0563 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "1100/1100 [==============================] - 0s 110us/sample - loss: 0.1662 - acc: 0.9291 - val_loss: 0.0569 - val_acc: 0.9872\n",
      "Epoch 36/100\n",
      "1100/1100 [==============================] - 0s 101us/sample - loss: 0.1631 - acc: 0.9364 - val_loss: 0.0158 - val_acc: 1.0000\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100/1100 [==============================] - 0s 110us/sample - loss: 0.2363 - acc: 0.9236 - val_loss: 0.1523 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1857 - acc: 0.9273 - val_loss: 0.1384 - val_acc: 0.9615\n",
      "Epoch 39/100\n",
      "1100/1100 [==============================] - 0s 110us/sample - loss: 0.1617 - acc: 0.9282 - val_loss: 0.0780 - val_acc: 0.9744\n",
      "Epoch 40/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.1839 - acc: 0.9227 - val_loss: 0.0701 - val_acc: 0.9615\n",
      "Epoch 41/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1662 - acc: 0.9291 - val_loss: 0.2806 - val_acc: 0.9359\n",
      "Epoch 42/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1540 - acc: 0.9400 - val_loss: 0.1984 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "1100/1100 [==============================] - 0s 110us/sample - loss: 0.1598 - acc: 0.9400 - val_loss: 0.3632 - val_acc: 0.9231\n",
      "Epoch 44/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.1685 - acc: 0.9309 - val_loss: 0.0616 - val_acc: 0.9744\n",
      "Epoch 45/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.1761 - acc: 0.9273 - val_loss: 0.0308 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.1764 - acc: 0.9200 - val_loss: 0.0883 - val_acc: 0.9487\n",
      "Epoch 47/100\n",
      "1100/1100 [==============================] - 0s 113us/sample - loss: 0.1611 - acc: 0.9373 - val_loss: 0.1445 - val_acc: 0.9487\n",
      "Epoch 48/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1510 - acc: 0.9345 - val_loss: 0.2314 - val_acc: 0.9103\n",
      "Epoch 49/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1655 - acc: 0.9355 - val_loss: 0.0826 - val_acc: 0.9744\n",
      "Epoch 50/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1697 - acc: 0.9318 - val_loss: 0.1242 - val_acc: 0.9744\n",
      "Epoch 51/100\n",
      "1100/1100 [==============================] - 0s 113us/sample - loss: 0.1310 - acc: 0.9464 - val_loss: 0.0539 - val_acc: 0.9744\n",
      "Epoch 52/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1522 - acc: 0.9373 - val_loss: 0.0349 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1518 - acc: 0.9345 - val_loss: 0.1178 - val_acc: 0.9744\n",
      "Epoch 54/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1439 - acc: 0.9427 - val_loss: 0.2289 - val_acc: 0.9359\n",
      "Epoch 55/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1732 - acc: 0.9382 - val_loss: 0.1676 - val_acc: 0.9487\n",
      "Epoch 56/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1763 - acc: 0.9373 - val_loss: 0.0575 - val_acc: 0.9615\n",
      "Epoch 57/100\n",
      "1100/1100 [==============================] - 0s 111us/sample - loss: 0.1400 - acc: 0.9382 - val_loss: 0.1831 - val_acc: 0.9744\n",
      "Epoch 58/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1432 - acc: 0.9427 - val_loss: 0.1105 - val_acc: 0.9744\n",
      "Epoch 59/100\n",
      "1100/1100 [==============================] - 0s 102us/sample - loss: 0.1500 - acc: 0.9427 - val_loss: 0.0556 - val_acc: 0.9744\n",
      "Epoch 60/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1399 - acc: 0.9355 - val_loss: 0.1591 - val_acc: 0.9615\n",
      "Epoch 61/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.1324 - acc: 0.9491 - val_loss: 0.0621 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1568 - acc: 0.9436 - val_loss: 0.2376 - val_acc: 0.9359\n",
      "Epoch 63/100\n",
      "1100/1100 [==============================] - 0s 113us/sample - loss: 0.1378 - acc: 0.9409 - val_loss: 0.2421 - val_acc: 0.9487\n",
      "Epoch 64/100\n",
      "1100/1100 [==============================] - 0s 100us/sample - loss: 0.1312 - acc: 0.9436 - val_loss: 0.2770 - val_acc: 0.9487\n",
      "Epoch 65/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1472 - acc: 0.9373 - val_loss: 0.2847 - val_acc: 0.9615\n",
      "Epoch 66/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1506 - acc: 0.9400 - val_loss: 0.1728 - val_acc: 0.9744\n",
      "Epoch 67/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1156 - acc: 0.9464 - val_loss: 0.0166 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1470 - acc: 0.9455 - val_loss: 0.0630 - val_acc: 0.9744\n",
      "Epoch 69/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1300 - acc: 0.9482 - val_loss: 0.2966 - val_acc: 0.8974\n",
      "Epoch 70/100\n",
      "1100/1100 [==============================] - 0s 100us/sample - loss: 0.1215 - acc: 0.9527 - val_loss: 0.1012 - val_acc: 0.9744\n",
      "Epoch 71/100\n",
      "1100/1100 [==============================] - 0s 116us/sample - loss: 0.1485 - acc: 0.9400 - val_loss: 0.1984 - val_acc: 0.9744\n",
      "Epoch 72/100\n",
      "1100/1100 [==============================] - 0s 102us/sample - loss: 0.1325 - acc: 0.9509 - val_loss: 0.1385 - val_acc: 0.9615\n",
      "Epoch 73/100\n",
      "1100/1100 [==============================] - 0s 112us/sample - loss: 0.1224 - acc: 0.9518 - val_loss: 0.3145 - val_acc: 0.9359\n",
      "Epoch 74/100\n",
      "1100/1100 [==============================] - 0s 99us/sample - loss: 0.1399 - acc: 0.9455 - val_loss: 0.3638 - val_acc: 0.9487\n",
      "Epoch 75/100\n",
      "1100/1100 [==============================] - 0s 100us/sample - loss: 0.1239 - acc: 0.9464 - val_loss: 0.0971 - val_acc: 0.9615\n",
      "Epoch 76/100\n",
      "1100/1100 [==============================] - 0s 100us/sample - loss: 0.1038 - acc: 0.9555 - val_loss: 0.4313 - val_acc: 0.9359\n",
      "Epoch 77/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1305 - acc: 0.9545 - val_loss: 0.0829 - val_acc: 0.9744\n",
      "Epoch 78/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1255 - acc: 0.9600 - val_loss: 0.0502 - val_acc: 0.9744\n",
      "Epoch 79/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1091 - acc: 0.9509 - val_loss: 0.2317 - val_acc: 0.9615\n",
      "Epoch 80/100\n",
      "1100/1100 [==============================] - 0s 110us/sample - loss: 0.1556 - acc: 0.9491 - val_loss: 0.0988 - val_acc: 0.9744\n",
      "Epoch 81/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1080 - acc: 0.9555 - val_loss: 0.0950 - val_acc: 0.9615\n",
      "Epoch 82/100\n",
      "1100/1100 [==============================] - 0s 109us/sample - loss: 0.1110 - acc: 0.9555 - val_loss: 0.1595 - val_acc: 0.9231\n",
      "Epoch 83/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1343 - acc: 0.9464 - val_loss: 0.0536 - val_acc: 0.9744\n",
      "Epoch 84/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1389 - acc: 0.9536 - val_loss: 0.1059 - val_acc: 0.9487\n",
      "Epoch 85/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1367 - acc: 0.9482 - val_loss: 0.0534 - val_acc: 0.9872\n",
      "Epoch 86/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.0957 - acc: 0.9609 - val_loss: 0.0843 - val_acc: 0.9615\n",
      "Epoch 87/100\n",
      "1100/1100 [==============================] - 0s 102us/sample - loss: 0.1505 - acc: 0.9500 - val_loss: 0.0376 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1057 - acc: 0.9582 - val_loss: 0.2493 - val_acc: 0.9615\n",
      "Epoch 89/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1439 - acc: 0.9582 - val_loss: 0.1404 - val_acc: 0.9615\n",
      "Epoch 90/100\n",
      "1100/1100 [==============================] - 0s 104us/sample - loss: 0.1198 - acc: 0.9573 - val_loss: 0.0664 - val_acc: 0.9744\n",
      "Epoch 91/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1224 - acc: 0.9545 - val_loss: 0.0645 - val_acc: 0.9744\n",
      "Epoch 92/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1286 - acc: 0.9555 - val_loss: 0.1594 - val_acc: 0.9744\n",
      "Epoch 93/100\n",
      "1100/1100 [==============================] - 0s 102us/sample - loss: 0.1322 - acc: 0.9536 - val_loss: 0.0907 - val_acc: 0.9744\n",
      "Epoch 94/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.0952 - acc: 0.9682 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "1100/1100 [==============================] - 0s 106us/sample - loss: 0.1310 - acc: 0.9545 - val_loss: 0.1408 - val_acc: 0.9487\n",
      "Epoch 96/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.0941 - acc: 0.9627 - val_loss: 0.1914 - val_acc: 0.9487\n",
      "Epoch 97/100\n",
      "1100/1100 [==============================] - 0s 108us/sample - loss: 0.1125 - acc: 0.9591 - val_loss: 0.1248 - val_acc: 0.9744\n",
      "Epoch 98/100\n",
      "1100/1100 [==============================] - 0s 103us/sample - loss: 0.1041 - acc: 0.9618 - val_loss: 0.3508 - val_acc: 0.9487\n",
      "Epoch 99/100\n",
      "1100/1100 [==============================] - 0s 105us/sample - loss: 0.1078 - acc: 0.9609 - val_loss: 0.2847 - val_acc: 0.9487\n",
      "Epoch 100/100\n",
      "1100/1100 [==============================] - 0s 107us/sample - loss: 0.1073 - acc: 0.9655 - val_loss: 0.3360 - val_acc: 0.9359\n",
      "\n",
      "Paciente:  10\n",
      "Train on 1097 samples, validate on 81 samples\n",
      "Epoch 1/100\n",
      "1097/1097 [==============================] - 1s 581us/sample - loss: 4.7096 - acc: 0.6882 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 2/100\n",
      "1097/1097 [==============================] - 0s 108us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 3/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 4/100\n",
      "1097/1097 [==============================] - 0s 123us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 5/100\n",
      "1097/1097 [==============================] - 0s 123us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 6/100\n",
      "1097/1097 [==============================] - 0s 107us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 7/100\n",
      "1097/1097 [==============================] - 0s 117us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 8/100\n",
      "1097/1097 [==============================] - 0s 109us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 9/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 10/100\n",
      "1097/1097 [==============================] - 0s 114us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 11/100\n",
      "1097/1097 [==============================] - 0s 122us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 12/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 13/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 14/100\n",
      "1097/1097 [==============================] - 0s 138us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 15/100\n",
      "1097/1097 [==============================] - 0s 136us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 16/100\n",
      "1097/1097 [==============================] - 0s 122us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 17/100\n",
      "1097/1097 [==============================] - 0s 130us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 18/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 19/100\n",
      "1097/1097 [==============================] - 0s 172us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 20/100\n",
      "1097/1097 [==============================] - 0s 183us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 21/100\n",
      "1097/1097 [==============================] - 0s 180us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 22/100\n",
      "1097/1097 [==============================] - 0s 171us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 23/100\n",
      "1097/1097 [==============================] - 0s 146us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 24/100\n",
      "1097/1097 [==============================] - 0s 136us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 25/100\n",
      "1097/1097 [==============================] - 0s 139us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 26/100\n",
      "1097/1097 [==============================] - 0s 139us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 27/100\n",
      "1097/1097 [==============================] - 0s 139us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 28/100\n",
      "1097/1097 [==============================] - 0s 148us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 29/100\n",
      "1097/1097 [==============================] - 0s 139us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 30/100\n",
      "1097/1097 [==============================] - 0s 147us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 31/100\n",
      "1097/1097 [==============================] - 0s 189us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 32/100\n",
      "1097/1097 [==============================] - 0s 181us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 33/100\n",
      "1097/1097 [==============================] - 0s 167us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 34/100\n",
      "1097/1097 [==============================] - 0s 183us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 35/100\n",
      "1097/1097 [==============================] - 0s 171us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 36/100\n",
      "1097/1097 [==============================] - 0s 172us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 37/100\n",
      "1097/1097 [==============================] - 0s 171us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 38/100\n",
      "1097/1097 [==============================] - 0s 158us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 39/100\n",
      "1097/1097 [==============================] - 0s 159us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 40/100\n",
      "1097/1097 [==============================] - 0s 139us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 41/100\n",
      "1097/1097 [==============================] - 0s 130us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 42/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 43/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 44/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 45/100\n",
      "1097/1097 [==============================] - 0s 133us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 46/100\n",
      "1097/1097 [==============================] - 0s 136us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 47/100\n",
      "1097/1097 [==============================] - 0s 138us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 48/100\n",
      "1097/1097 [==============================] - 0s 135us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 49/100\n",
      "1097/1097 [==============================] - 0s 128us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 50/100\n",
      "1097/1097 [==============================] - 0s 128us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 51/100\n",
      "1097/1097 [==============================] - 0s 118us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 52/100\n",
      "1097/1097 [==============================] - 0s 107us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097/1097 [==============================] - 0s 119us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 54/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 55/100\n",
      "1097/1097 [==============================] - 0s 159us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 56/100\n",
      "1097/1097 [==============================] - 0s 174us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 57/100\n",
      "1097/1097 [==============================] - 0s 214us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 58/100\n",
      "1097/1097 [==============================] - 0s 193us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 59/100\n",
      "1097/1097 [==============================] - 0s 170us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 60/100\n",
      "1097/1097 [==============================] - 0s 203us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 61/100\n",
      "1097/1097 [==============================] - 0s 149us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 62/100\n",
      "1097/1097 [==============================] - 0s 131us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 63/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 64/100\n",
      "1097/1097 [==============================] - 0s 117us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 65/100\n",
      "1097/1097 [==============================] - 0s 118us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 66/100\n",
      "1097/1097 [==============================] - 0s 132us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 67/100\n",
      "1097/1097 [==============================] - 0s 187us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 68/100\n",
      "1097/1097 [==============================] - 0s 137us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 69/100\n",
      "1097/1097 [==============================] - 0s 175us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 70/100\n",
      "1097/1097 [==============================] - 0s 157us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 71/100\n",
      "1097/1097 [==============================] - 0s 134us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 72/100\n",
      "1097/1097 [==============================] - 0s 126us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 73/100\n",
      "1097/1097 [==============================] - 0s 114us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 74/100\n",
      "1097/1097 [==============================] - 0s 115us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 75/100\n",
      "1097/1097 [==============================] - 0s 114us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 76/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 77/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 78/100\n",
      "1097/1097 [==============================] - 0s 108us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 79/100\n",
      "1097/1097 [==============================] - 0s 117us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 80/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 81/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 82/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 83/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 84/100\n",
      "1097/1097 [==============================] - 0s 180us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 85/100\n",
      "1097/1097 [==============================] - 0s 163us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 86/100\n",
      "1097/1097 [==============================] - 0s 186us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 87/100\n",
      "1097/1097 [==============================] - 0s 133us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 88/100\n",
      "1097/1097 [==============================] - 0s 184us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 89/100\n",
      "1097/1097 [==============================] - 0s 172us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 90/100\n",
      "1097/1097 [==============================] - 0s 131us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 91/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 92/100\n",
      "1097/1097 [==============================] - 0s 196us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 93/100\n",
      "1097/1097 [==============================] - 0s 153us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 94/100\n",
      "1097/1097 [==============================] - 0s 185us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 95/100\n",
      "1097/1097 [==============================] - 0s 183us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 96/100\n",
      "1097/1097 [==============================] - 0s 158us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 97/100\n",
      "1097/1097 [==============================] - 0s 168us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 98/100\n",
      "1097/1097 [==============================] - 0s 178us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 99/100\n",
      "1097/1097 [==============================] - 0s 195us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "Epoch 100/100\n",
      "1097/1097 [==============================] - 0s 145us/sample - loss: 4.6123 - acc: 0.7001 - val_loss: 4.9365 - val_acc: 0.6790\n",
      "\n",
      "Paciente:  11\n",
      "Train on 1099 samples, validate on 79 samples\n",
      "Epoch 1/100\n",
      "1099/1099 [==============================] - 1s 917us/sample - loss: 4.8701 - acc: 0.6715 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 2/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 3/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 4/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 5/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 6/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 7/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 8/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 9/100\n",
      "1099/1099 [==============================] - 0s 127us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 10/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 11/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 12/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 13/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 14/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 15/100\n",
      "1099/1099 [==============================] - 0s 126us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 16/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 17/100\n",
      "1099/1099 [==============================] - 0s 128us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 18/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 19/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 20/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 21/100\n",
      "1099/1099 [==============================] - 0s 128us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 22/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 23/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 24/100\n",
      "1099/1099 [==============================] - 0s 127us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 25/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 26/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 27/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 28/100\n",
      "1099/1099 [==============================] - 0s 131us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 29/100\n",
      "1099/1099 [==============================] - 0s 129us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 30/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 31/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 32/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 33/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 34/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 35/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 36/100\n",
      "1099/1099 [==============================] - 0s 112us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 37/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 38/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 39/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 40/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 41/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 42/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 43/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 44/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 45/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 46/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 47/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 48/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 49/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 50/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 51/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 52/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 53/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 54/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 55/100\n",
      "1099/1099 [==============================] - 0s 110us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 56/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 57/100\n",
      "1099/1099 [==============================] - 0s 111us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 58/100\n",
      "1099/1099 [==============================] - 0s 131us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 59/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 60/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 61/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 62/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 63/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 64/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 65/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 66/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 67/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 68/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099/1099 [==============================] - 0s 114us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 70/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 71/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 72/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 73/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 74/100\n",
      "1099/1099 [==============================] - 0s 134us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 75/100\n",
      "1099/1099 [==============================] - 0s 142us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 76/100\n",
      "1099/1099 [==============================] - 0s 129us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 77/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 78/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 79/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 80/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 81/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 82/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 83/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 84/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 85/100\n",
      "1099/1099 [==============================] - 0s 107us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 86/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 87/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 88/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 89/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 90/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 91/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 92/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 93/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 94/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 95/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 96/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 97/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 98/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 99/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "Epoch 100/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 4.6319 - acc: 0.6988 - val_loss: 4.6721 - val_acc: 0.6962\n",
      "\n",
      "Paciente:  13\n",
      "Train on 1099 samples, validate on 79 samples\n",
      "Epoch 1/100\n",
      "1099/1099 [==============================] - 1s 699us/sample - loss: 3.3404 - acc: 0.6433 - val_loss: 0.6659 - val_acc: 0.6709\n",
      "Epoch 2/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.8663 - acc: 0.6606 - val_loss: 1.0951 - val_acc: 0.6962\n",
      "Epoch 3/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.6920 - acc: 0.6952 - val_loss: 0.8146 - val_acc: 0.4430\n",
      "Epoch 4/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.6015 - acc: 0.7088 - val_loss: 0.5239 - val_acc: 0.7722\n",
      "Epoch 5/100\n",
      "1099/1099 [==============================] - 0s 126us/sample - loss: 0.5380 - acc: 0.7452 - val_loss: 0.9371 - val_acc: 0.4051\n",
      "Epoch 6/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.4876 - acc: 0.7816 - val_loss: 1.0025 - val_acc: 0.5823\n",
      "Epoch 7/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.4679 - acc: 0.8262 - val_loss: 0.9038 - val_acc: 0.5443\n",
      "Epoch 8/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.4333 - acc: 0.8235 - val_loss: 0.3619 - val_acc: 0.8861\n",
      "Epoch 9/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.4152 - acc: 0.8107 - val_loss: 0.3296 - val_acc: 0.8101\n",
      "Epoch 10/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.3841 - acc: 0.8571 - val_loss: 0.4941 - val_acc: 0.7975\n",
      "Epoch 11/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.3227 - acc: 0.8717 - val_loss: 0.3036 - val_acc: 0.8608\n",
      "Epoch 12/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.3503 - acc: 0.8699 - val_loss: 0.2203 - val_acc: 0.9494\n",
      "Epoch 13/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.3068 - acc: 0.8781 - val_loss: 0.9301 - val_acc: 0.7722\n",
      "Epoch 14/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.3176 - acc: 0.8781 - val_loss: 0.1369 - val_acc: 0.9747\n",
      "Epoch 15/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.2709 - acc: 0.8890 - val_loss: 0.4923 - val_acc: 0.7215\n",
      "Epoch 16/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.2929 - acc: 0.8863 - val_loss: 0.2969 - val_acc: 0.8481\n",
      "Epoch 17/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.2600 - acc: 0.8926 - val_loss: 0.4351 - val_acc: 0.7848\n",
      "Epoch 18/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.2595 - acc: 0.8899 - val_loss: 0.1756 - val_acc: 0.9241\n",
      "Epoch 19/100\n",
      "1099/1099 [==============================] - 0s 107us/sample - loss: 0.2499 - acc: 0.9008 - val_loss: 0.7909 - val_acc: 0.6456\n",
      "Epoch 20/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.2428 - acc: 0.9026 - val_loss: 0.2798 - val_acc: 0.8861\n",
      "Epoch 21/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.2426 - acc: 0.9008 - val_loss: 0.1400 - val_acc: 0.9494\n",
      "Epoch 22/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.2296 - acc: 0.9045 - val_loss: 0.8866 - val_acc: 0.6709\n",
      "Epoch 23/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.2213 - acc: 0.9163 - val_loss: 0.0869 - val_acc: 0.9747\n",
      "Epoch 24/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.2212 - acc: 0.9090 - val_loss: 0.2903 - val_acc: 0.8101\n",
      "Epoch 25/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.2218 - acc: 0.9045 - val_loss: 0.2582 - val_acc: 0.8608\n",
      "Epoch 26/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2220 - acc: 0.9181 - val_loss: 0.3066 - val_acc: 0.8734\n",
      "Epoch 27/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.2463 - acc: 0.9054 - val_loss: 1.4897 - val_acc: 0.5316\n",
      "Epoch 28/100\n",
      "1099/1099 [==============================] - 0s 127us/sample - loss: 0.1976 - acc: 0.9208 - val_loss: 0.8419 - val_acc: 0.6835\n",
      "Epoch 29/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.2258 - acc: 0.9108 - val_loss: 0.6699 - val_acc: 0.6962\n",
      "Epoch 30/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.2145 - acc: 0.9154 - val_loss: 0.0727 - val_acc: 0.9873\n",
      "Epoch 31/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.2073 - acc: 0.9181 - val_loss: 0.3993 - val_acc: 0.7468\n",
      "Epoch 32/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.2082 - acc: 0.9163 - val_loss: 0.0928 - val_acc: 0.9747\n",
      "Epoch 33/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.2070 - acc: 0.9117 - val_loss: 0.2304 - val_acc: 0.8608\n",
      "Epoch 34/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.2120 - acc: 0.9190 - val_loss: 0.1552 - val_acc: 0.9620\n",
      "Epoch 35/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1862 - acc: 0.9281 - val_loss: 0.2605 - val_acc: 0.8734\n",
      "Epoch 36/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.2053 - acc: 0.9172 - val_loss: 0.0672 - val_acc: 0.9873\n",
      "Epoch 37/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.2243 - acc: 0.9290 - val_loss: 0.1924 - val_acc: 0.9241\n",
      "Epoch 38/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1760 - acc: 0.9254 - val_loss: 0.1226 - val_acc: 0.9620\n",
      "Epoch 39/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1822 - acc: 0.9336 - val_loss: 0.1352 - val_acc: 0.9241\n",
      "Epoch 40/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1833 - acc: 0.9327 - val_loss: 1.3179 - val_acc: 0.6076\n",
      "Epoch 41/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1701 - acc: 0.9308 - val_loss: 0.0940 - val_acc: 0.9620\n",
      "Epoch 42/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1814 - acc: 0.9299 - val_loss: 0.0676 - val_acc: 0.9747\n",
      "Epoch 43/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1902 - acc: 0.9190 - val_loss: 0.2375 - val_acc: 0.8734\n",
      "Epoch 44/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1792 - acc: 0.9281 - val_loss: 0.0921 - val_acc: 0.9620\n",
      "Epoch 45/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1856 - acc: 0.9263 - val_loss: 0.2859 - val_acc: 0.8734\n",
      "Epoch 46/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1783 - acc: 0.9217 - val_loss: 0.1515 - val_acc: 0.9241\n",
      "Epoch 47/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.2185 - acc: 0.9281 - val_loss: 0.0868 - val_acc: 0.9620\n",
      "Epoch 48/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1456 - acc: 0.9327 - val_loss: 0.0554 - val_acc: 0.9873\n",
      "Epoch 49/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1952 - acc: 0.9281 - val_loss: 0.1187 - val_acc: 0.9620\n",
      "Epoch 50/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.1687 - acc: 0.9372 - val_loss: 0.3408 - val_acc: 0.8101\n",
      "Epoch 51/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1671 - acc: 0.9336 - val_loss: 0.1280 - val_acc: 0.9494\n",
      "Epoch 52/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1616 - acc: 0.9390 - val_loss: 0.3369 - val_acc: 0.8354\n",
      "Epoch 53/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1657 - acc: 0.9318 - val_loss: 0.0946 - val_acc: 0.9747\n",
      "Epoch 54/100\n",
      "1099/1099 [==============================] - 0s 132us/sample - loss: 0.1723 - acc: 0.9318 - val_loss: 0.0541 - val_acc: 0.9873\n",
      "Epoch 55/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1748 - acc: 0.9354 - val_loss: 0.1600 - val_acc: 0.9367\n",
      "Epoch 56/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1576 - acc: 0.9409 - val_loss: 0.2375 - val_acc: 0.9114\n",
      "Epoch 57/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1660 - acc: 0.9409 - val_loss: 0.0973 - val_acc: 0.9620\n",
      "Epoch 58/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1622 - acc: 0.9354 - val_loss: 0.2906 - val_acc: 0.8608\n",
      "Epoch 59/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.1672 - acc: 0.9272 - val_loss: 0.2055 - val_acc: 0.8987\n",
      "Epoch 60/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.1412 - acc: 0.9509 - val_loss: 0.1781 - val_acc: 0.8861\n",
      "Epoch 61/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1434 - acc: 0.9409 - val_loss: 0.0885 - val_acc: 0.9747\n",
      "Epoch 62/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1586 - acc: 0.9399 - val_loss: 0.0742 - val_acc: 0.9747\n",
      "Epoch 63/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1682 - acc: 0.9281 - val_loss: 0.3548 - val_acc: 0.8354\n",
      "Epoch 64/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1563 - acc: 0.9427 - val_loss: 0.2868 - val_acc: 0.8861\n",
      "Epoch 65/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.1321 - acc: 0.9427 - val_loss: 0.4968 - val_acc: 0.8228\n",
      "Epoch 66/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1620 - acc: 0.9308 - val_loss: 0.1156 - val_acc: 0.9367\n",
      "Epoch 67/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1734 - acc: 0.9363 - val_loss: 0.1552 - val_acc: 0.9494\n",
      "Epoch 68/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1217 - acc: 0.9445 - val_loss: 0.0565 - val_acc: 0.9747\n",
      "Epoch 69/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.1448 - acc: 0.9436 - val_loss: 0.6793 - val_acc: 0.7848\n",
      "Epoch 70/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1516 - acc: 0.9390 - val_loss: 0.4446 - val_acc: 0.8734\n",
      "Epoch 71/100\n",
      "1099/1099 [==============================] - 0s 131us/sample - loss: 0.1458 - acc: 0.9472 - val_loss: 0.1746 - val_acc: 0.9241\n",
      "Epoch 72/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1534 - acc: 0.9372 - val_loss: 0.2097 - val_acc: 0.8987\n",
      "Epoch 73/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1316 - acc: 0.9481 - val_loss: 0.2645 - val_acc: 0.8861\n",
      "Epoch 74/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1300 - acc: 0.9472 - val_loss: 0.1483 - val_acc: 0.9494\n",
      "Epoch 75/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1384 - acc: 0.9454 - val_loss: 0.4068 - val_acc: 0.8608\n",
      "Epoch 76/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1447 - acc: 0.9427 - val_loss: 0.0442 - val_acc: 0.9873\n",
      "Epoch 77/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1607 - acc: 0.9409 - val_loss: 0.4478 - val_acc: 0.8228\n",
      "Epoch 78/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1397 - acc: 0.9390 - val_loss: 0.2592 - val_acc: 0.8987\n",
      "Epoch 79/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1610 - acc: 0.9436 - val_loss: 0.1421 - val_acc: 0.9620\n",
      "Epoch 80/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1289 - acc: 0.9463 - val_loss: 0.0502 - val_acc: 0.9873\n",
      "Epoch 81/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1889 - acc: 0.9345 - val_loss: 0.1114 - val_acc: 0.9620\n",
      "Epoch 82/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1231 - acc: 0.9490 - val_loss: 0.0800 - val_acc: 0.9620\n",
      "Epoch 83/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1143 - acc: 0.9554 - val_loss: 0.1067 - val_acc: 0.9367\n",
      "Epoch 84/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1208 - acc: 0.9500 - val_loss: 1.7024 - val_acc: 0.7089\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1792 - acc: 0.9345 - val_loss: 0.0880 - val_acc: 0.9494\n",
      "Epoch 86/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1327 - acc: 0.9536 - val_loss: 0.1096 - val_acc: 0.9494\n",
      "Epoch 87/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1322 - acc: 0.9509 - val_loss: 0.1061 - val_acc: 0.9494\n",
      "Epoch 88/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1213 - acc: 0.9509 - val_loss: 0.4523 - val_acc: 0.8228\n",
      "Epoch 89/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1232 - acc: 0.9536 - val_loss: 0.0331 - val_acc: 0.9873\n",
      "Epoch 90/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1293 - acc: 0.9472 - val_loss: 0.3526 - val_acc: 0.8481\n",
      "Epoch 91/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1400 - acc: 0.9481 - val_loss: 0.1589 - val_acc: 0.9241\n",
      "Epoch 92/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1074 - acc: 0.9572 - val_loss: 0.2679 - val_acc: 0.8861\n",
      "Epoch 93/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1261 - acc: 0.9554 - val_loss: 0.4162 - val_acc: 0.8481\n",
      "Epoch 94/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1080 - acc: 0.9481 - val_loss: 0.4763 - val_acc: 0.8354\n",
      "Epoch 95/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.1415 - acc: 0.9500 - val_loss: 0.1113 - val_acc: 0.9747\n",
      "Epoch 96/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1212 - acc: 0.9500 - val_loss: 0.1541 - val_acc: 0.9367\n",
      "Epoch 97/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1094 - acc: 0.9527 - val_loss: 0.1333 - val_acc: 0.9241\n",
      "Epoch 98/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1293 - acc: 0.9454 - val_loss: 0.0968 - val_acc: 0.9620\n",
      "Epoch 99/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1362 - acc: 0.9554 - val_loss: 1.1275 - val_acc: 0.8354\n",
      "Epoch 100/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1492 - acc: 0.9509 - val_loss: 0.3095 - val_acc: 0.8861\n",
      "\n",
      "Paciente:  14\n",
      "Train on 1099 samples, validate on 79 samples\n",
      "Epoch 1/100\n",
      "1099/1099 [==============================] - 1s 590us/sample - loss: 4.5637 - acc: 0.6542 - val_loss: 4.5769 - val_acc: 0.6962\n",
      "Epoch 2/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 4.0568 - acc: 0.6642 - val_loss: 2.8089 - val_acc: 0.6962\n",
      "Epoch 3/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 2.4874 - acc: 0.6988 - val_loss: 0.8663 - val_acc: 0.7089\n",
      "Epoch 4/100\n",
      "1099/1099 [==============================] - 0s 130us/sample - loss: 1.5189 - acc: 0.7225 - val_loss: 0.7255 - val_acc: 0.7089\n",
      "Epoch 5/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.8859 - acc: 0.7671 - val_loss: 0.9427 - val_acc: 0.6076\n",
      "Epoch 6/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.7262 - acc: 0.7743 - val_loss: 0.5501 - val_acc: 0.7975\n",
      "Epoch 7/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.5685 - acc: 0.8171 - val_loss: 0.4967 - val_acc: 0.8354\n",
      "Epoch 8/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.4748 - acc: 0.8535 - val_loss: 0.6724 - val_acc: 0.7722\n",
      "Epoch 9/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.4596 - acc: 0.8435 - val_loss: 0.4888 - val_acc: 0.7848\n",
      "Epoch 10/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.3471 - acc: 0.8617 - val_loss: 0.5960 - val_acc: 0.7468\n",
      "Epoch 11/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.3321 - acc: 0.8699 - val_loss: 1.4223 - val_acc: 0.6329\n",
      "Epoch 12/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.3543 - acc: 0.8763 - val_loss: 0.5965 - val_acc: 0.7975\n",
      "Epoch 13/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.2992 - acc: 0.8808 - val_loss: 1.0773 - val_acc: 0.6962\n",
      "Epoch 14/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.2836 - acc: 0.8944 - val_loss: 1.3220 - val_acc: 0.6709\n",
      "Epoch 15/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.2573 - acc: 0.8999 - val_loss: 0.9336 - val_acc: 0.7595\n",
      "Epoch 16/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.2638 - acc: 0.8990 - val_loss: 0.9178 - val_acc: 0.7342\n",
      "Epoch 17/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.2542 - acc: 0.9063 - val_loss: 0.7061 - val_acc: 0.7848\n",
      "Epoch 18/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2144 - acc: 0.9217 - val_loss: 0.8219 - val_acc: 0.7595\n",
      "Epoch 19/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.2460 - acc: 0.9017 - val_loss: 0.8620 - val_acc: 0.7595\n",
      "Epoch 20/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.2258 - acc: 0.9136 - val_loss: 0.9731 - val_acc: 0.8101\n",
      "Epoch 21/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.2191 - acc: 0.9327 - val_loss: 1.0923 - val_acc: 0.7722\n",
      "Epoch 22/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1989 - acc: 0.9172 - val_loss: 1.2992 - val_acc: 0.7468\n",
      "Epoch 23/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.2305 - acc: 0.9236 - val_loss: 0.9566 - val_acc: 0.7975\n",
      "Epoch 24/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.2214 - acc: 0.9154 - val_loss: 1.1181 - val_acc: 0.7468\n",
      "Epoch 25/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1933 - acc: 0.9208 - val_loss: 1.8419 - val_acc: 0.7342\n",
      "Epoch 26/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.2069 - acc: 0.9308 - val_loss: 1.1556 - val_acc: 0.7468\n",
      "Epoch 27/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1842 - acc: 0.9299 - val_loss: 1.3932 - val_acc: 0.7722\n",
      "Epoch 28/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1913 - acc: 0.9372 - val_loss: 2.1449 - val_acc: 0.7215\n",
      "Epoch 29/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.1841 - acc: 0.9327 - val_loss: 1.4381 - val_acc: 0.7468\n",
      "Epoch 30/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1842 - acc: 0.9381 - val_loss: 1.5028 - val_acc: 0.7722\n",
      "Epoch 31/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1623 - acc: 0.9318 - val_loss: 1.7233 - val_acc: 0.7595\n",
      "Epoch 32/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.2177 - acc: 0.9245 - val_loss: 1.4699 - val_acc: 0.7722\n",
      "Epoch 33/100\n",
      "1099/1099 [==============================] - 0s 111us/sample - loss: 0.1658 - acc: 0.9381 - val_loss: 1.9561 - val_acc: 0.7468\n",
      "Epoch 34/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1916 - acc: 0.9272 - val_loss: 1.3943 - val_acc: 0.7722\n",
      "Epoch 35/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1604 - acc: 0.9409 - val_loss: 1.4258 - val_acc: 0.7722\n",
      "Epoch 36/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1788 - acc: 0.9409 - val_loss: 1.2834 - val_acc: 0.7722\n",
      "Epoch 37/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1778 - acc: 0.9427 - val_loss: 1.3652 - val_acc: 0.7215\n",
      "Epoch 38/100\n",
      "1099/1099 [==============================] - 0s 126us/sample - loss: 0.1817 - acc: 0.9327 - val_loss: 1.5700 - val_acc: 0.7595\n",
      "Epoch 39/100\n",
      "1099/1099 [==============================] - 0s 127us/sample - loss: 0.1654 - acc: 0.9336 - val_loss: 1.4932 - val_acc: 0.7342\n",
      "Epoch 40/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1818 - acc: 0.9409 - val_loss: 1.4131 - val_acc: 0.7595\n",
      "Epoch 41/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1517 - acc: 0.9463 - val_loss: 1.7965 - val_acc: 0.7468\n",
      "Epoch 42/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1627 - acc: 0.9336 - val_loss: 1.6890 - val_acc: 0.7342\n",
      "Epoch 43/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1637 - acc: 0.9427 - val_loss: 1.3685 - val_acc: 0.7722\n",
      "Epoch 44/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1390 - acc: 0.9327 - val_loss: 2.1301 - val_acc: 0.7215\n",
      "Epoch 45/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1811 - acc: 0.9418 - val_loss: 1.8425 - val_acc: 0.7342\n",
      "Epoch 46/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1463 - acc: 0.9454 - val_loss: 1.3882 - val_acc: 0.7722\n",
      "Epoch 47/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1546 - acc: 0.9409 - val_loss: 1.6567 - val_acc: 0.7722\n",
      "Epoch 48/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1335 - acc: 0.9518 - val_loss: 1.7801 - val_acc: 0.7215\n",
      "Epoch 49/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1535 - acc: 0.9427 - val_loss: 1.8958 - val_acc: 0.7595\n",
      "Epoch 50/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1400 - acc: 0.9372 - val_loss: 2.2480 - val_acc: 0.7215\n",
      "Epoch 51/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1470 - acc: 0.9445 - val_loss: 1.7102 - val_acc: 0.7848\n",
      "Epoch 52/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1354 - acc: 0.9463 - val_loss: 1.8240 - val_acc: 0.7595\n",
      "Epoch 53/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1417 - acc: 0.9500 - val_loss: 2.1136 - val_acc: 0.7722\n",
      "Epoch 54/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1366 - acc: 0.9463 - val_loss: 1.9380 - val_acc: 0.7342\n",
      "Epoch 55/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1630 - acc: 0.9390 - val_loss: 2.1392 - val_acc: 0.7595\n",
      "Epoch 56/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1507 - acc: 0.9454 - val_loss: 2.0078 - val_acc: 0.7468\n",
      "Epoch 57/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1432 - acc: 0.9445 - val_loss: 2.2770 - val_acc: 0.7468\n",
      "Epoch 58/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1363 - acc: 0.9463 - val_loss: 2.6916 - val_acc: 0.7342\n",
      "Epoch 59/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1354 - acc: 0.9445 - val_loss: 1.9267 - val_acc: 0.7215\n",
      "Epoch 60/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1135 - acc: 0.9563 - val_loss: 1.8765 - val_acc: 0.7722\n",
      "Epoch 61/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1334 - acc: 0.9554 - val_loss: 2.0736 - val_acc: 0.7215\n",
      "Epoch 62/100\n",
      "1099/1099 [==============================] - 0s 110us/sample - loss: 0.1439 - acc: 0.9472 - val_loss: 1.8043 - val_acc: 0.7722\n",
      "Epoch 63/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1495 - acc: 0.9500 - val_loss: 2.2261 - val_acc: 0.7468\n",
      "Epoch 64/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1288 - acc: 0.9563 - val_loss: 1.7495 - val_acc: 0.7722\n",
      "Epoch 65/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1278 - acc: 0.9554 - val_loss: 2.3268 - val_acc: 0.7342\n",
      "Epoch 66/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.1435 - acc: 0.9536 - val_loss: 2.5059 - val_acc: 0.7342\n",
      "Epoch 67/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1474 - acc: 0.9536 - val_loss: 2.3602 - val_acc: 0.7215\n",
      "Epoch 68/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1409 - acc: 0.9445 - val_loss: 2.2294 - val_acc: 0.7468\n",
      "Epoch 69/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1324 - acc: 0.9527 - val_loss: 2.2592 - val_acc: 0.7468\n",
      "Epoch 70/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1348 - acc: 0.9509 - val_loss: 2.0711 - val_acc: 0.7848\n",
      "Epoch 71/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1406 - acc: 0.9581 - val_loss: 2.7758 - val_acc: 0.7468\n",
      "Epoch 72/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1152 - acc: 0.9554 - val_loss: 2.2906 - val_acc: 0.7342\n",
      "Epoch 73/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1215 - acc: 0.9581 - val_loss: 1.9550 - val_acc: 0.7468\n",
      "Epoch 74/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.1154 - acc: 0.9536 - val_loss: 2.1743 - val_acc: 0.7468\n",
      "Epoch 75/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1062 - acc: 0.9572 - val_loss: 2.2281 - val_acc: 0.7595\n",
      "Epoch 76/100\n",
      "1099/1099 [==============================] - 0s 129us/sample - loss: 0.1072 - acc: 0.9627 - val_loss: 2.2188 - val_acc: 0.7468\n",
      "Epoch 77/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1127 - acc: 0.9527 - val_loss: 2.1651 - val_acc: 0.7468\n",
      "Epoch 78/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1173 - acc: 0.9518 - val_loss: 2.4247 - val_acc: 0.7468\n",
      "Epoch 79/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1107 - acc: 0.9618 - val_loss: 2.1864 - val_acc: 0.7468\n",
      "Epoch 80/100\n",
      "1099/1099 [==============================] - 0s 107us/sample - loss: 0.1110 - acc: 0.9563 - val_loss: 2.0985 - val_acc: 0.7722\n",
      "Epoch 81/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1270 - acc: 0.9554 - val_loss: 1.9620 - val_acc: 0.7468\n",
      "Epoch 82/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1206 - acc: 0.9527 - val_loss: 2.2952 - val_acc: 0.7468\n",
      "Epoch 83/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1014 - acc: 0.9563 - val_loss: 2.0337 - val_acc: 0.8101\n",
      "Epoch 84/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.1074 - acc: 0.9518 - val_loss: 2.2536 - val_acc: 0.7595\n",
      "Epoch 85/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1188 - acc: 0.9591 - val_loss: 2.4163 - val_acc: 0.7595\n",
      "Epoch 86/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1089 - acc: 0.9618 - val_loss: 2.3156 - val_acc: 0.7468\n",
      "Epoch 87/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.1013 - acc: 0.9563 - val_loss: 2.1565 - val_acc: 0.7595\n",
      "Epoch 88/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1503 - acc: 0.9463 - val_loss: 2.5916 - val_acc: 0.7342\n",
      "Epoch 89/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.0996 - acc: 0.9609 - val_loss: 2.1675 - val_acc: 0.7722\n",
      "Epoch 90/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1369 - acc: 0.9536 - val_loss: 2.1205 - val_acc: 0.7848\n",
      "Epoch 91/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.0923 - acc: 0.9627 - val_loss: 2.8381 - val_acc: 0.6962\n",
      "Epoch 92/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1247 - acc: 0.9545 - val_loss: 2.2203 - val_acc: 0.7595\n",
      "Epoch 93/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1012 - acc: 0.9600 - val_loss: 2.3805 - val_acc: 0.7595\n",
      "Epoch 94/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1123 - acc: 0.9527 - val_loss: 2.5393 - val_acc: 0.7468\n",
      "Epoch 95/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1163 - acc: 0.9636 - val_loss: 2.4229 - val_acc: 0.7215\n",
      "Epoch 96/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1118 - acc: 0.9600 - val_loss: 2.3736 - val_acc: 0.7595\n",
      "Epoch 97/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.0984 - acc: 0.9718 - val_loss: 1.9831 - val_acc: 0.7595\n",
      "Epoch 98/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1045 - acc: 0.9609 - val_loss: 2.1684 - val_acc: 0.7595\n",
      "Epoch 99/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1272 - acc: 0.9572 - val_loss: 2.3236 - val_acc: 0.7595\n",
      "Epoch 100/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1128 - acc: 0.9572 - val_loss: 2.6533 - val_acc: 0.7468\n",
      "\n",
      "Paciente:  16\n",
      "Train on 1099 samples, validate on 79 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099/1099 [==============================] - 1s 717us/sample - loss: 4.4430 - acc: 0.6433 - val_loss: 0.8695 - val_acc: 0.3924\n",
      "Epoch 2/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 1.9275 - acc: 0.6579 - val_loss: 0.4217 - val_acc: 0.7595\n",
      "Epoch 3/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 1.0157 - acc: 0.6988 - val_loss: 0.4093 - val_acc: 0.9367\n",
      "Epoch 4/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.8152 - acc: 0.7106 - val_loss: 0.2479 - val_acc: 0.8734\n",
      "Epoch 5/100\n",
      "1099/1099 [==============================] - 0s 128us/sample - loss: 0.6965 - acc: 0.7616 - val_loss: 0.2703 - val_acc: 0.9114\n",
      "Epoch 6/100\n",
      "1099/1099 [==============================] - 0s 130us/sample - loss: 0.5999 - acc: 0.7516 - val_loss: 0.2537 - val_acc: 0.9494\n",
      "Epoch 7/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.4733 - acc: 0.8080 - val_loss: 0.1275 - val_acc: 0.9873\n",
      "Epoch 8/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.4504 - acc: 0.8308 - val_loss: 0.1320 - val_acc: 0.9873\n",
      "Epoch 9/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.4195 - acc: 0.8226 - val_loss: 0.0917 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.3940 - acc: 0.8435 - val_loss: 0.0786 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.3467 - acc: 0.8544 - val_loss: 0.0555 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.3304 - acc: 0.8653 - val_loss: 0.0491 - val_acc: 0.9873\n",
      "Epoch 13/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.3302 - acc: 0.8672 - val_loss: 0.0459 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "1099/1099 [==============================] - 0s 128us/sample - loss: 0.3346 - acc: 0.8635 - val_loss: 0.1189 - val_acc: 0.9747\n",
      "Epoch 15/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.3043 - acc: 0.8826 - val_loss: 0.0914 - val_acc: 0.9620\n",
      "Epoch 16/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.2996 - acc: 0.8872 - val_loss: 0.0276 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.2735 - acc: 0.8935 - val_loss: 0.0885 - val_acc: 0.9620\n",
      "Epoch 18/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.2816 - acc: 0.8981 - val_loss: 0.0567 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.2922 - acc: 0.8990 - val_loss: 0.0516 - val_acc: 0.9873\n",
      "Epoch 20/100\n",
      "1099/1099 [==============================] - 0s 134us/sample - loss: 0.2297 - acc: 0.9054 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2410 - acc: 0.8917 - val_loss: 0.1282 - val_acc: 0.9494\n",
      "Epoch 22/100\n",
      "1099/1099 [==============================] - 0s 128us/sample - loss: 0.2481 - acc: 0.9008 - val_loss: 0.0646 - val_acc: 0.9747\n",
      "Epoch 23/100\n",
      "1099/1099 [==============================] - 0s 127us/sample - loss: 0.2303 - acc: 0.9172 - val_loss: 0.0615 - val_acc: 0.9873\n",
      "Epoch 24/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2316 - acc: 0.8999 - val_loss: 0.0226 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.2383 - acc: 0.9017 - val_loss: 0.0845 - val_acc: 0.9873\n",
      "Epoch 26/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.2068 - acc: 0.9108 - val_loss: 0.0288 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2434 - acc: 0.8999 - val_loss: 0.0454 - val_acc: 0.9873\n",
      "Epoch 28/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.2192 - acc: 0.9117 - val_loss: 0.0420 - val_acc: 0.9873\n",
      "Epoch 29/100\n",
      "1099/1099 [==============================] - 0s 110us/sample - loss: 0.2185 - acc: 0.9163 - val_loss: 0.0627 - val_acc: 0.9873\n",
      "Epoch 30/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1893 - acc: 0.9208 - val_loss: 0.1670 - val_acc: 0.9620\n",
      "Epoch 31/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.2025 - acc: 0.9208 - val_loss: 0.0215 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "1099/1099 [==============================] - 0s 114us/sample - loss: 0.2133 - acc: 0.9154 - val_loss: 0.0127 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1898 - acc: 0.9263 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.2453 - acc: 0.9145 - val_loss: 0.0360 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "1099/1099 [==============================] - 0s 128us/sample - loss: 0.1929 - acc: 0.9227 - val_loss: 0.0309 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.2127 - acc: 0.9281 - val_loss: 0.0478 - val_acc: 0.9873\n",
      "Epoch 37/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1849 - acc: 0.9254 - val_loss: 0.0362 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.2129 - acc: 0.9181 - val_loss: 0.0957 - val_acc: 0.9620\n",
      "Epoch 39/100\n",
      "1099/1099 [==============================] - 0s 137us/sample - loss: 0.2104 - acc: 0.9227 - val_loss: 0.1032 - val_acc: 0.9873\n",
      "Epoch 40/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1947 - acc: 0.9236 - val_loss: 0.0356 - val_acc: 0.9873\n",
      "Epoch 41/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1762 - acc: 0.9354 - val_loss: 0.0259 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1894 - acc: 0.9227 - val_loss: 0.0249 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1743 - acc: 0.9290 - val_loss: 0.0232 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1955 - acc: 0.9281 - val_loss: 0.0270 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.2059 - acc: 0.9263 - val_loss: 0.0760 - val_acc: 0.9873\n",
      "Epoch 46/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1648 - acc: 0.9409 - val_loss: 0.0521 - val_acc: 0.9873\n",
      "Epoch 47/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.2253 - acc: 0.9236 - val_loss: 0.0248 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1606 - acc: 0.9345 - val_loss: 0.0274 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1536 - acc: 0.9354 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1456 - acc: 0.9318 - val_loss: 0.0382 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1757 - acc: 0.9354 - val_loss: 0.3206 - val_acc: 0.9494\n",
      "Epoch 52/100\n",
      "1099/1099 [==============================] - 0s 130us/sample - loss: 0.1459 - acc: 0.9354 - val_loss: 0.0854 - val_acc: 0.9873\n",
      "Epoch 53/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1822 - acc: 0.9272 - val_loss: 0.0390 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "1099/1099 [==============================] - 0s 110us/sample - loss: 0.1562 - acc: 0.9409 - val_loss: 0.0829 - val_acc: 0.9494\n",
      "Epoch 55/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1646 - acc: 0.9390 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "1099/1099 [==============================] - 0s 113us/sample - loss: 0.1861 - acc: 0.9336 - val_loss: 0.0520 - val_acc: 0.9873\n",
      "Epoch 57/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1399 - acc: 0.9363 - val_loss: 0.0175 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1545 - acc: 0.9354 - val_loss: 0.0761 - val_acc: 0.9873\n",
      "Epoch 59/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.1486 - acc: 0.9336 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1530 - acc: 0.9327 - val_loss: 0.0733 - val_acc: 0.9747\n",
      "Epoch 61/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.1544 - acc: 0.9354 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1487 - acc: 0.9409 - val_loss: 0.1617 - val_acc: 0.9873\n",
      "Epoch 63/100\n",
      "1099/1099 [==============================] - 0s 120us/sample - loss: 0.1550 - acc: 0.9363 - val_loss: 0.0441 - val_acc: 0.9873\n",
      "Epoch 64/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1534 - acc: 0.9399 - val_loss: 0.0784 - val_acc: 0.9873\n",
      "Epoch 65/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1503 - acc: 0.9399 - val_loss: 0.0753 - val_acc: 0.9873\n",
      "Epoch 66/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1264 - acc: 0.9436 - val_loss: 0.0141 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1353 - acc: 0.9327 - val_loss: 0.0760 - val_acc: 0.9620\n",
      "Epoch 68/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1496 - acc: 0.9418 - val_loss: 0.0974 - val_acc: 0.9620\n",
      "Epoch 69/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1344 - acc: 0.9445 - val_loss: 0.0112 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1345 - acc: 0.9472 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1687 - acc: 0.9299 - val_loss: 0.0315 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1154 - acc: 0.9518 - val_loss: 0.0243 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "1099/1099 [==============================] - 0s 119us/sample - loss: 0.1223 - acc: 0.9481 - val_loss: 0.0595 - val_acc: 0.9747\n",
      "Epoch 74/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1149 - acc: 0.9509 - val_loss: 0.0698 - val_acc: 0.9620\n",
      "Epoch 75/100\n",
      "1099/1099 [==============================] - 0s 115us/sample - loss: 0.1204 - acc: 0.9518 - val_loss: 0.0869 - val_acc: 0.9494\n",
      "Epoch 76/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1545 - acc: 0.9372 - val_loss: 0.0193 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1227 - acc: 0.9481 - val_loss: 0.1034 - val_acc: 0.9873\n",
      "Epoch 78/100\n",
      "1099/1099 [==============================] - 0s 127us/sample - loss: 0.1082 - acc: 0.9527 - val_loss: 0.1781 - val_acc: 0.9747\n",
      "Epoch 79/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1314 - acc: 0.9454 - val_loss: 0.0793 - val_acc: 0.9873\n",
      "Epoch 80/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1134 - acc: 0.9500 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1770 - acc: 0.9427 - val_loss: 0.0163 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1311 - acc: 0.9536 - val_loss: 0.1005 - val_acc: 0.9873\n",
      "Epoch 83/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1106 - acc: 0.9554 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1093 - acc: 0.9563 - val_loss: 0.0117 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "1099/1099 [==============================] - 0s 125us/sample - loss: 0.1167 - acc: 0.9490 - val_loss: 0.0546 - val_acc: 0.9620\n",
      "Epoch 86/100\n",
      "1099/1099 [==============================] - 0s 108us/sample - loss: 0.1479 - acc: 0.9509 - val_loss: 0.2073 - val_acc: 0.9367\n",
      "Epoch 87/100\n",
      "1099/1099 [==============================] - 0s 126us/sample - loss: 0.1645 - acc: 0.9563 - val_loss: 0.0634 - val_acc: 0.9873\n",
      "Epoch 88/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1016 - acc: 0.9609 - val_loss: 0.0414 - val_acc: 0.9873\n",
      "Epoch 89/100\n",
      "1099/1099 [==============================] - 0s 117us/sample - loss: 0.1313 - acc: 0.9545 - val_loss: 0.1092 - val_acc: 0.9494\n",
      "Epoch 90/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1282 - acc: 0.9472 - val_loss: 0.0260 - val_acc: 0.9873\n",
      "Epoch 91/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.1006 - acc: 0.9618 - val_loss: 0.0657 - val_acc: 0.9873\n",
      "Epoch 92/100\n",
      "1099/1099 [==============================] - 0s 124us/sample - loss: 0.1368 - acc: 0.9409 - val_loss: 0.0658 - val_acc: 0.9747\n",
      "Epoch 93/100\n",
      "1099/1099 [==============================] - 0s 109us/sample - loss: 0.1107 - acc: 0.9518 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1160 - acc: 0.9490 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "1099/1099 [==============================] - 0s 121us/sample - loss: 0.1383 - acc: 0.9527 - val_loss: 0.1472 - val_acc: 0.9620\n",
      "Epoch 96/100\n",
      "1099/1099 [==============================] - 0s 116us/sample - loss: 0.1027 - acc: 0.9600 - val_loss: 0.0148 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1429 - acc: 0.9518 - val_loss: 0.0174 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "1099/1099 [==============================] - 0s 118us/sample - loss: 0.1647 - acc: 0.9536 - val_loss: 0.0271 - val_acc: 0.9873\n",
      "Epoch 99/100\n",
      "1099/1099 [==============================] - 0s 123us/sample - loss: 0.1603 - acc: 0.9518 - val_loss: 0.1269 - val_acc: 0.9873\n",
      "Epoch 100/100\n",
      "1099/1099 [==============================] - 0s 122us/sample - loss: 0.0952 - acc: 0.9609 - val_loss: 0.0537 - val_acc: 0.9873\n",
      "\n",
      "Paciente:  17\n",
      "Train on 1097 samples, validate on 81 samples\n",
      "Epoch 1/100\n",
      "1097/1097 [==============================] - 1s 704us/sample - loss: 3.1980 - acc: 0.5998 - val_loss: 0.9110 - val_acc: 0.6914\n",
      "Epoch 2/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 1.5000 - acc: 0.6809 - val_loss: 3.1568 - val_acc: 0.3580\n",
      "Epoch 3/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.9474 - acc: 0.7192 - val_loss: 1.1462 - val_acc: 0.6543\n",
      "Epoch 4/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 0.7087 - acc: 0.7685 - val_loss: 0.7015 - val_acc: 0.6543\n",
      "Epoch 5/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.5202 - acc: 0.7995 - val_loss: 0.9919 - val_acc: 0.5185\n",
      "Epoch 6/100\n",
      "1097/1097 [==============================] - 0s 125us/sample - loss: 0.4809 - acc: 0.8040 - val_loss: 0.8245 - val_acc: 0.5432\n",
      "Epoch 7/100\n",
      "1097/1097 [==============================] - 0s 122us/sample - loss: 0.3320 - acc: 0.8760 - val_loss: 0.9891 - val_acc: 0.4691\n",
      "Epoch 8/100\n",
      "1097/1097 [==============================] - 0s 118us/sample - loss: 0.3600 - acc: 0.8797 - val_loss: 1.2642 - val_acc: 0.5309\n",
      "Epoch 9/100\n",
      "1097/1097 [==============================] - 0s 114us/sample - loss: 0.3065 - acc: 0.8788 - val_loss: 1.3781 - val_acc: 0.4815\n",
      "Epoch 10/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.2652 - acc: 0.9043 - val_loss: 1.0488 - val_acc: 0.4691\n",
      "Epoch 11/100\n",
      "1097/1097 [==============================] - 0s 108us/sample - loss: 0.2986 - acc: 0.8806 - val_loss: 1.2484 - val_acc: 0.5556\n",
      "Epoch 12/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.2705 - acc: 0.9015 - val_loss: 1.6599 - val_acc: 0.5432\n",
      "Epoch 13/100\n",
      "1097/1097 [==============================] - 0s 112us/sample - loss: 0.2734 - acc: 0.9006 - val_loss: 1.1090 - val_acc: 0.5309\n",
      "Epoch 14/100\n",
      "1097/1097 [==============================] - 0s 126us/sample - loss: 0.2057 - acc: 0.9234 - val_loss: 1.8631 - val_acc: 0.4938\n",
      "Epoch 15/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 0.2340 - acc: 0.9189 - val_loss: 1.3351 - val_acc: 0.5062\n",
      "Epoch 16/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.2136 - acc: 0.9234 - val_loss: 2.5060 - val_acc: 0.5432\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097/1097 [==============================] - 0s 116us/sample - loss: 0.1884 - acc: 0.9243 - val_loss: 1.1447 - val_acc: 0.5185\n",
      "Epoch 18/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.1985 - acc: 0.9253 - val_loss: 1.2728 - val_acc: 0.5062\n",
      "Epoch 19/100\n",
      "1097/1097 [==============================] - 0s 109us/sample - loss: 0.1865 - acc: 0.9262 - val_loss: 1.3771 - val_acc: 0.4444\n",
      "Epoch 20/100\n",
      "1097/1097 [==============================] - 0s 118us/sample - loss: 0.1706 - acc: 0.9371 - val_loss: 1.1206 - val_acc: 0.5185\n",
      "Epoch 21/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.1949 - acc: 0.9362 - val_loss: 1.3344 - val_acc: 0.4815\n",
      "Epoch 22/100\n",
      "1097/1097 [==============================] - 0s 126us/sample - loss: 0.1865 - acc: 0.9417 - val_loss: 1.3465 - val_acc: 0.5432\n",
      "Epoch 23/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.1873 - acc: 0.9307 - val_loss: 1.0991 - val_acc: 0.5556\n",
      "Epoch 24/100\n",
      "1097/1097 [==============================] - 0s 125us/sample - loss: 0.1693 - acc: 0.9380 - val_loss: 1.4640 - val_acc: 0.4568\n",
      "Epoch 25/100\n",
      "1097/1097 [==============================] - 0s 123us/sample - loss: 0.1574 - acc: 0.9435 - val_loss: 1.5960 - val_acc: 0.4568\n",
      "Epoch 26/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.1760 - acc: 0.9335 - val_loss: 1.5498 - val_acc: 0.5679\n",
      "Epoch 27/100\n",
      "1097/1097 [==============================] - 0s 107us/sample - loss: 0.1528 - acc: 0.9508 - val_loss: 1.7641 - val_acc: 0.5679\n",
      "Epoch 28/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.1550 - acc: 0.9480 - val_loss: 1.5842 - val_acc: 0.5926\n",
      "Epoch 29/100\n",
      "1097/1097 [==============================] - 0s 125us/sample - loss: 0.1607 - acc: 0.9453 - val_loss: 1.5953 - val_acc: 0.5679\n",
      "Epoch 30/100\n",
      "1097/1097 [==============================] - 0s 112us/sample - loss: 0.1518 - acc: 0.9389 - val_loss: 1.5364 - val_acc: 0.5432\n",
      "Epoch 31/100\n",
      "1097/1097 [==============================] - 0s 113us/sample - loss: 0.1463 - acc: 0.9453 - val_loss: 2.1239 - val_acc: 0.5679\n",
      "Epoch 32/100\n",
      "1097/1097 [==============================] - 0s 125us/sample - loss: 0.1835 - acc: 0.9490 - val_loss: 1.6558 - val_acc: 0.5926\n",
      "Epoch 33/100\n",
      "1097/1097 [==============================] - 0s 109us/sample - loss: 0.1348 - acc: 0.9480 - val_loss: 1.7401 - val_acc: 0.4938\n",
      "Epoch 34/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 0.1502 - acc: 0.9462 - val_loss: 0.9974 - val_acc: 0.5679\n",
      "Epoch 35/100\n",
      "1097/1097 [==============================] - 0s 111us/sample - loss: 0.1321 - acc: 0.9499 - val_loss: 1.4682 - val_acc: 0.5185\n",
      "Epoch 36/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.1218 - acc: 0.9535 - val_loss: 1.8150 - val_acc: 0.5556\n",
      "Epoch 37/100\n",
      "1097/1097 [==============================] - 0s 123us/sample - loss: 0.1541 - acc: 0.9544 - val_loss: 1.4728 - val_acc: 0.5309\n",
      "Epoch 38/100\n",
      "1097/1097 [==============================] - 0s 114us/sample - loss: 0.1358 - acc: 0.9544 - val_loss: 1.1958 - val_acc: 0.5556\n",
      "Epoch 39/100\n",
      "1097/1097 [==============================] - 0s 110us/sample - loss: 0.1432 - acc: 0.9572 - val_loss: 1.2997 - val_acc: 0.5556\n",
      "Epoch 40/100\n",
      "1097/1097 [==============================] - 0s 123us/sample - loss: 0.1522 - acc: 0.9544 - val_loss: 1.2029 - val_acc: 0.5556\n",
      "Epoch 41/100\n",
      "1097/1097 [==============================] - 0s 122us/sample - loss: 0.1181 - acc: 0.9562 - val_loss: 1.1510 - val_acc: 0.5432\n",
      "Epoch 42/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 0.1425 - acc: 0.9517 - val_loss: 2.0885 - val_acc: 0.5062\n",
      "Epoch 43/100\n",
      "1097/1097 [==============================] - 0s 115us/sample - loss: 0.1718 - acc: 0.9553 - val_loss: 1.3932 - val_acc: 0.5679\n",
      "Epoch 44/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.1167 - acc: 0.9517 - val_loss: 1.6747 - val_acc: 0.5926\n",
      "Epoch 45/100\n",
      "1097/1097 [==============================] - 0s 128us/sample - loss: 0.1275 - acc: 0.9590 - val_loss: 1.2238 - val_acc: 0.5062\n",
      "Epoch 46/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.1093 - acc: 0.9617 - val_loss: 1.9687 - val_acc: 0.5926\n",
      "Epoch 47/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.1678 - acc: 0.9562 - val_loss: 2.6565 - val_acc: 0.5679\n",
      "Epoch 48/100\n",
      "1097/1097 [==============================] - 0s 126us/sample - loss: 0.1429 - acc: 0.9553 - val_loss: 1.2616 - val_acc: 0.5432\n",
      "Epoch 49/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.1133 - acc: 0.9581 - val_loss: 1.8022 - val_acc: 0.5309\n",
      "Epoch 50/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.1215 - acc: 0.9672 - val_loss: 1.2156 - val_acc: 0.5556\n",
      "Epoch 51/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 0.1302 - acc: 0.9617 - val_loss: 2.4842 - val_acc: 0.5432\n",
      "Epoch 52/100\n",
      "1097/1097 [==============================] - 0s 126us/sample - loss: 0.0804 - acc: 0.9699 - val_loss: 2.5715 - val_acc: 0.5185\n",
      "Epoch 53/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 0.1626 - acc: 0.9553 - val_loss: 1.6128 - val_acc: 0.5309\n",
      "Epoch 54/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.0909 - acc: 0.9663 - val_loss: 2.7338 - val_acc: 0.5556\n",
      "Epoch 55/100\n",
      "1097/1097 [==============================] - 0s 122us/sample - loss: 0.0916 - acc: 0.9699 - val_loss: 1.9672 - val_acc: 0.5185\n",
      "Epoch 56/100\n",
      "1097/1097 [==============================] - 0s 123us/sample - loss: 0.0983 - acc: 0.9654 - val_loss: 2.1408 - val_acc: 0.5432\n",
      "Epoch 57/100\n",
      "1097/1097 [==============================] - 0s 126us/sample - loss: 0.0978 - acc: 0.9617 - val_loss: 2.6793 - val_acc: 0.4691\n",
      "Epoch 58/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.1026 - acc: 0.9681 - val_loss: 1.4679 - val_acc: 0.5556\n",
      "Epoch 59/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.0973 - acc: 0.9635 - val_loss: 2.9457 - val_acc: 0.5185\n",
      "Epoch 60/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 0.0758 - acc: 0.9727 - val_loss: 2.5355 - val_acc: 0.6049\n",
      "Epoch 61/100\n",
      "1097/1097 [==============================] - 0s 117us/sample - loss: 0.1226 - acc: 0.9617 - val_loss: 1.8209 - val_acc: 0.5309\n",
      "Epoch 62/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.0881 - acc: 0.9736 - val_loss: 1.6384 - val_acc: 0.4938\n",
      "Epoch 63/100\n",
      "1097/1097 [==============================] - 0s 126us/sample - loss: 0.0806 - acc: 0.9727 - val_loss: 2.1861 - val_acc: 0.4815\n",
      "Epoch 64/100\n",
      "1097/1097 [==============================] - 0s 122us/sample - loss: 0.1200 - acc: 0.9699 - val_loss: 1.6607 - val_acc: 0.5432\n",
      "Epoch 65/100\n",
      "1097/1097 [==============================] - 0s 114us/sample - loss: 0.0959 - acc: 0.9681 - val_loss: 2.1615 - val_acc: 0.5556\n",
      "Epoch 66/100\n",
      "1097/1097 [==============================] - 0s 125us/sample - loss: 0.0918 - acc: 0.9644 - val_loss: 3.1107 - val_acc: 0.5679\n",
      "Epoch 67/100\n",
      "1097/1097 [==============================] - 0s 122us/sample - loss: 0.0840 - acc: 0.9690 - val_loss: 2.3032 - val_acc: 0.5432\n",
      "Epoch 68/100\n",
      "1097/1097 [==============================] - 0s 118us/sample - loss: 0.1115 - acc: 0.9699 - val_loss: 1.6473 - val_acc: 0.5556\n",
      "Epoch 69/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 0.0979 - acc: 0.9717 - val_loss: 1.5831 - val_acc: 0.5309\n",
      "Epoch 70/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.0797 - acc: 0.9727 - val_loss: 2.1743 - val_acc: 0.5062\n",
      "Epoch 71/100\n",
      "1097/1097 [==============================] - 0s 122us/sample - loss: 0.0924 - acc: 0.9708 - val_loss: 2.2449 - val_acc: 0.5185\n",
      "Epoch 72/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.0873 - acc: 0.9736 - val_loss: 1.9053 - val_acc: 0.5309\n",
      "Epoch 73/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.0850 - acc: 0.9727 - val_loss: 2.3211 - val_acc: 0.5309\n",
      "Epoch 74/100\n",
      "1097/1097 [==============================] - 0s 117us/sample - loss: 0.0741 - acc: 0.9717 - val_loss: 2.8197 - val_acc: 0.6173\n",
      "Epoch 75/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.0802 - acc: 0.9736 - val_loss: 2.0002 - val_acc: 0.5185\n",
      "Epoch 76/100\n",
      "1097/1097 [==============================] - 0s 123us/sample - loss: 0.0750 - acc: 0.9727 - val_loss: 2.1045 - val_acc: 0.5432\n",
      "Epoch 77/100\n",
      "1097/1097 [==============================] - 0s 125us/sample - loss: 0.0713 - acc: 0.9727 - val_loss: 1.6944 - val_acc: 0.5926\n",
      "Epoch 78/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.1028 - acc: 0.9681 - val_loss: 1.6272 - val_acc: 0.5432\n",
      "Epoch 79/100\n",
      "1097/1097 [==============================] - 0s 125us/sample - loss: 0.0769 - acc: 0.9727 - val_loss: 2.4518 - val_acc: 0.5309\n",
      "Epoch 80/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.1044 - acc: 0.9690 - val_loss: 2.8358 - val_acc: 0.5062\n",
      "Epoch 81/100\n",
      "1097/1097 [==============================] - 0s 123us/sample - loss: 0.0866 - acc: 0.9717 - val_loss: 2.5149 - val_acc: 0.5679\n",
      "Epoch 82/100\n",
      "1097/1097 [==============================] - 0s 122us/sample - loss: 0.0629 - acc: 0.9763 - val_loss: 3.4939 - val_acc: 0.5556\n",
      "Epoch 83/100\n",
      "1097/1097 [==============================] - 0s 120us/sample - loss: 0.1269 - acc: 0.9708 - val_loss: 2.7117 - val_acc: 0.5062\n",
      "Epoch 84/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.0679 - acc: 0.9727 - val_loss: 3.1788 - val_acc: 0.5432\n",
      "Epoch 85/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.0816 - acc: 0.9754 - val_loss: 2.8056 - val_acc: 0.5802\n",
      "Epoch 86/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.1185 - acc: 0.9727 - val_loss: 2.5002 - val_acc: 0.4938\n",
      "Epoch 87/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.0836 - acc: 0.9772 - val_loss: 2.9613 - val_acc: 0.5432\n",
      "Epoch 88/100\n",
      "1097/1097 [==============================] - 0s 115us/sample - loss: 0.0950 - acc: 0.9727 - val_loss: 3.3978 - val_acc: 0.5309\n",
      "Epoch 89/100\n",
      "1097/1097 [==============================] - 0s 127us/sample - loss: 0.0848 - acc: 0.9736 - val_loss: 3.4966 - val_acc: 0.5802\n",
      "Epoch 90/100\n",
      "1097/1097 [==============================] - 0s 118us/sample - loss: 0.0781 - acc: 0.9727 - val_loss: 2.7252 - val_acc: 0.5185\n",
      "Epoch 91/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 0.0802 - acc: 0.9717 - val_loss: 3.0969 - val_acc: 0.5432\n",
      "Epoch 92/100\n",
      "1097/1097 [==============================] - 0s 126us/sample - loss: 0.0884 - acc: 0.9736 - val_loss: 2.1923 - val_acc: 0.5309\n",
      "Epoch 93/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 0.0633 - acc: 0.9754 - val_loss: 1.5810 - val_acc: 0.4938\n",
      "Epoch 94/100\n",
      "1097/1097 [==============================] - 0s 116us/sample - loss: 0.0606 - acc: 0.9763 - val_loss: 4.2635 - val_acc: 0.5185\n",
      "Epoch 95/100\n",
      "1097/1097 [==============================] - 0s 125us/sample - loss: 0.1060 - acc: 0.9717 - val_loss: 2.4192 - val_acc: 0.5309\n",
      "Epoch 96/100\n",
      "1097/1097 [==============================] - 0s 124us/sample - loss: 0.0634 - acc: 0.9790 - val_loss: 3.7996 - val_acc: 0.5556\n",
      "Epoch 97/100\n",
      "1097/1097 [==============================] - 0s 121us/sample - loss: 0.0820 - acc: 0.9708 - val_loss: 3.0326 - val_acc: 0.5185\n",
      "Epoch 98/100\n",
      "1097/1097 [==============================] - 0s 115us/sample - loss: 0.0883 - acc: 0.9772 - val_loss: 2.7763 - val_acc: 0.5309\n",
      "Epoch 99/100\n",
      "1097/1097 [==============================] - 0s 117us/sample - loss: 0.1050 - acc: 0.9745 - val_loss: 2.4941 - val_acc: 0.5432\n",
      "Epoch 100/100\n",
      "1097/1097 [==============================] - 0s 119us/sample - loss: 0.0587 - acc: 0.9772 - val_loss: 3.1164 - val_acc: 0.5802\n"
     ]
    }
   ],
   "source": [
    "for idx in pacientes:\n",
    "    #print(\"Paciente {0}.\".format(idx))\n",
    "    print('\\nPaciente: ', idx)\n",
    "    X_train, y_train, X_val, y_val = leave_one_out(df, idx)\n",
    "\n",
    "    model = modelo_nn(layer_1_dim)\n",
    "    hist = model.fit(x = X_train,\n",
    "                     y = y_train,\n",
    "                     epochs=100,\n",
    "                     batch_size = 25,\n",
    "                     validation_data = (X_val, y_val),\n",
    "                     verbose = 1\n",
    "                    )\n",
    "    lista_hist.append(hist.history)\n",
    "    \n",
    "    ## retorna a matriz de consusão e a acurária\n",
    "    matriz, acc = testar_modelo(model, df_teste, df_rotulos)\n",
    "    lista_matriz.append(matriz)\n",
    "    lista_acc.append(acc)\n",
    "    # lista_modelo.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### média de acertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.21852033317002"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lista_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('Teste de acurácias em Validação Leave One por pacientes que entraram apenas como dados de teste.')\n",
    "sns.barplot(x=pacientes, y=lista_acc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAIuCAYAAABJmPjHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeZyVdd34/9d72FxwQ5DVxDXXxAWVcsFdNMXuSrOy7u7Kyvx9876ru9XuFLUsxTI1Nc0lDbTMLDe8XTLxVhGXXMJcUUFQEFBEQZbP748ZEZWBYeZzzXWdc17Px4OHzBlm5s05vmbOeXOdc0VKCUmSJEmSpCprKnsASZIkSZKklXGBIUmSJEmSKs8FhiRJkiRJqjwXGJIkSZIkqfJcYEiSJEmSpMpzgSFJkiRJkirPBYYkSZIkSao8FxgNICJ6RMRFEfFcRMyNiAcjYkTZc0m1LiIuj4hpEfFaRDwREV8qeyapXkTE5hExPyIuL3sWqdZFxN9aenq95de/yp5JqgcR8amImBQR8yLi6YjYo+yZ6p0LjMbQFXgB2AtYBzgBuCoiBpc4k1QPfgIMTimtDRwGnBwRO5U8k1QvzgHuK3sIqY4cl1Lq2fLrg2UPI9W6iNgfOA34ArAWsCfwTKlDNQAXGA0gpTQvpfTjlNLklNKSlNJ1wLOAD7SkDkgpPZZSWvD2my2/Ni1xJKkuRMSngDnArWXPIklSK04ETkop3dPyGGtqSmlq2UPVOxcYDSgi+gJbAI+VPYtU6yLi3Ih4A3gcmAbcUPJIUk2LiLWBk4Bvlj2LVGd+EhEzI+KuiBhe9jBSLYuILsDOQJ+IeCoipkTE2RGxetmz1TsXGA0mIroBVwCXppQeL3seqdallI6l+bDBPYA/AQtW/BGSVmIUcFFK6YWyB5HqyHeATYCBwAXAXyPCIwal9usLdAM+QfN9wCHADsAPyxyqEbjAaCAR0QT8DngLOK7kcaS6kVJanFIaDwwCvlb2PFKtioghwH7AmWXPItWTlNK9KaW5KaUFKaVLgbuAg8ueS6phb7b891cppWkppZnAaOyqcF3LHkCdIyICuIjmbeHBKaWFJY8k1aOu+BoYUkcMBwYDzzf/2KIn0CUitk4p7VjiXFK9SUCUPYRUq1JKsyNiCs0tqRN5BEbj+DWwFXBoSunNlf1hSSsWERu0nDqrZ0R0iYgDgaOA28qeTaphF9C8BBzS8us84HrgwDKHkmpZRKwbEQdGxGoR0TUiPkPz2RLGlT2bVOMuBv6/lvuE6wHHA9eVPFPd8wiMBhARGwFfofm5+dNb/lUL4CsppStKG0yqbYnmp4ucR/My+Dng+JTStaVOJdWwlNIbwBtvvx0RrwPzU0ozyptKqnndgJOBLYHFNL/o9OEppX+VOpVU+0YBvYEngPnAVcAppU7UACIlj3qRJEmSJEnV5lNIJEmSJElS5bnAkCRJkiRJlecCQ5IkSZIkVZ4LDEmSJEmSVHmVPQvJwpnP+OqiBVhz4J5lj1CX3lowpSbOpW5XxVh9wB5lj1CXFr011a4a1FqDhpc9Qt2aP/95u2pQdlUcu2pcdlWc1rryCAxJkiRJklR5LjAkSZIkSVLlucCQJEmSJEmV5wJDkiRJkiRVngsMSZIkSZJUeS4wJEmSJElS5bnAkCRJkiRJlecCQ5IkSZIkVZ4LDEmSJEmSVHkuMCRJkiRJUuW5wJAkSZIkSZXnAkOSJEmSJFWeCwxJkiRJklR5LjAkSZIkSVLlucCQJEmSJEmV5wJDkiRJkiRVngsMSZIkSZJUeS4wJEmSJElS5bnAkCRJkiRJlecCQ5IkSZIkVZ4LDEmSJEmSVHkuMCRJkiRJUuW5wJAkSZIkSZXnAkOSJEmSJFWeCwxJkiRJklR5LjAkSZIkSVLlucCQJEmSJEmV5wJDkiRJkiRVngsMSZIkSZJUeV2L+sQRsSUwEhgIJOBF4C8ppUlFfU2p3tmVlJ9dSXnZlJSfXUnNCjkCIyK+A4wFApgA3Nfy+zER8d0ivqZU7+xKys+upLxsSsrPrqR3REop/yeNeALYJqW08D2XdwceSylt3srHHQMcA3DuGSfv9KXPHZV9tka35sA9yx6hLr21YEoU/TXsqrpWH7BH2SPUpUVvTbWrBrXWoOFlj1C35s9/vtCu2ttUy5+xqwLZVXHsqnHZVXFa66qop5AsAQYAz73n8v4t71uulNIFwAUAC2c+k3+zUqAfnjqav981gV7rrcufLz8PgFdfm8s3T/gJL05/iQH9+nLGqO+xztpr8eprcznhJ2fywtRp9OjenVHf/08232RwuX+Birrg/NM5+OD9mDFjJjvsuF/Z45TNroBxt93JuRddzjPPvcCY3/yCbbfaYumf/81lV/Kn68bRpamJ7/3n1/jIrjuVNXrlHXjAcEaPPokuTU389uIx/Ozn55Q9UlnsitZ/Xv32ij9y/c23A7B48WKeee4F7rx+LOusvVaZf4VK6tGjB7fc8gd69OhO165dueaaGxg1anTZY5WhXU1B43Q19/V5fPeknzHtpRksXrSYf//0x/nYIQeU/DeopkGD+nPRRWfSt28flixJXHTR7znnnN+WPVYZ7MqusqiHpop6Ec/jgVsj4saIuKDl103ArcA3CvqapTr84P05b/TJ77rswt9dxW47D+GGKy9it52HcNHlVwHND7K23HxTrrns15x6wrf46S/OK2PkmnDZ7/7ARw/9bNljVIVdAZttshG/OPUEdhqy7bsuf/rZ57jx1ju49vLzOG/0yYw6/WwWL17cmePWjKamJs765Sl89NDPst32e3PkkYez1Vat/uNNvbMrWv959R+f+QRXX3oOV196Dsd/9d/Zech2Li9asWDBAg466FPssstB7LLLQey//17ssssOZY9VhoZrClatqzFX/5VNB3+AP116LheffRo//9VvWLhw4fI+bcNbtGgx3/nOyQwZsi977jmSr371c2y5ZUP+vLKrFnbVMfXQVCELjJTSTcAWwInAOOBm4MfAB1veV3eWd6fu9jvvZuSI5qMGRo7Yj9v+fjcAT09+nt122h6ATTbakKnTXmLmrNmdO3CNGD/+XmbPnlP2GJVgV802HfwBNt5o0Pv+7G133sOIffeie/fuDBrQjw8MGsAjk57orFFryi5Dd+Dppyfz7LPPs3DhQq666loOO/TAsscqhV01a+3n1bJuuOUODt5/r06ZsVbNm/cGAN26daVbt64U8TTdqmvEpmDVuooI5r3xJikl3nhzPuusvRZdunTp9JlrwfTpL/PQQ48C8Prr83j88acYOLBfyVN1Prt6h111TD00VdhZSFJKS4B7ivr8teCV2XPo07sXAH1692LWnFcB+OBmm3DLHf/HjttvyyP//BfTXnqZl16eSe9e65U5rmqAXbXu5Rmv8KFtt1z6dt8NevPyjJklTlRdAwb244UpLy59e8rUaewytCH/pRiwK2j959Xb3pw/n/H3TOQH/3VsGePVjKamJu6++3o23XQw5513Gffd91DZI5XCppq11tWnP34ox33nRPYe+RnmvfEmp5/0PZqaijooun5stNEghgzZhgkTHix7lFLYVTO7yqdWm/JWLcGXjv4kr819nY9//utc8ce/sOXmm7ohlDoo8f5/6QwKfw3ImhTx/uulEf+lWG33t/H3ssOHtvbpIyuxZMkSdt11BJtuuitDh27P1ltvsfIPUsO5a8L9bLn5Jtx+7RVcfck5nDr6XF6fN6/ssSptzTXXYMyY8/nWt05k7tzXyx5HFWRXq6aWm3KBUaD111uXGTNnATBj5ix6rbsOAD3XXJOTf/BfXH3pOfzkhG8xe86rDBrQt8xRpZrXt09vpr80Y+nbL708kz591i9xouqaOmUaGw4asPTtQQP7M23aSyVOpLK19vPqbTfeegcH7ze8hMlq06uvvsbf/34PBxwwvOxRVKLWurrm+v9lv70+QkTwgUEDGNi/H88+N6XMUSuta9eujB17PmPHXsO119btsyXURnbVcbXelAuMAg3ffTeuvfEWAK698Rb23mMYAK/NfX3pi8pc/deb2GnIdvRcc83S5pTqwd6778aNt97BW2+9xZQXp/P8lBfZbiv/9XN57pv4EJtttjGDB29It27dOOKIkfz1upvLHkslau3nFcDc1+cx8cFH3nWZ3q93716ss87aAKy2Wg/22Wd3/vWvp0ueSmVqrav+fftwz/3NTy+aOWs2k5+fwqABtfUc9M50/vk/5/HHn+Kssy4sexRVgF11XK03Fa0dNhwRO67oA1NKDxQyUYtaO83Pt//np9z34MPMmfMa6/dal2O/eDT77jmMb55wKtNemkH/vn0YffIPWGfttXjo0Ul8f9TpdGlqYpPBH+Ck7x3faYflrjlwz075Orn87rKz2XPPYfTu3YuXXprJSaPO4JJLxpY91vu8tWBKm56rYFerZnldrbN2T35y5q+ZNedV1urZky0334QLzjwFgPMvHcM1191M1y5d+M43vsIew4Z2ypyrD9ijU75OTiMO2oczzjiRLk1NXHLplfzkp2eVPdL7LHprql0VYFV+XgH8+fr/Zfy9Ezn9pO912oxrDRreaV8rl2233ZILLxxNly5daGpq4uqrr+PUU39Z9ljvM3/+83ZVgFXp6uUZr/CDU85g5iuzSSnxxaOP4NAD9yl8xlrs6sMfHsptt13NI49MYsmS5rOF/uhHP2PcuNtLnuzd7KoYdpVfrTQFrXe1ogXGiv4WKaVU6P8RtRZYrai1BUatWIUFhl3VoVpcYNSCVVhg2FWdqbU7hLVkFR5o2VWdsavi2FXjsqvitNZVq2chSSntXdw4UmOyKyk/u5LysyspP7uSOq5Np1GNiG2BrYHV3r4spXRZUUNJjcCupPzsSsrPrqT87Epqn5UuMCLif4DhNAd2AzACGA8YmNROdiXlZ1dSfnYl5WdXUvu15SwknwD2BaanlL4AbA/0KHQqqf7ZlZSfXUn52ZWUn11J7dSWBcabKaUlwKKIWBt4Gdik2LGkumdXUn52JeVnV1J+diW1U1teA2NiRKwL/Aa4H3gdmFDoVFL9syspP7uS8rMrKT+7ktqp1dOoLvcPRwwG1k4pPVzUQG/zND/F8DSqxWjraVSXx65qn6dRLUZbT6O6PHZV2zwtXXHaerrH5bGr2mZXxbGrxmVXxWmtq5U+hSSafTYifpRSmgzMiYhdcg8oNRK7kvKzKyk/u5Lysyup/dryGhjnAsOAo1rengucU9hEUmOwKyk/u5LysyspP7uS2qktr4Gxa0ppx4h4ECClNDsiuhc8l1Tv7ErKz66k/OxKys+upHZqyxEYCyOiC5AAIqIPsKTQqaT6Z1dSfnYl5WdXUn52JbVTWxYYZwHXABtExCnAeODUQqeS6p9dSfnZlZSfXUn52ZXUTit9CklK6YqIuB/YFwjg8JTSpMInk+qYXUn52ZWUn11J+dmV1H5teQ0MUkqPA48DRMS6EfGDlNIphU4m1Tm7kvKzKyk/u5LysyupfVp9CklEbBgRF0TEdRHxpYhYIyLOAJ4ANui8EaX6YVdSfnYl5WdXUn52JXXcio7AuAy4A7gaOAi4B3gM+FBKaXonzCbVI7uS8rMrKT+7kvKzK6mDVrTA6JVS+nHL78dFxEvA0JTSguLHkuqWXUn52ZWUn11J+dmV1EErfA2MiFiP5heWAZgOrBERawKklGYVPJtUl+xKys+upPzsSsrPrqSOWdECYx3gft4JDOCBlv8mYJOihpLqmF1J+dmVlJ9dSfnZldRBrS4wUkqDO3EOqSHYlZSfXUn52ZWUn11JHdfqWUgkSZIkSZKqwgWGJEmSJEmqvBW+iGeZVh+wR9kj1KU/9tqr7BFUonU/sE/ZI9Sl3/bZu+wRVCJ/XuU3dv3hZY+gktlVfld7H7Dh2VV+dtX5VngERkQ0RcSjnTWM1AjsSsrPrqT87ErKz66kjlnhAiOltAT4R0R8oJPmkeqeXUn52ZWUn11J+dmV1DFteQpJf+CxiJgAzHv7wpTSYYVNJdU/u5LysyspP7uS8rMrqZ3assA4sfAppMZjV1J+diXlZ1dSfnYltdNKFxgppTsioi8wtOWiCSmll4sdS6pvdiXlZ1dSfnYl5WdXUvut9DSqEXEEMAH4JHAEcG9EfKLowaR6ZldSfnYl5WdXUn52JbVfW55C8gNg6NtbwYjoA9wC/LHIwaQ6Z1dSfnYl5WdXUn52JbXTSo/AAJrec0jTK238OEmtsyspP7uS8rMrKT+7ktqpLUdg3BQR44AxLW8fCdxQ3EhSQ7ArKT+7kvKzKyk/u5LaqS0v4vntiPg48BEggAtSStcUPplUx+xKys+upPzsSsrPrqT2a8sRGKSUrgauLngWqaHYlZSfXUn52ZWUn11J7dPqAiMi5gJpee8CUkpp7cKmkuqUXUn52ZWUn11J+dmV1HGtLjBSSmt15iBSI7ArKT+7kvKzKyk/u5I6rk1PIQGIiA2A1d5+O6X0fCETSQ3ErqT87ErKz66k/OxKWnUrPV1PRBwWEU8CzwJ3AJOBGwueS6prdiXlZ1dSfnYl5WdXUvu15XzDo4DdgCdSShsD+wJ3FTqVVP/sSsrPrqT87ErKz66kdmrLAmNhSukVoCkimlJKtwNDCp5Lqnd2JeVnV1J+diXlZ1dSO7XlNTDmRERP4O/AFRHxMrCo2LGkumdXUn52JeVnV1J+diW1U1uOwBgJvAn8J3AT8DRwaJFDSQ3ArqT87ErKz66k/OxKaqdWj8CIiLOB36eU/m+Ziy8tfiSpftmVlJ9dSfnZlZSfXUkdt6IjMJ4EzoiIyRFxWkT4vCyp4+xKys+upPzsSsrPrqQOanWBkVL6ZUppGLAXMAu4OCImRcSPImKLTptQqiN2JeVnV1J+diXlZ1dSx630NTBSSs+llE5LKe0AfBr4GDCp8MmkOmZXUn52JeVnV1J+diW130oXGBHRLSIOjYgrgBuBJ4CPFz6ZVMfsSsrPrqT87ErKz66k9lvRi3juDxwFHAJMAMYCx6SU5nXSbFLdsSspP7uS8rMrKT+7kjqu1QUG8H3g98C3UkqzOmkeqd7ZlZSfXUn52ZWUn11JHdTqAiOltHdnDiI1AruS8rMrKT+7kvKzK6njVvoaGJIkSZIkSWVzgSFJkiRJkirPBYYkSZIkSaq8Tl9gRMQXOvtrSvXOrqT87ErKz66k/OxKjaSMIzBObO0dEXFMREyMiIlLlng2IWkVtKmrRYvmduZMUq3z55WUn11J+dmVGsaKTqPabhHxcGvvAvq29nEppQuACwC6dh+YChitUAceMJzRo0+iS1MTv714DD/7+Tllj1Q/moLh405h/vRZ3HP06ez+5x/RredqAHTvvQ5zHnyae78wuuQhi5WjqzXXGFxTXfXo0YOb//dKenTvQZeuXfjzn2/klJPPLHusuhFNwSE3juKN6bO57fNnLL18l1GfY9Mj92TMFl8qcbrO0Yg/r35zwRkccvB+vDxjJkN22LfscepLU7DvTSczf/ps7vrc6QBs891PMuiju5KWLOGZS2/lqYvGlTxk8RqxK+8DFqgp2KvlPuC9LfcBu7bcB+zRex1mP/g0E+r8PiDYlV1lVsNdFbLAoDmiA4HZ77k8gP8r6GuWqqmpibN+eQoHHXwUU6ZM4567b+Cv193MpElPlj1aXdj0yyOY++RUuq21OgDjDz9p6ft2ufB4po27v6zROlPDdbVgwQIOHvFp5s17g65du3LLrX/k5nF/4777Hix7tLqw5ZcO4tUnX1zaFcD6H9qYbuusUeJUna7hurrssqs499yLufjiX5Y9St3Z/MsHMXeZpjY6ck/WGLA+4/b4NqREj/XXLnnCTtNQXXkfsFibfnkErz85la7LuQ849MLjmd4Y9wHBruwqo1ruqqinkFwH9EwpPfeeX5OBvxX0NUu1y9AdePrpyTz77PMsXLiQq666lsMOPbDsserCav170W+/ITx3xe3ve1/XNVej9+7bMO3GiSVM1ukariuAefPeAKBbt65069aVRE3940FlrdG/F4P2HcKTY/629LJoCnY64SgeOHlseYN1vobr6s7x9zJr9pyyx6g7q/fvRf99h/Ds79/5WbXp5/fjn6OvgdT8fWvBK6+VNV5na6iuvA9YnNX696Kv9wHfZld2lUWtd1XIAiOl9MWU0vhW3vfpIr5m2QYM7McLU15c+vaUqdMYMKBfiRPVj+1GHc2jo8YsvQO4rP4HD2XG+EdZ9PqbJUzWuRqxK2jewN99zw1Mfu5+brt1PBPve6jskerC0BM/y/0njyEteaerD37hAF64+QHefLlxHtw2alfKb/uTjubhk8fAMk2tudEGbDhyN/a5aRS7X/Hf9Ny41aO860qjdeV9wOJsN+poHhs1htTKfcCZDXIfEOzKrvKp9a48jWomEfG+y5b3P4VWTd/9d2DBzNd49eFnl/v+QR8bxtRr6u6oOS1jyZIlDNvtYLbYfBg77bw9W2+9Rdkj1byB+w1h/szXmPXI5KWXrd53XQZ/dBce/+3N5Q0m1aj+++3AgpmvMufhye+6vEuPbiyev5DbDjqBZ664jZ3PPKacAVUo7wMWY2X3AQd+bBhTvA9Yt+yqGPXQVVGvgdFwpk6ZxoaDBix9e9DA/kyb9lKJE9WH9YduQf8DdqTfvkNo6tGNrj1XZ6ezj+X+486l23o9WW/Iptz7BV/UsRG8+upr3HnnPey//178859PlD1OTdtg5y0YdMCODNxne7r06Ea3tVbnsNtOY8lbC/nYXc0v5tl19e4cPv4M/rz7N0ueVqq+9XfZgv4H7ES/fYfQpUc3uq61OkPP/hpvTJvF1OsnAPDiDRMZeuZXSp5URfA+YDF6Dd2CfgfsSN9l7gPuePaxPLDMfcAJ3gesW3ZVjHroygVGJvdNfIjNNtuYwYM3ZOrU6RxxxEiO/tzXyx6r5v3z1Cv556lXAtD7w1ux2dcO4f7jzgVg4KG7Mv2WB1myYGGZI6pAvXv3YuHCRbz66mustloP9t77I4wefV7ZY9W8B396FQ/+9CoA+g7bim2+evC7zkICcNQTF7q8kNro0VOv5NGWn1V9hm3FFl87hPuO+zXbfv9INth9GyaPvYM+w7Zi7jPTSp5URfA+YDEmnXolk1q6Wr/lPuAD3gdsGHZVjHroygVGJosXL+Ybx/+QG67/PV2amrjk0iv9V+KCDTp8GE/86i9lj6EC9eu3ARf85gy6NDXR1NTE1X+6nptuvK3ssaSadvnvzmGvPYfRu3cvJj8zkRNPOp2LL2moF27tNP86+6/scs6xbH7MCBbNm8/937yw7JFUAO8Ddr6Bhw/jSe8D1jW76ny10lVU9blEtXae4lrxx157lT1CXTp8+u/f/0S9ClpzjcF2VYBfr7d72SPUpc9NvbwmuvLnVX5j1x9e9gh16xPTrrCrBnW19wELM7JG7gfaVX52VZzWuvJFPCVJkiRJUuW5wJAkSZIkSZXnAkOSJEmSJFWeCwxJkiRJklR5LjAkSZIkSVLlucCQJEmSJEmV5wJDkiRJkiRVngsMSZIkSZJUeS4wJEmSJElS5bnAkCRJkiRJlecCQ5IkSZIkVZ4LDEmSJEmSVHkuMCRJkiRJUuW5wJAkSZIkSZXnAkOSJEmSJFWeCwxJkiRJklR5LjAkSZIkSVLlucCQJEmSJEmV5wJDkiRJkiRVngsMSZIkSZJUeS4wJEmSJElS5bnAkCRJkiRJlRcppbJnqHkRcUxK6YKy56g3Xq+Nzdu/GF6vjc3bvxher43N278YXq+Nzdu/GPVwvXoERh7HlD1AnfJ6bWze/sXwem1s3v7F8HptbN7+xfB6bWze/sWo+evVBYYkSZIkSao8FxiSJEmSJKnyXGDkUdPPI6owr9fG5u1fDK/XxubtXwyv18bm7V8Mr9fG5u1fjJq/Xn0RT0mSJEmSVHkegSFJkiRJkirPBUYHRcRBEfGviHgqIr5b9jz1ICJ+GxEvR8SjZc+icthVfnbV2GyqGHbV2OyqGHbV2OyqGPXUlQuMDoiILsA5wAhga+CoiNi63KnqwiXAQWUPoXLYVWEuwa4akk0V6hLsqiHZVaEuwa4akl0V6hLqpCsXGB2zC/BUSumZlNJbwFhgZMkz1byU0t+BWWXPodLYVQHsqqHZVEHsqqHZVUHsqqHZVUHqqSsXGB0zEHhhmbentFwmqf3sSsrLpqT87ErKz660Ui4wOiaWc5mndZE6xq6kvGxKys+upPzsSivlAqNjpgAbLvP2IODFkmaR6oVdSXnZlJSfXUn52ZVWygVGx9wHbB4RG0dEd+BTwF9KnkmqdXYl5WVTUn52JeVnV1opFxgdkFJaBBwHjAMmAVellB4rd6raFxFjgLuBD0bElIj4YtkzqfPYVTHsqnHZVHHsqnHZVXHsqnHZVXHqqatIyacVSZIkSZKkavMIDEmSJEmSVHkuMCRJkiRJUuW5wJAkSZIkSZXnAkOSJEmSJFWeCwxJkiRJklR5LjAqLiIWR8RDEfFoRPwhItbowOcaHhHXtfz+sIj4br5JpdpgU1J+diXlZ1dSfnZV+1xgVN+bKaUhKaVtgbeAry77zmi2yrdjSukvKaWf5hpSqiE2JeVnV1J+diXlZ1c1zgVGbbkT2CwiBkfEpIg4F3gA2DAiDoiIuyPigZZtYk+AiDgoIh6PiPHAv739iSLi3yPi7Jbf942IayLiHy2/Ptxy+Z8j4v6IeCwijlnmY4+KiEdaNpendeYVIGVmU1J+diXlZ1dSfnZVg1xg1IiI6AqMAB5pueiDwGUppR2AecAPgf1SSjsCE4H/iojVgN8AhwJ7AP1a+fRnAXeklLYHdgQea7n8P1JKOwE7A/8vItaPiAHAacA+wBBgaEQcnvdvKxXPpqT87ErKz66k/OyqdrnAqL7VI+IhmsN5Hrio5fLnUkr3tPx+N2Br4K6WP/t5YCNgS+DZlNKTKaUEXN7K19gH+DVASmlxSunVlsv/X0T8A7gH2BDYHBgK/C2lNCOltAi4Atgz319XKpxNSfnZlZSfXUn52VWN61r2AFqpN1NKQ5a9ICKgeTO49CLgf1NKR73nzw0BUnu+aEQMB/YDhqWU3oiIvwGrtZBjtI0AACAASURBVHwtqZbZlJSfXUn52ZWUn13VOI/AqA/3AB+JiM0AImKNiNgCeBzYOCI2bflzR7Xy8bcCX2v52C4RsTawDjC7JbAtad5EAtwL7BURvSOiS8vnvKOQv5VUHpuS8rMrKT+7kvKzqwpzgVEHUkozgH8HxkTEwzRHt2VKaT5wDHB9NL/QzHOtfIpvAHtHxCPA/cA2wE1A15bPN6rlc5JSmgZ8D7gd+AfwQErp2qL+blIZbErKz66k/OxKys+uqi2an74jSZIkSZJUXR6BIUmSJEmSKs8FhiRJkiRJqjwXGJIkSZIkqfJcYEiSJEmSpMpzgSFJkiRJkirPBYYkSZIkSao8FxiSJEmSJKnyXGBIkiRJkqTKc4EhSZIkSZIqzwWGJEmSJEmqPBcYkiRJkiSp8lxgSJIkSZKkynOBIUmSJEmSKs8FRoOIiMERcUNEzI6I6RFxdkR0LXsuqZZFxFYRcVtEvBoRT0XEx8qeSao1EXFcREyMiAURccl73rdvRDweEW9ExO0RsVFJY0o1o7WmIqJ7RPwxIiZHRIqI4eVNKdWWFXS1dcvls1t+3RIRW5c4at1zgdE4zgVeBvoDQ4C9gGNLnUiqYS0LwGuB64BewDHA5RGxRamDSbXnReBk4LfLXhgRvYE/ASfQ3NhE4MpOn06qPcttqsV44LPA9E6dSKp9rXX1IvAJmn9O9Qb+Aozt3NEaiwuMxrExcFVKaX5KaTpwE7BNyTNJtWxLYABwZkppcUrpNuAu4Ohyx5JqS0rpTymlPwOvvOdd/wY8llL6Q0ppPvBjYPuI2LKzZ5RqSWtNpZTeSin9IqU0HlhcznRSbVpBV3NSSpNTSgkImtvarIwZG4ULjMbxS+BTEbFGRAwERtC8xJDUPtHKZdt29iBSndoG+Mfbb6SU5gFP4/JdklQxETEHmA/8Cji15HHqmguMxnEHzXf6XgOm0Hwo7p9LnUiqbY/T/LSsb0dEt4g4gOanZq1R7lhS3egJvPqey14F1iphFkmSWpVSWhdYBzgOeLDkceqaC4wGEBFNwDian0u8Js3Pz1oPOK3MuaRallJaCBwOHELzc4m/CVxF84JQUse9Dqz9nsvWBuaWMIskSSvUcqTgecBlEbFB2fPUKxcYjaEXsCFwdkppQUrpFeBi4OByx5JqW0rp4ZTSXiml9VNKBwKbABPKnkuqE48B27/9RkSsCWzacrkkSVXURPPRuAPLHqReucBoACmlmcCzwNciomtErAt8nmWeWyxp1UXEhyJitZbXlvkWzWf5uaTksaSa0vJzaTWgC9ClpamuwDXAthHx8Zb3/wh4OKX0eJnzSlW3gqaIiB4t7wPo3vK+5b2mk6RltNZVROwfETtERJeIWBsYDcwGJpU6cB1zgdE4/g04CJgBPAUsAv6z1Imk2nc0MI3m18LYF9g/pbSg3JGkmvND4E3guzSf3vFN4IcppRnAx4FTaL4zuCvwqbKGlGrIcptqed+/Wt4eSPPTi98ENiphRqnWtNbVusAYml+j6Wmaz0ByUMvZs1SAaD7jiyRJkiRJUnV5BIYkSZIkSao8FxiSJEmSJKnyXGBIkiRJkqTKc4EhSZIkSZIqzwWGJEmSJEmqvK5lD9CahTOf8fQoBVhr0PCyR6hL8+c/XxPnULerYqw+YI+yR6hLi96aalcNyqaKY1eNy66KY1eNy8dWxWnt8ZVHYEiSJEmSpMpzgSFJkiRJkirPBYYkSZIkSao8FxiSJEmSJKnyXGBIkiRJkqTKc4EhSZIkSZIqzwWGJEmSJEmqPBcYkiRJkiSp8lxgSJIkSZKkynOBIUmSJEmSKs8FhiRJkiRJqjwXGJIkSZIkqfJcYEiSJEmSpMpzgSFJkiRJkirPBYYkSZIkSao8FxiSJEmSJKnyXGBIkiRJkqTKc4EhSZIkSZIqzwWGJEmSJEmqPBcYkiRJkiSp8lxgSJIkSZKkynOBIUmSJEmSKs8FhiRJkiRJqjwXGJIkSZIkqfJcYEiSJEmSpMpzgSFJkiRJkirPBYYkSZIkSao8FxiSJEmSJKnyuhb1iSNiS2AkMBBIwIvAX1JKk4r6mlK9syspP7uS8rIpKT+7kpoVcgRGRHwHGAsEMAG4r+X3YyLiu0V8Tane2ZWUn11JedmUlJ9dSe+IlFL+TxrxBLBNSmnhey7vDjyWUtq8lY87BjgG4NwzTt7pS587KvtsjW6tQcPLHqEuzZ//fBT9NeyqulYfsEfZI9SlRW9NtasGZVPFKbqr9jbV8mfsqkB2VRy7alw+tipOa4+vinoNjCXAgOVc3r/lfcuVUrogpbRzSmnnWovrh6eOZs9DPsXhn/3q0stefW0uX/rG9zn4yC/ypW98n1dfmwvA3Nfn8fX//h/+7fPHMvIzX+Ga628ua+zKGzSoP+PGjeWhh27lgQdu4etf/4+yRyqTXQHjbruTkZ/5CtvtfjCPTnriXX/+N5ddyYgj/oOPfupL3HXv/Z09bk058IDhPPbo33n8n+P5729/vexxymRX2FUudgW0symwK72fTS1lVy18fNUx9fDYqqgFxvHArRFxY0Rc0PLrJuBW4BsFfc1SHX7w/pw3+uR3XXbh765it52HcMOVF7HbzkO46PKrABhz9V/ZdPAH+NOl53Lx2afx81/9hoULFy7v0za8RYsW853vnMyQIfuy554j+epXP8eWW7a6ZK53dgVstslG/OLUE9hpyLbvuvzpZ5/jxlvv4NrLz+O80Scz6vSzWbx4cWeOWzOampo465en8NFDP8t22+/NkUcezlZb2ZVd2VVH2NVSDdcU2FURbOpd7KqFj686ph4eWxWywEgp3QRsAZwIjANuBn4MfLDlfXVn5yHbsc7aa73rstvvvJuRI/YDYOSI/bjt73cDEBHMe+NNUkq88eZ81ll7Lbp06dLpM9eC6dNf5qGHHgXg9dfn8fjjTzFwYL+SpyqHXTXbdPAH2HijQe/7s7fdeQ8j9t2L7t27M2hAPz4waACPvOdfvNRsl6E78PTTk3n22edZuHAhV111LYcdemDZY5XCrprZVcfZVbNGbArsqgg29Q67eoePrzqmHh5bFXYWkpTSEuCeoj5/LXhl9hz69O4FQJ/evZg151UAPv3xQznuOyey98jPMO+NNzn9pO/R1OQZbVdmo40GMWTINkyY8GDZo5TGrlr38oxX+NC2Wy59u+8GvXl5xswSJ6quAQP78cKUF5e+PWXqNHYZukOJE5XLrlpnV21nV++wqRWzq7axqXezq2Y+vsqnVh9beauW4K4J97Pl5ptw+7VXcPUl53Dq6HN5fd68sseqtDXXXIMxY87nW986kblzXy97HFVQ4v0vSBwU/hqQNSni/ddLES/orNpnV21nV2oru2obm9Kq8PHVqqnlx1YuMAq0/nrrMmPmLABmzJxFr3XXAeCa6/+X/fb6CBHBBwYNYGD/fjz73JQyR620rl27Mnbs+Ywdew3XXlu3R8mpg/r26c30l2Ysffull2fSp8/6JU5UXVOnTGPDQe+8Ftiggf2ZNu2lEidSVdlV29mV2squ2samtDw+vuq4Wn9s5QKjQMN3341rb7wFgGtvvIW99xgGQP++fbjn/ocAmDlrNpOfn8KgAbX13KPOdP75P+fxx5/irLMuLHsUVdjeu+/GjbfewVtvvcWUF6fz/JQX2W6rLcoeq5Lum/gQm222MYMHb0i3bt044oiR/PU6X61b72dXbWdXaiu7ahub0vL4+Krjav2xVbR2KFZE7LiiD0wpPVDIRC0Wznympo4R+/b//JT7HnyYOXNeY/1e63LsF49m3z2H8c0TTmXaSzPo37cPo0/+AeusvRYvz3iFH5xyBjNfmU1KiS8efQSHHrhPp8xZa+cq/vCHh3LbbVfzyCOTWLKk+SxRP/rRzxg37vaSJ3u31s5T/F52tWqW19U6a/fkJ2f+mllzXmWtnj3ZcvNNuODMUwA4/9IxXHPdzXTt0oXvfOMr7DFsaKfMufqAPTrl6+Q04qB9OOOME+nS1MQll17JT356Vtkjvc+it6baVQFqoatabArsKie7yq8Wu6qFpsCuilILj698bFWc1h5frWiBsaK/RUopFfp/RK0FVitqLbJasQoLDLuqQ7V4p7AWrMIdQruqMzZVHLtqXHZVHLtqXD62Kk5rj69aPQtJSmnv4saRGpNdSfnZlZSfXUn52ZXUcW06jWpEbAtsDaz29mUppcuKGkpqBHYl5WdXUn52JeVnV1L7rHSBERH/AwynObAbgBHAeMDApHayKyk/u5LysyspP7uS2q8tZyH5BLAvMD2l9AVge6BHoVNJ9c+upPzsSsrPrqT87Epqp7YsMN5MKS0BFkXE2sDLwCbFjiXVPbuS8rMrKT+7kvKzK6md2vIaGBMjYl3gN8D9wOvAhEKnkuqfXUn52ZWUn11J+dmV1E6tnkZ1uX84YjCwdkrp4aIGepun+SmGp/opRltPo7o8dlX7PDVdMdp6WrrlsavaZlPFsavGZVfFsavG5WOr4rT2+GqlTyGJZp+NiB+llCYDcyJil9wDSo3ErqT87ErKz66k/OxKar+2vAbGucAw4KiWt+cC5xQ2kdQY7ErKz66k/OxKys+upHZqy2tg7JpS2jEiHgRIKc2OiO4FzyXVO7uS8rMrKT+7kvKzK6md2nIExsKI6AIkgIjoAywpdCqp/tmVlJ9dSfnZlZSfXUnt1JYFxlnANcAGEXEKMB44tdCppPpnV1J+diXlZ1dSfnYltdNKn0KSUroiIu4H9gUCODylNKnwyaQ6ZldSfnYl5WdXUn52JbVfW14Dg5TS48DjABGxbkT8IKV0SqGTSXXOrqT87ErKz66k/OxKap9Wn0ISERtGxAURcV1EfCki1oiIM4AngA06b0SpftiVlJ9dSfnZlZSfXUkdt6IjMC4D7gCuBg4C7gEeAz6UUpreCbNJ9ciupPzsSsrPrqT87ErqoBUtMHqllH7c8vtxEfESMDSltKD4saS6ZVdSfnYl5WdXUn52JXXQCl8DIyLWo/mFZQCmA2tExJoAKaVZBc8m1SW7kvKzKyk/u5LysyupY1a0wFgHuJ93AgN4oOW/CdikqKGkOmZXUn52JeVnV1J+diV1UKsLjJTS4E6cQ2oIdiXlZ1dSfnYl5WdXUse1ehYSSZIkSZKkqnCBIUmSJEmSKm+FL+JZptUH7FH2CHXp6l57lT2CSrTmwD3LHqEujVl/eNkjqER/3faHZY8g1R27kvLz8VV+PrbqfCs8AiMimiLi0c4aRmoEdiXlZ1dSfnYl5WdXUsescIGRUloC/CMiPtBJ80h1z66k/OxKys+upPzsSuqYtjyFpD/wWERMAOa9fWFK6bDCppLqn11J+dmVlJ9dSfnZldRObVlgnFj4FFLjsSspP7uS8rMrKT+7ktpppQuMlNIdEdEXGNpy0YSU0svFjiXVN7uS8rMrKT+7kvKzK6n9Vnoa1Yg4ApgAfBI4Arg3Ij5R9GBSPbMrKT+7kvKzKyk/u5Lary1PIfkBMPTtrWBE9AFuAf5Y5GBSnbMrKT+7kvKzKyk/u5LaaaVHYABN7zmk6ZU2fpyk1tmVlJ9dSfnZlZSfXUnt1JYjMG6KiHHAmJa3jwRuKG4kqSHYlZSfXUn52ZWUn11J7dSWF/H8dkR8HPgIEMAFKaVrCp9MqmN2JeVnV1J+diXlZ1dS+7XlCAxSSlcDVxc8i9RQ7ErKz66k/OxKys+upPZpdYEREXOBtLx3ASmltHZhU0l1yq6k/OxKys+upPzsSuq4VhcYKaW1OnMQqRHYlZSfXUn52ZWUn11JHdemp5AARMQGwGpvv51Ser6QiaQGYldSfnYl5WdXUn52Ja26lZ6uJyIOi4gngWeBO4DJwI0FzyXVNbuS8rMrKT+7kvKzK6n92nK+4VHAbsATKaWNgX2BuwqdSqp/diXlZ1dSfnYl5WdXUju1ZYGxMKX0CtAUEU0ppduBIQXPJdU7u5LysyspP7uS8rMrqZ3a8hoYcyKiJ/B34IqIeBlYVOxYUt2zKyk/u5LysyspP7uS2qktR2CMBN4E/hO4CXgaOLTIoaQGYFdSfnYl5WdXUn52JbVTq0dgRMTZwO9TSv+3zMWXFj+SVL/sSsrPrqT87ErKz66kjlvRERhPAmdExOSIOC0ifF6W1HF2JeVnV1J+diXlZ1dSB7W6wEgp/TKlNAzYC5gFXBwRkyLiRxGxRadNKNURu5LysyspP7uS8rMrqeNW+hoYKaXnUkqnpZR2AD4NfAyYVPhkUh2zKyk/u5LysyspP7uS2m+lC4yI6BYRh0bEFcCNwBPAxwufTKpjdiXlZ1dSfnYl5WdXUvut6EU89weOAg4BJgBjgWNSSvM6aTap7tiVlJ9dSfnZlZSfXUkd1+oCA/g+8HvgWymlWZ00j1Tv7ErKz66k/OxKys+upA5qdYGRUtq7MweRGoFdSfnZlZSfXUn52ZXUcSt9DQxJkiRJkqSyucCQJEmSJEmV5wJDkiRJkiRVXqcvMCLiC539NaV6Z1dSfnYl5WdXUn52pUZSxhEYJ7b2jog4JiImRsTEJUs8m5C0CtrW1WK7klZBm7q6+Y2nOnMmqdbZlZSfj6/UMFZ0GtV2i4iHW3sX0Le1j0spXQBcANC1+8BUwGiFOvCA4YwefRJdmpr47cVj+NnPzyl7pPrRFOw17hTmT5/FvUefzu5//hFde64GQI/e6zD7waeZ8IXRJQ9ZrBxdde8xqKa6uuD80zn44P2YMWMmO+y4X9nj1J+mYL+bTubN6bO563OnA7Dtdz/JoI/uSlqyhKcvvZWnLhpX8pDFytHVn/p9uqa6AqAp2GfcKbw5fRZ3H306ey7ne+o9df49tQjeD2hmV3aVi029oxEfX3n7F6iGH1sVssCgOaIDgdnvuTyA/yvoa5aqqamJs355CgcdfBRTpkzjnrtv4K/X3cykSU+WPVpd2PTLI3j9yal0XWt1AMYfftLS9w298Himj7u/rNE6U8N1ddnv/sC5v76Ei3/7i7JHqUubf/kg5j754tKuBh+5J6sPWJ+b9vg2pESP9dcuecJO0XBdAWz25RHMXeZ76t+X+Z6664XHM60xvqdm5f2Ad7Er7KqjbOp9Gqorb/9i1fJjq6KeQnId0DOl9Nx7fk0G/lbQ1yzVLkN34OmnJ/Pss8+zcOFCrrrqWg479MCyx6oLq/XvRd/9hvDcFbe/731d11yN3rtvw7QbJ5YwWadruK7Gj7+X2bPnlD1GXVq9fy/67zuEZ37/Tlebfn4//jn6GkjN/0Cz4JXXyhqvMzVcV6v370W//YYwuZXvqX1234YXG+N7albeD3gXu1qGXbWPTb1PQ3Xl7V+cWn9sVcgCI6X0xZTS+Fbe9+kivmbZBgzsxwtTXlz69pSp0xgwoF+JE9WP7UYdzWOjxpDS+49663/wUGaOf5RFr79ZwmSdqxG7UnGGnHQ0D588Bpa809WaG23AhiN3Y9+bRrH7Ff9Nz41bPSK1bjRiVx8adTSPtvI9dcDBQ5nRIN9Tc/N+wDvs6t3sqn1s6t0arStv/+LU+mMrT6OaSUS877Ll/U+hVdN3/x1YMPM1Xn342eW+f+DHhjHlmro7ak4qVP/9dmD+zFeZ8/Dkd13epUc3lsxfyK0HncCzV9zGzmceU86AKky/lu+pc1r5njroY8N4we+p7eL9gMZlV8Wwqcbm7V+MenhsVdRrYDScqVOmseGgAUvfHjSwP9OmvVTiRPWh19At6HfAjvTddwhNPbrRtefq7Hj2sTxw3Ll0W68n6w3ZlAlfOLPsMaWasv4uWzDggJ3ov+8QuvToRte1VmeXs7/GG9NmMeX6CQBMvWEiQ8/8SsmTKrf1h25B/5bvqV1avqfufPaxTDzuXLq3fE+9x++p7eL9gMZlV8Wwqcbm7V+Menhs5QIjk/smPsRmm23M4MEbMnXqdI44YiRHf+7rZY9V8yadeiWTTr0SgPU/vBWbfe0QHjjuXAAGHror0295kCULFpY5olRzHj31Sh5t6arPsK3Y4muHMOG4X7Pd949kg923YfLYO+gzbCvmPjOt5EmV22OnXsljLbd97w9vxeZfO4SJfk/NwvsBjcuuimFTjc3bvxj18NjKBUYmixcv5hvH/5Abrv89XZqauOTSK/nnP58oe6y6NvDwYTz5q7+UPYYK9LvLzmbPPYfRu3cvnnn6Pk4adQaXXDK27LHq1uNn/5VdzzmWLY4ZwaJ585n4zQvLHkmdaNDhw3jC76nt5v0ALY9dtZ9NNTZv/85XK4+toqrPJaq18xTXiqt77VX2CHVp5PTfv/+JehXUvccguyrAFXZViE9Ou6ImuvpTv0/bVWZHzLqj7BHq1qK3ptpVg7Kr4tRKVz6+ys/HVsVp7fGVL+IpSZIkSZIqzwWGJEmSJEmqPBcYkiRJkiSp8lxgSJIkSZKkynOBIUmSJEmSKs8FhiRJkiRJqjwXGJIkSZIkqfJcYEiSJEmSpMpzgSFJkiRJkirPBYYkSZIkSao8FxiSJEmSJKnyXGBIkiRJkqTKc4EhSZIkSZIqzwWGJEmSJEmqPBcYkiRJkiSp8lxgSJIkSZKkynOBIUmSJEmSKs8FhiRJkiRJqjwXGJIkSZIkqfJcYEiSJEmSpMpzgSFJkiRJkirPBYYkSZIkSaq8SCmVPUPNi4hjUkoXlD1HvfF6bWze/sXwem1s3v7F8HptbN7+xfB6bWze/sWoh+vVIzDyOKbsAeqU12tj8/YvhtdrY/P2L4bXa2Pz9i+G12tj8/YvRs1fry4wJEmSJElS5bnAkCRJkiRJlecCI4+afh5RhXm9NjZv/2J4vTY2b/9ieL02Nm//Yni9NjZv/2LU/PXqi3hKkiRJkqTK8wgMSZIkSZJUeS4wOigiDoqIf0XEUxHx3bLnqQcR8duIeDkiHi17FpXDrvKzq8ZmU8Wwq8ZmV8Wwq8ZmV8Wop65cYHRARHQBzgFGAFsDR0XE1uVOVRcuAQ4qewiVw64Kcwl21ZBsqlCXYFcNya4KdQl21ZDsqlCXUCdducDomF2Ap1JKz6SU3gLGAiNLnqnmpZT+Dswqew6Vxq4KYFcNzaYKYlcNza4KYlcNza4KUk9ducDomIHAC8u8PaXlMkntZ1dSXjYl5WdXUn52pZVygdExsZzLPK2L1DF2JeVlU1J+diXlZ1daKRcYHTMF2HCZtwcBL5Y0i1Qv7ErKy6ak/OxKys+utFIuMDrmPmDziNg4IroDnwL+UvJMUq2zKykvm5LysyspP7vSSrnA6ICU0iLgOGAcMAm4KqX0WLlT1b6IGAPcDXwwIqZExBfLnkmdx66KYVeNy6aKY1eNy66KY1eNy66KU09dRUo+rUiSJEmSJFWbR2BIkiRJkqTKc4EhSZIkSZIqzwWGJEmSJEmqPBcYkiRJkiSp8lxgSJIkSZKkynOBUXERsTgiHoqIRyPiDxGxRgc+1/CIuK7l94dFxHfzTSrVBpuS8rMrKT+7kvKzq9rnAqP63kwpDUkpbQu8BXx12XdGs1W+HVNKf0kp/TTXkFINsSkpP7uS8rMrKT+7qnEuMGrLncBmETE4IiZFxLnAA8CGEXFARNwdEQ+0bBN7AkTEQRHxeESMB/7t7U8UEf8eEWe3/L5vRFwTEf9o+fXhlsv/HBH3R8RjEXHMMh97VEQ80rK5PK0zrwApM5uS8rMrKT+7kvKzqxrkAqNGRERXYATwSMtF/397dx5vVV3vf/z1ORwUxRFwYHBIHHAoDyoGDoiB5pBDP8uulUPD9Wb5KG91f2lq5UR5U7t21QozFUPUq6mZU2mKYiGC88DVFDUUFGQQEBXhe/84W3PgwDHW2mvttV/Px+M8PHvt6b33ebxdm89ew1bA6JTSQGAhcBIwIqW0AzAJ+HZEdAMuBA4Adgc27ODhfw6MSyltD+wAPFZb/uWU0o7ATsA3I6JnRPQBzgQ+AbQBgyLi4GxfrZQ/OyVlz15J2bNXUvbsVeNygFF+q0XEg7QX53ngotry51JKE2q/Dwa2Ae6p3fZIYBNgADA1pfRUSikBv+3gOT4B/AIgpbQkpTSvtvybEfEQMAHYCNgCGATcmVKamVJ6CxgDDM3u5Uq5s1NS9uyVlD17JWXPXjW41qIDaIUWpZTa3r0gIqB9MvjOIuBPKaXD3ne7NiD9M08aEcOAEcCQlNJrEXEn0K32XFIjs1NS9uyVlD17JWXPXjU4t8CohgnArhGxOUBErB4RWwJTgI9ERP/a7Q7r4P63A8fU7tslItYC1gbm1Ao2gPZJJMC9wB4R0SsiutQec1wur0oqjp2SsmevpOzZKyl79qrEHGBUQEppJnAUMDYiHqa9dANSSq8DRwM3RvuBZp7r4CG+BewZEY8Ak4FtgVuA1trjnVZ7TFJK04ETgDuAh4D7U0rX5/XapCLYKSl79krKnr2Ssmevyi3ad9+RJEmSJEkqL7fAkCRJkiRJpecAQ5IkSZIklZ4DDEmSJEmSVHoOMCRJkiRJUuk5wJAkSZIkSaXnAEOSJEmSJJWeAwxJkiRJklR6DjAkSZIkSVLpOcCQJEmSJEml5wBDkiRJkiSVngMMSZIkSZJUeg4wJEmSJElS6TnAkCRJkiRJpecAo6Ii4tiImBQRb0TEJR3c5ocRkSJiRJ3jSQ1peb2KiNUj4oKImBUR8yLiroJiSg2lo15FxBciYsG7fl6rrbN2LDCuJEkqUGvRAZSbF4HTgU8Cq73/yojoD3wGmF7nXFIjW16vRtH+/9StgdlAW32jSQ1rmb1KKY0Bxrx9OSKOAk4G7q9zPkmSVBIOMCoqpfQ7gIjYCei3jJucB3wPuKCeuaRG1lGvImIr4ECgX0rp1driyfVPKDWeTqyv3nYkMDqllOoSTJIklY67kDShiPgs8GZK6aais0gV8XHgOeCU2i4kj0TEIUWHkqoiIjYBhgKji84iSZKK4wCjyUTEGsBI4Liis0gV0g/YDpgHuzzpegAAGPJJREFU9AGOBS6NiK0LTSVVxxHA3SmlqUUHkSRJxXGA0XxOAS7zQ6CUqUXAYuD0lNKbKaVxwB3A3sXGkirjCODSokNIkqRiOcBoPsOBb0bEjIiYAWwEXBUR3ys4l9TIHi46gFRVEbEr7Vs2XV10FkmSVCwHGBUVEa0R0Q3oAnSJiG4R0Ur7AGM72s+Q0Eb70d//DTi/sLBSg1hOr+4CngdOqN1mV2AYcGtxaaXGsJxeve1I4JqU0vxiEkqSpLJwgFFdJ9G+WfvxwBdrv5+UUnolpTTj7R9gCTAnpbSgwKxSo+ioV4uBg4D9aD8OxoXAESmlKUUFlRrIMnsFUBtsHIq7j0iSJCA8G5kkSZIkSSo7t8CQJEmSJEml5wBDkiRJkiSVngMMSZIkSZJUeg4wJEmSJElS6TnAkCRJkiRJpde64psUY/GsZzw9Sg7W3Xh40REqacFrU6PoDJ1hr/LRve/QoiNU0ptvTLNXTcpO5adReiVJ0rK4BYYkSZIkSSo9BxiSJEmSJKn0HGBIkiRJkqTSc4AhSZIkSZJKzwGGJEmSJEkqPQcYkiRJkiSp9BxgSJIkSZKk0nOAIUmSJEmSSs8BhiRJkiRJKj0HGJIkSZIkqfQcYEiSJEmSpNJzgCFJkiRJkkrPAYYkSZIkSSo9BxiSJEmSJKn0HGBIkiRJkqTSc4AhSZIkSZJKzwGGJEmSJEkqPQcYkiRJkiSp9BxgSJIkSZKk0nOAIUmSJEmSSs8BhiRJkiRJKj0HGJIkSZIkqfQcYEiSJEmSpNJzgCFJkiRJkkrPAYYkSZIkSSo9BxiSJEmSJKn0HGBIkiRJkqTSc4AhSZIkSZJKrzWvB46IAcBBQF8gAS8Cv08pPZHXc0pVZ6+k7NkrSZKkxpDLFhgR8T3gCiCAicB9td/HRsTxeTynVHX2SsqevZIkSWockVLK/kEjngS2TSktft/yVYDHUkpbdHC/o4GjAS44+/Qdv3rEYZlna3brbjy86AiVtOC1qZH3c9ir8ured2jRESrpzTem2asmZafyU49eSZKUl7x2IVkK9AGee9/y3rXrlimlNAoYBbB41jPZT1ZydNLIc7jrnon0WHcdrvvtLwGY9+p8vnPyj3lxxkv02XADzj7tBNZea01+M+ZqbvzjHQAsWbKEZ577O3ffeAVrr7VmkS+hlFZddRVu/dNVrLrKKrS2duG6627mjNP/q+hYRbFXdNyr+QsWcvyp/8n0l2ay5K0lHPX5Q/j0/nsX/ArKqV+/3vzmonPZcMP1WLp0Kb++6HLOO++iomMVxV7Rca/mvTqfk3/8M/7+wnRWXWUVTvv+v7PFZpsW+wJKatSvzmK//UYwc+YsBu4woug4kiRVUl4H8TwOuD0ibo6IUbWfW4DbgW/l9JyFOni/vfjlOae/Z9mvL7uKwTu1cdOVFzF4pzYu+u1VAHz5C5/hmkvP55pLz+e4rx3FTm0fdXjRgTfeeJP99/08Qwbvx5DB+zNirz0YNKit6FhFsVd03Kux19xA/0035neXXsDF553JT//7QhYvXrysh216b721hP//vVP52PZ7stvuB3LM145k6wHL3NCgGdgrOu7VhaOvZMAW/bl29C8YefJ3+cl//bKIyA1h9GX/w6cO+GLRMSRJqrRcBhgppVuALYFTgFuBPwI/AraqXVc5yxpC3HH3Xzlo3/ZvYQ7adwR/vuuvH7jfTbeNY7+99qhLxka1cOFrAHTt2krXrq001FedGbJX7TrqVUSw8LVFpJR4bdHrrL3WmnTp0qXumRvBjBkv8+CDjwKwYMFCpkx5ij59Nyw4VTHsVbuOevX0s88zeMftAdhsk414YfpLzJo9p76BG8T48fcyZ87comNIklRpuZ2FJKW0FJiQ1+M3glfmzGW9Xj0AWK9XD2bPnfee6xe9/jrjJ0zixG9/vYh4DaOlpYXxf7mBzTbbhFG/uoxJ9z1YdKTC2KuOe/X5Qw7g2O+dwp4HfYGFry3irFNPoKXFM0WvyCab9GP77bdj4sQHio5SGHvVca+22nwzbhv3F3bYfjseefx/mf7Sy7z08ix69Vi3yLiSJKlJ+em+QHeOv5eBH9vG3UdWYOnSpewyeH+22mIIO+20Pdtss2XRkVRC90yczIAtNuOO68dwzSXnM/KcC1iwcGHRsUqte/fVufKKUXz3uz9i/vwFRcdRCX318M/y6vwFHHLkNxhz9e8ZsEV/t2ySJEmFcYCRo57rrsPMWbMBmDlrNj3WWfs91998+zj2GzGsgGSNad68+dx99wRGuMtNU+uoV9fe+CdG7LErEcHG/frQt/eGTH1uWpFRS621tZUrrxzF2Cuu5brrby46jgrWUa/W6N6d00/8Ntdcej4/Pvm7zJk7j359NigyqiRJamIOMHI0bLfBXH/zbQBcf/Nt7Ln7kHeum79gIZMeeOQ9y/RBvXr1YO2127dQ6dZtVfbcczeefPLpglOpSB31qvcG6zFhcvvuRbNmz+HZ56fRr09zHtehM0b96iymTPkb5557YdFRVAId9erV+QveORjuNTfcwo5tH2WN7t0LyylJkppbpLTsQyJGxA7Lu2NK6f5cEtU02mnp/uOHP+G+Bx5m7txX6dljHb7+lcMZPnQI3zl5JNNfmknvDdbjnNNPfGd3ketu/BPj753EWaeeUNec6248vK7Pt7K23W4Aoy48iy4tXWhpCX73uxv5yY//u+hYH7DgtanRmdvZqw/nw/Tq5ZmvcOIZZzPrlTmklPjK4YdywCc/UZec3fsOrcvzZGWXXQZx5x3X8sgjT7B0afuZQk/+wZnccsufC072Xm++Mc1e5eDD9OrBR5/g+6edRZeWFjbbdGNOPeG4uuz22GidArhs9HkMHTqEXr168NJLszj1tLO55JIrio71AZ3tlSRJZbS8AcYdy7lfSinl+i+DRvtA2CgabYDRKD7EAMNeVVAj/mOrEXyIAYa9qhg7lR8HGJKkRtbhWUhSSnvWM4jUDOyVlD17JUmS1Bw6dRrViNgO2Abo9vaylNLovEJJzcBeSdmzV5IkSdW1wgFGRPwQGEb7B8KbgH2B8YAfCKV/kr2SsmevJEmSqq0zZyH5DDAcmJFS+hKwPbBqrqmk6rNXUvbslSRJUoV1ZoCxKKW0FHgrItYCXgY2yzeWVHn2SsqevZIkSaqwzhwDY1JErANcCEwGFgATc00lVZ+9krJnryRJkiqsw9OoLvPGEZsCa6WUHs4r0Ns8LV0+PI1qPjp7GtVlsVeNz1M+5mNlTvdorxqbncqPp1GVJDWyFe5CEu2+GBE/SCk9C8yNiJ3zjyZVl72SsmevJEmSqq0zx8C4ABgCHFa7PB84P7dEUnOwV1L27JUkSVKFdeYYGB9PKe0QEQ8ApJTmRMQqOeeSqs5eSdmzV5IkSRXWmS0wFkdEFyABRMR6wNJcU0nVZ6+k7NkrSZKkCuvMAOPnwLXA+hFxBjAeGJlrKqn67JWUPXslSZJUYSvchSSlNCYiJgPDgQAOTik9kXsyqcLslZQ9eyVJklRtnTkGBimlKcAUgIhYJyJOTCmdkWsyqeLslZQ9eyVJklRdHe5CEhEbRcSoiPhDRHw1IlaPiLOBJ4H16xdRqg57JWXPXkmSJDWH5W2BMRoYB1wD7ANMAB4DPpZSmlGHbFIV2Sspe/ZKkiSpCSxvgNEjpfSj2u+3RsRLwKCU0hv5x5Iqy15J2bNXkiRJTWC5x8CIiHVpPxAawAxg9YjoDpBSmp1zNqmS7JWUPXslSZJUfcsbYKwNTOYfHwgB7q/9NwGb5RVKqjB7JWXPXkmSJDWBDgcYKaVN65hDagr2SsqevZIkSWoOHZ6FRJIkSZIkqSwcYEiSJEmSpNJb7kE8i7Ran92LjlBJ1/YYWnQEFche5WNsz2FFR1CB7FX2ru6xR9ERJElSCS13C4yIaImIR+sVRmoG9krKnr2SJEmqvuUOMFJKS4GHImLjOuWRKs9eSdmzV5IkSdXXmV1IegOPRcREYOHbC1NKB+aWSqo+eyVlz15JkiRVWGcGGKfknkJqPvZKyp69kiRJqrAVDjBSSuMiYgNgUG3RxJTSy/nGkqrNXknZs1eSJEnVtsLTqEbEocBE4LPAocC9EfGZvINJVWavpOzZK0mSpGrrzC4kJwKD3v4WKyLWA24Drs4zmFRx9krKnr2SJEmqsBVugQG0vG8T3Fc6eT9JHbNXUvbslSRJUoV1ZguMWyLiVmBs7fLngJvyiyQ1BXslZc9eSZIkVVhnDuL5HxFxCLArEMColNK1uSeTKsxeSdmzV5IkSdXWmS0wSCldA1yTcxapqdgrKXv2SpIkqbo6HGBExHwgLesqIKWU1sotlVRR9krKnr2SJElqDh0OMFJKa9YziNQM7JWUPXslSZLUHDq1CwlARKwPdHv7ckrp+VwSSU3EXknZs1eSJEnVtMLTy0XEgRHxFDAVGAc8C9yccy6p0uyVlD17JUmSVG0rHGAApwGDgSdTSh8BhgP35JpKqj57JWXPXkmSJFVYZwYYi1NKrwAtEdGSUroDaMs5l1R19krKnr2SJEmqsM4cA2NuRKwB3AWMiYiXgbfyjSVVnr2SsmevJEmSKqwzW2AcBCwC/h24BXgaOCDPUFITsFdS9uyVJElShXW4BUZEnAdcnlL6y7sWX5p/JKm67JWUPXslSZLUHJa3BcZTwNkR8WxEnBkR7kcsrTx7JWXPXkmSJDWBDgcYKaVzU0pDgD2A2cDFEfFERPwgIrasW0KpQuyVlD17JUmS1BxWeAyMlNJzKaUzU0oDgc8DnwaeyD2ZVGH2SsqevZIkSaq2FQ4wIqJrRBwQEWOAm4EngUNyTyZVmL2SsmevJEmSqm15B/HcCzgM2B+YCFwBHJ1SWlinbFLl2Cspe/ZKkiSpOXQ4wAC+D1wOfDelNLtOeaSqs1dS9uyVJElSE+hwgJFS2rOeQaRmYK+k7NkrSZKk5rDCY2BIkiRJkiQVzQGGJEmSJEkqPQcYkiRJkiSp9Oo+wIiIL9X7OaWqs1dS9uyVJElSuRSxBcYpHV0REUdHxKSImLR0qWe/kz4EeyVlz15JkiSVyPJOo/pPi4iHO7oK2KCj+6WURgGjAFpX6ZtyiJarT+49jHPOOZUuLS385uKx/OdPzy86UnW0BENvHcnrM2Yz8fCfsst1P6R1jW4ArNprbeY+8Dfu+9I5BYfMVzP26sJRZ7P/fiN4eeYs2gYOLzpO9bQEI245nUUz5nDPEWcBsN3xn6Xfpz5OWrqUpy+9nb9ddGvBIfPVjL0C11e5aQmG3XoGr8+YzYTDz2K3635A19q6apVeazP3gae5t+LrKkmS8pTLAIP2D32fBOa8b3kAf8npOQvV0tLCz889g332O4xp06Yz4a83ccMf/sgTTzxVdLRK2Oxf92X+Uy/Qdc3VAPjLwf/4YnSnXx/HjFsnFxWtnpquV6NHX8UFF1zMxRefW3SUStriX/dh/lMv0lrr1aafG8pqfXpyy+7/ASmxas+1Ck5YF03XK9dX+en/vnXV+INPfee6nX99HNObY10lSVJu8tqF5A/AGiml59738yxwZ07PWaidBw3k6aefZerU51m8eDFXXXU9Bx7wyaJjVUK33j1Yf8RAnh9zxweu69K9Gz1325YZN08qIFndNV2v7h5/L7PnzC06RiWt1rsHvYe38czl/+hV/yNH8Pg510Jq36DgjVdeLSpePTVdr1xf5aNb7x5sOKKN55axrmrt3o1eu23L9OZYV0mSlJtctsBIKX1lOdd9Po/nLFqfvhvy92kvvnN52gvT2XnQwAITVce2px3BE6dd/s4uI+/We79BzBr/GG8tWFRAsvpqxl4pP22nHs7Dp4+la/fV3lnWfZP12eigwfTddyfeeGU+D550KQumvlRgyvw1Y69cX+Xjo6cdzqOnjX1nl5F3673fIGaOf7Qp1lWSJOXJ06hmJCI+sCylhtstunTW32sgb856lXkPT13m9X0/vQsvXlvJrbyl3PQeMZDXZ81j7sPPvmd5l1W7svT1xdy+z8lMHfNndvrZ0cUEVK5cX2Vvg70G8sZy1lX9Pj2EF1xXSZK00vI6BkbTeWHadDbq1+edy/369mb69Gp/c1kPPQZtxQZ778D6w9toWbUrXddYjYHnfYMHjj2fruuuwTpt/St/8E4paz133pI+e+9I7+FtdFm1K61rrsbO5x3Da9NnM+3GiQC8cNMkBv3s3wpOqjy4vspez0Fb0nvvHdiwtq5qXWM1djzv60w+9gK6rrsG67b1594v/azomJIkNTwHGBm5b9KDbL75R9h004144YUZHHroQRx+xDeKjtXwpoy8gikjrwCg5y5b0/+YT/HAse1Hy+9zwGBeuu0Blr6xuMiIUsN5dOSVPDrySgDWG7I1Wx6zPxOP/QUf/f7nWH+3bXn2inGsN2Rr5j8zveCkyoPrq+w9PvJKHq91qtcuW7P5Mfsz+dgLAOh7wMeZ4bpKkqRMuAtJRpYsWcK3jjuJm268nEcfvpOrr76Bxx9/suhYldbnYDfJrbrfXnY+4+/6PVtt2Z9nn5nEl476l6IjVdqU826g3/6D2PvPP+Gj3/8ck77z66IjKQeur+qr38FDmOa6SpKkTERZ93ttXaVvOYM1uGt7DC06QiUdMGPsB3cqLyF7lY+xPYcVHaGSPjt9jL1qUlf32KPoCJV18IzLG6JXkiQti1tgSJIkSZKk0nOAIUmSJEmSSs8BhiRJkiRJKj0HGJIkSZIkqfQcYEiSJEmSpNJzgCFJkiRJkkrPAYYkSZIkSSo9BxiSJEmSJKn0HGBIkiRJkqTSc4AhSZIkSZJKzwGGJEmSJEkqPQcYkiRJkiSp9BxgSJIkSZKk0nOAIUmSJEmSSs8BhiRJkiRJKj0HGJIkSZIkqfQcYEiSJEmSpNJzgCFJkiRJkkrPAYYkSZIkSSo9BxiSJEmSJKn0HGBIkiRJkqTSc4AhSZIkSZJKL1JKRWdoeBFxdEppVNE5qsb3tbn598+H72tz8++fD99XSZLqwy0wsnF00QEqyve1ufn3z4fva3Pz758P31dJkurAAYYkSZIkSSo9BxiSJEmSJKn0HGBkw/1e8+H72tz8++fD97W5+ffPh++rJEl14EE8JUmSJElS6bkFhiRJkiRJKj0HGJIkSZIkqfQcYKykiNgnIv43Iv4WEccXnacKIuI3EfFyRDxadBYVw15lz141NzuVD3slSVJ9OcBYCRHRBTgf2BfYBjgsIrYpNlUlXALsU3QIFcNe5eYS7FVTslO5ugR7JUlS3TjAWDk7A39LKT2TUnoTuAI4qOBMDS+ldBcwu+gcKoy9yoG9amp2Kif2SpKk+nKAsXL6An9/1+VptWWS/nn2SsqWnZIkSZXgAGPlxDKWeV5aaeXYKylbdkqSJFWCA4yVMw3Y6F2X+wEvFpRFqgp7JWXLTkmSpEpwgLFy7gO2iIiPRMQqwL8Avy84k9To7JWULTslSZIqwQHGSkgpvQUcC9wKPAFclVJ6rNhUjS8ixgJ/BbaKiGkR8ZWiM6l+7FU+7FXzslP5sVeSJNVXpORusJIkSZIkqdzcAkOSJEmSJJWeAwxJkiRJklR6DjAkSZIkSVLpOcCQJEmSJEml5wBDkiRJkiSVngOMkouIJRHxYEQ8GhH/ExGrr8RjDYuIP9R+PzAijs8uqdQY7JSUPXslSZLqwQFG+S1KKbWllLYD3gS+9u4ro92H/jumlH6fUvpJViGlBmKnpOzZK0mSlDsHGI3lbmDziNg0Ip6IiAuA+4GNImLviPhrRNxf+/ZrDYCI2CcipkTEeOD/vf1AEXFURJxX+32DiLg2Ih6q/exSW35dREyOiMci4uh33fewiHik9k3bmfV8A6SM2Skpe/ZKkiTlwgFGg4iIVmBf4JHaoq2A0SmlgcBC4CRgREppB2AS8O2I6AZcCBwA7A5s2MHD/xwYl1LaHtgBeKy2/MsppR2BnYBvRkTPiOgDnAl8AmgDBkXEwdm+Wil/dkrKnr2SJEl5coBRfqtFxIO0f9B7Hriotvy5lNKE2u+DgW2Ae2q3PRLYBBgATE0pPZVSSsBvO3iOTwC/AEgpLUkpzast/2ZEPARMADYCtgAGAXemlGamlN4CxgBDs3u5Uu7slJQ9eyVJknLXWnQArdCilFLbuxdEBLR/k/XOIuBPKaXD3ne7NiD9M08aEcOAEcCQlNJrEXEn0K32XFIjs1NS9uyVJEnKnVtgVMMEYNeI2BwgIlaPiC2BKcBHIqJ/7XaHdXD/24FjavftEhFrAWsDc2ofCAfQ/s0ZwL3AHhHRKyK61B5zXC6vSiqOnZKyZ68kSdJKcYBRASmlmcBRwNiIeJj2D4kDUkqvA0cDN9YOjPZcBw/xLWDPiHgEmAxsC9wCtNYe77TaY5JSmg6cANwBPATcn1K6Pq/XJhXBTknZs1eSJGllRfvuppIkSZIkSeXlFhiSJEmSJKn0HGBIkiRJkqTSc4AhSZIkSZJKzwGGJEmSJEkqPQcYkiRJkiSp9BxgSJIkSZKk0nOAIUmSJEmSSu//AAn2dJtn7PSqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x720 with 13 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for i in range(len(lista_matriz)):\n",
    "    plt.subplot(4,5 ,i+1)\n",
    "    cm = lista_matriz[i]\n",
    "    annot_kws = {\"ha\": 'left',\"va\": 'top'}\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='d', cbar=False, annot_kws=annot_kws);\n",
    "    # correção do bug das numerações em cada quadrado da matriz\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    \n",
    "    plt.title(pacientes[i])\n",
    "    plt.xlabel('Predicao');\n",
    "    plt.ylabel('Valor Real');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
